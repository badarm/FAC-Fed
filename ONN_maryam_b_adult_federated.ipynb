{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140e5580",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import collections\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from mab import algs\n",
    "\n",
    "\n",
    "class ONN(nn.Module):\n",
    "    def __init__(self, features_size, max_num_hidden_layers, qtd_neuron_per_hidden_layer, n_classes, batch_size=1,\n",
    "                 b=0.99, n=0.01, s=0.2, use_cuda=False):\n",
    "        super(ONN, self).__init__()\n",
    "\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            print(\"Using CUDA :]\")\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "\n",
    "        self.features_size = features_size\n",
    "        self.max_num_hidden_layers = max_num_hidden_layers\n",
    "        self.qtd_neuron_per_hidden_layer = qtd_neuron_per_hidden_layer\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.b = Parameter(torch.tensor(\n",
    "            b), requires_grad=False).to(self.device)\n",
    "        self.n = Parameter(torch.tensor(\n",
    "            n), requires_grad=False).to(self.device)\n",
    "        self.s = Parameter(torch.tensor(\n",
    "            s), requires_grad=False).to(self.device)\n",
    "\n",
    "        self.hidden_layers = []\n",
    "        self.output_layers = []\n",
    "\n",
    "        self.hidden_layers.append(\n",
    "            nn.Linear(features_size, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        for i in range(max_num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(\n",
    "                nn.Linear(qtd_neuron_per_hidden_layer, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        for i in range(max_num_hidden_layers):\n",
    "            self.output_layers.append(\n",
    "                nn.Linear(qtd_neuron_per_hidden_layer, n_classes))\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList(self.hidden_layers).to(self.device)\n",
    "        self.output_layers = nn.ModuleList(self.output_layers).to(self.device)\n",
    "\n",
    "        self.alpha = Parameter(torch.Tensor(self.max_num_hidden_layers).fill_(1 / (self.max_num_hidden_layers + 1)),\n",
    "                               requires_grad=False).to(\n",
    "            self.device)\n",
    "\n",
    "        self.loss_array = []\n",
    "        self.tp=0.1\n",
    "        self.tn=0.1\n",
    "        self.fp=0.1\n",
    "        self.fn=0.1\n",
    "        self.bal_acc = 0\n",
    "        self.bal_acc_list = []\n",
    "        self.sen = 0\n",
    "        self.spec = 0\n",
    "        \n",
    "        self.protected_pos = 0.1\n",
    "        self.protected_neg = 0.1\n",
    "        self.non_protected_pos = 0.1\n",
    "        self.non_protected_neg = 0.1\n",
    "        self.stp_score=0\n",
    "        \n",
    "        self.tp_protected = 0.1\n",
    "        self.fn_protected = 0.1\n",
    "        self.tp_non_protected = 0.1\n",
    "        self.fn_non_protected = 0.1\n",
    "        self.eqop_score=0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            self.output_layers[i].weight.grad.data.fill_(0)\n",
    "            self.output_layers[i].bias.grad.data.fill_(0)\n",
    "            self.hidden_layers[i].weight.grad.data.fill_(0)\n",
    "            self.hidden_layers[i].bias.grad.data.fill_(0)\n",
    "\n",
    "    def update_weights(self, X, Y, weight, show_loss, test):\n",
    "        Y = torch.from_numpy(Y).to(self.device)\n",
    "        if test=='yes':\n",
    "            prediction_1 = self.predict_1(X)\n",
    "            self.update_eval_metrics(prediction_1,Y)\n",
    "            self.update_stp_score(prediction_1,X)\n",
    "            self.update_eqop_score(prediction_1,X,Y)\n",
    "        \n",
    "        predictions_per_layer = self.forward(X)\n",
    "        losses_per_layer = []\n",
    "\n",
    "        for out in predictions_per_layer:\n",
    "            criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "            loss = criterion(out.view(self.batch_size, self.n_classes), Y.view(\n",
    "                self.batch_size).long())\n",
    "            losses_per_layer.append(loss*weight)\n",
    "\n",
    "        w = [None] * len(losses_per_layer)\n",
    "        b = [None] * len(losses_per_layer)\n",
    "        \n",
    "        with torch.no_grad():     #Context-manager that disabled gradient calculation\n",
    "            self.weights_output_layers = list()\n",
    "            self.biases_output_layers = list()\n",
    "            for i in range(len(losses_per_layer)):\n",
    "                losses_per_layer[i].backward(retain_graph=True)\n",
    "                self.output_layers[i].weight.data -= self.n * \\\n",
    "                                                   self.alpha[i] * self.output_layers[i].weight.grad.data\n",
    "                self.output_layers[i].bias.data -= self.n * \\\n",
    "                                                 self.alpha[i] * self.output_layers[i].bias.grad.data\n",
    "                self.weights_output_layers.append(self.output_layers[i].weight.data) \n",
    "                self.biases_output_layers.append(self.output_layers[i].bias.data)\n",
    "                \n",
    "                for j in range(i + 1):\n",
    "                    if w[j] is None:\n",
    "                        w[j] = self.alpha[i] * self.hidden_layers[j].weight.grad.data\n",
    "                        b[j] = self.alpha[i] * self.hidden_layers[j].bias.grad.data\n",
    "                    else:\n",
    "                        w[j] += self.alpha[i] * self.hidden_layers[j].weight.grad.data\n",
    "                        b[j] += self.alpha[i] * self.hidden_layers[j].bias.grad.data\n",
    "                self.zero_grad()\n",
    "            self.weights_hidden_layers= list()\n",
    "            self.biases_hidden_layers = list()\n",
    "            \n",
    "            for i in range(len(losses_per_layer)):\n",
    "                self.hidden_layers[i].weight.data -= self.n * w[i]\n",
    "                self.hidden_layers[i].bias.data -= self.n * b[i]\n",
    "                self.weights_hidden_layers.append(self.hidden_layers[i].weight.data)\n",
    "                self.biases_hidden_layers.append(self.hidden_layers[i].bias.data)\n",
    "            for i in range(len(losses_per_layer)):\n",
    "                self.alpha[i] *= torch.pow(self.b, losses_per_layer[i])\n",
    "                self.alpha[i] = torch.max(\n",
    "                  self.alpha[i], self.s / self.max_num_hidden_layers)\n",
    "\n",
    "        z_t = torch.sum(self.alpha)\n",
    "\n",
    "        self.alpha = Parameter(\n",
    "            self.alpha / z_t, requires_grad=False).to(self.device)\n",
    "        if show_loss:\n",
    "            \n",
    "            real_output = torch.sum(torch.mul(\n",
    "                self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, self.batch_size).view(\n",
    "                    self.max_num_hidden_layers, self.batch_size, 1), predictions_per_layer), 0)\n",
    "            criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "            loss = criterion(real_output.view(self.batch_size, self.n_classes), Y.view(self.batch_size).long())\n",
    "            self.loss_array.append(loss)\n",
    "            if (len(self.loss_array) % 1000) == 0:\n",
    "                #print(\"WARNING: Set 'show_loss' to 'False' when not debugging. \"\n",
    "                #      \"It will deteriorate the fitting performance.\")\n",
    "                loss = torch.Tensor(self.loss_array).mean().cpu().numpy()\n",
    "                #print(\"Alpha:\" + str(self.alpha.data.cpu().numpy()))\n",
    "                print(\"Training Loss: \" + str(loss))\n",
    "                print(\"Balanced accuracy: \" + str(self.bal_acc))\n",
    "                print(\"Sensitivity: \" + str(self.sen))\n",
    "                print(\"Specificity: \" + str(self.spec))\n",
    "                print(\"Stp score: \" + str(self.stp_score))\n",
    "                print(\"Eqop score: \" + str(self.eqop_score))\n",
    "                self.loss_array.clear()\n",
    "\n",
    "    def get_weights(self,network_name):\n",
    "        \n",
    "        if network_name == 'global':\n",
    "            weights_output_layers = list()\n",
    "            biases_output_layers = list()\n",
    "            weights_hidden_layers = list()\n",
    "            biases_hidden_layers= list()\n",
    "            for i in range(self.max_num_hidden_layers):\n",
    "                weights_output_layers.append(self.output_layers[i].weight.data) \n",
    "                biases_output_layers.append(self.output_layers[i].bias.data)\n",
    "                weights_hidden_layers.append(self.hidden_layers[i].weight.data)\n",
    "                biases_hidden_layers.append(self.hidden_layers[i].bias.data)\n",
    "               \n",
    "            return self.alpha, weights_output_layers, biases_output_layers, weights_hidden_layers, biases_hidden_layers\n",
    "        else:\n",
    "            return self.alpha, self.weights_output_layers, self.biases_output_layers, self.weights_hidden_layers, self.biases_hidden_layers\n",
    "    \n",
    "    def set_weights(self, alpha, w_output_layer, b_output_layer, w_hidden_layer, b_hidden_layer):\n",
    "        \n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            self.alpha[i] = alpha[i].clone().detach()\n",
    "            self.output_layers[i].weight.data = w_output_layer[i].clone().detach()\n",
    "            self.output_layers[i].bias.data = b_output_layer [i].clone().detach()\n",
    "            self.hidden_layers[i].weight.data =  w_hidden_layer [i].clone().detach()\n",
    "            self.hidden_layers[i].bias.data = b_hidden_layer [i].clone().detach()    \n",
    "    def forward(self, X):\n",
    "        hidden_connections = []\n",
    "\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "\n",
    "        x = F.relu(self.hidden_layers[0](X))\n",
    "        hidden_connections.append(x)\n",
    "\n",
    "        for i in range(1, self.max_num_hidden_layers):\n",
    "            hidden_connections.append(\n",
    "                F.relu(self.hidden_layers[i](hidden_connections[i - 1])))\n",
    "\n",
    "        output_class = []\n",
    "\n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            output_class.append(self.output_layers[i](hidden_connections[i]))\n",
    "\n",
    "        pred_per_layer = torch.stack(output_class)\n",
    "\n",
    "        return pred_per_layer\n",
    "\n",
    "    def validate_input_X(self, data):\n",
    "        \n",
    "        if len(data.shape) != 2:\n",
    "            raise Exception(\n",
    "                \"Wrong dimension for this X data. It should have only two dimensions.\")\n",
    "\n",
    "    def validate_input_Y(self, data):\n",
    "        if len(data.shape) != 1:\n",
    "            raise Exception(\n",
    "                \"Wrong dimension for this Y data. It should have only one dimensions.\")\n",
    "\n",
    "    def partial_fit_(self, X_data, Y_data, weight, show_loss=True, test='yes'):\n",
    "        \n",
    "        self.validate_input_X(X_data)\n",
    "        self.validate_input_Y(Y_data)\n",
    "        self.update_weights(X_data, Y_data, weight, show_loss, test)\n",
    "\n",
    "    def partial_fit(self, X_data, Y_data, weight, show_loss=True, test='yes'):\n",
    "        \n",
    "        self.partial_fit_(X_data, Y_data, weight, show_loss, test)\n",
    "\n",
    "    def reset_eval_metrics(self):\n",
    "        self.tp=0.1\n",
    "        self.tn=0.1\n",
    "        self.fp=0.1\n",
    "        self.fn=0.1\n",
    "        self.bal_acc = 0\n",
    "        self.bal_acc_list = []\n",
    "        self.sen = 0\n",
    "        self.spec = 0\n",
    "        \n",
    "        self.protected_pos = 0.1\n",
    "        self.protected_neg = 0.1\n",
    "        self.non_protected_pos = 0.1\n",
    "        self.non_protected_neg = 0.1\n",
    "        self.stp_score=0\n",
    "        \n",
    "        self.tp_protected = 0.1\n",
    "        self.fn_protected = 0.1\n",
    "        self.tp_non_protected = 0.1\n",
    "        self.fn_non_protected = 0.1\n",
    "        self.eqop_score=0\n",
    "    def update_eval_metrics(self,prediction_1,Y):\n",
    "        if prediction_1==1 and Y==1:\n",
    "            self.tp+=1\n",
    "        elif prediction_1==1 and Y==0:\n",
    "            self.fp+=1\n",
    "        elif prediction_1==0 and Y==1:\n",
    "            self.fn+=1\n",
    "        else:\n",
    "            self.tn+=1\n",
    "        \n",
    "        self.sen = self.tp/(self.tp + self.fn)\n",
    "        self.spec= self.tn/(self.tn + self.fp)\n",
    "        self.bal_acc = (self.sen + self.spec)/2\n",
    "        self.bal_acc_list.append(self.bal_acc)\n",
    "            \n",
    "    def update_stp_score(self,prediction_1,X):\n",
    "        if X[0][sa_index] == p_Group:\n",
    "            if prediction_1 == 1:\n",
    "                self.protected_pos += 1.\n",
    "            else:\n",
    "                self.protected_neg += 1.\n",
    "        else:\n",
    "            if prediction_1 == 1:\n",
    "                self.non_protected_pos += 1.\n",
    "            else:\n",
    "                self.non_protected_neg += 1.\n",
    "            \n",
    "        C_prot = (self.protected_pos) / (self.protected_pos + self.protected_neg)\n",
    "        C_non_prot = (self.non_protected_pos) / (self.non_protected_pos + self.non_protected_neg)\n",
    "\n",
    "        self.stp_score = C_non_prot - C_prot\n",
    "    \n",
    "    def update_eqop_score(self,prediction_1,X,Y):\n",
    "        if X[0][sa_index] == p_Group:\n",
    "            if prediction_1==1 and Y==1:\n",
    "                self.tp_protected += 1.\n",
    "            elif prediction_1==0 and Y==1:\n",
    "                self.fn_protected += 1.\n",
    "        else:\n",
    "            if prediction_1==1 and Y==1:\n",
    "                self.tp_non_protected += 1.\n",
    "            elif prediction_1==0 and Y==1:\n",
    "                self.fn_non_protected += 1.\n",
    "            \n",
    "        tpr_protected = self.tp_protected / (self.tp_protected + self.fn_protected)\n",
    "        tpr_non_protected = self.tp_non_protected / (self.tp_non_protected + self.fn_non_protected)\n",
    "        self.eqop_score = tpr_non_protected - tpr_protected\n",
    "    \n",
    "    def predict_1(self, X_data):\n",
    "        self.validate_input_X(X_data)\n",
    "        return torch.argmax(torch.sum(torch.mul(\n",
    "            self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, 1).view(\n",
    "                self.max_num_hidden_layers, len(X_data), 1), self.forward(X_data)), 0), dim=1).cpu().numpy()\n",
    "    def predict_(self, X_data):\n",
    "        self.validate_input_X(X_data)\n",
    "        return torch.argmax(torch.sum(torch.mul(\n",
    "            self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, len(X_data)).view(\n",
    "                self.max_num_hidden_layers, len(X_data), 1), self.forward(X_data)), 0), dim=1).cpu().numpy()\n",
    "\n",
    "    def predict(self, X_data):\n",
    "        pred = self.predict_(X_data)\n",
    "        return pred\n",
    "\n",
    "    def export_params_to_json(self):\n",
    "        state_dict = self.state_dict()\n",
    "        params_gp = {}\n",
    "        for key, tensor in state_dict.items():\n",
    "            params_gp[key] = tensor.cpu().numpy().tolist()\n",
    "\n",
    "        return json.dumps(params_gp)\n",
    "\n",
    "    def load_params_from_json(self, json_data):\n",
    "        params = json.loads(json_data)\n",
    "        o_dict = collections.OrderedDict()\n",
    "        for key, tensor in params.items():\n",
    "            o_dict[key] = torch.tensor(tensor).to(self.device)\n",
    "        self.load_state_dict(o_dict)\n",
    "\n",
    "\n",
    "class ONN_THS(ONN):\n",
    "    def __init__(self, features_size, max_num_hidden_layers, qtd_neuron_per_hidden_layer, n_classes, b=0.99, n=0.01,\n",
    "                 s=0.2, e=[0.5, 0.35, 0.2, 0.1, 0.05], use_cuda=False):\n",
    "        super().__init__(features_size, max_num_hidden_layers, qtd_neuron_per_hidden_layer, n_classes, b=b, n=n, s=s,\n",
    "                         use_cuda=use_cuda)\n",
    "        self.e = Parameter(torch.tensor(e), requires_grad=False)\n",
    "        self.arms_values = Parameter(\n",
    "            torch.arange(n_classes), requires_grad=False)\n",
    "        self.explorations_mab = []\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            self.explorations_mab.append(algs.ThompsomSampling(len(e)))\n",
    "\n",
    "    def partial_fit(self, X_data, Y_data, exp_factor, show_loss=True):\n",
    "        self.partial_fit_(X_data, Y_data, show_loss)\n",
    "        self.explorations_mab[Y_data[0]].reward(exp_factor)\n",
    "\n",
    "    def predict(self, X_data):\n",
    "        pred = self.predict_(X_data)[0]\n",
    "        exp_factor = self.explorations_mab[pred].select()[0]\n",
    "        if np.random.uniform() < self.e[exp_factor]:\n",
    "            removed_arms = self.arms_values.clone().numpy().tolist()\n",
    "            removed_arms.remove(pred)\n",
    "            return random.choice(removed_arms), exp_factor\n",
    "\n",
    "        return pred, exp_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30aa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from imblearn.datasets import make_imbalance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e47b4179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib2\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import preprocessing\n",
    "from random import seed, shuffle\n",
    "\n",
    "# import utils as ut\n",
    "\n",
    "SEED = 1234\n",
    "seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "def load_adult():\n",
    "    FEATURES_CLASSIFICATION = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\n",
    "                               \"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\"] #features to be used for classification\n",
    "    CONT_VARIABLES = [\"age\",\"fnlwgt\",\"education-num\",\"capital-gain\",\"capital-loss\",\"hours-per-week\"] # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"y\" # the decision variable\n",
    "    SENSITIVE_ATTRS = [\"sex\"]\n",
    "    \n",
    "    COMPAS_INPUT_FILE = \"./datasets/adult2.csv\"\n",
    "\n",
    "    # load the data and get some stats\n",
    "    df = pd.read_csv(COMPAS_INPUT_FILE)\n",
    "    df.dropna()\n",
    "    # convert to np arra\n",
    "    data = df.to_dict('list')\n",
    "\n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "    # convert class label 0 to -1\n",
    "    y = data[CLASS_FEATURE]\n",
    "    y[y==0] = -1\n",
    "    X = np.array([]).reshape(len(y), 0) # empty array with num rows same as num examples, will hstack the features to it\n",
    "    x_control = defaultdict(list)\n",
    "    \n",
    "    feature_names = []\n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        vals = data[attr]\n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            vals = preprocessing.scale(vals) # 0 mean and 1 variance\n",
    "            vals = np.reshape(vals, (len(y), -1)) # convert from 1-d arr to a 2-d arr with one col\n",
    "        \n",
    "        else: # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            lb = preprocessing.LabelBinarizer()   #LabelEncoder() # Label Encoder\n",
    "            lb.fit(vals)\n",
    "            vals = lb.transform(vals)\n",
    "            #vals = np.reshape(vals, (len(y), -1))\n",
    "            #if attr ==\"sex\": \n",
    "            #    print(lb.classes_)\n",
    "            #    print(lb.transform(lb.classes_))\n",
    "        \n",
    "        # add to sensitive features dict\n",
    "        if attr in SENSITIVE_ATTRS:\n",
    "            x_control[attr] = vals\n",
    "\n",
    "        # add to learnable features\n",
    "        X = np.hstack((X, vals))\n",
    "\n",
    "        if attr in CONT_VARIABLES: # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else: # categorical features\n",
    "            if vals.shape[1] == 1: # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in lb.classes_: # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "\n",
    "    # convert the sensitive feature to 1-d array\n",
    "    x_control = dict(x_control)\n",
    "    for k in x_control.keys():\n",
    "        assert(x_control[k].shape[1] == 1) # make sure that the sensitive feature is binary after one hot encoding\n",
    "        x_control[k] = np.array(x_control[k]).flatten()\n",
    "\n",
    "    \n",
    "    \n",
    "    return X, y, feature_names.index(SENSITIVE_ATTRS[0]), 0, x_control\n",
    "\n",
    "\n",
    "X_1,y, sa_index, p_Group, x_control= load_adult()\n",
    "\n",
    "#print(X[0])\n",
    "#print(X[0][1])\n",
    "#print(sa_index)\n",
    "np_Group = 1 #non-protected group's sa_value\n",
    "Y_1 = []\n",
    "for i in y:\n",
    "    if (i == -1):\n",
    "        Y_1.append(0)\n",
    "    else:\n",
    "        Y_1.append(1)\n",
    "Y_1 = np.array(Y_1)\n",
    "from sklearn.utils import shuffle\n",
    "X_1, Y_1 = shuffle(X_1, Y_1)\n",
    "X, x_test, Y, y_test = train_test_split(X_1,Y_1,test_size=0.2)\n",
    "X, Y = X_1, Y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be6c708b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37155"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.unique(Y_1))\n",
    "list(Y_1).count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "520200fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(data, predict, k):\n",
    "    #k=8\n",
    "    #if len(data) >= k:\n",
    "    #    warnings.warn('K is set to a value less than total voting groups!')\n",
    "\n",
    "    distances = []\n",
    "    count = 0\n",
    "    for sample in data:\n",
    "        euclidean_distance = np.linalg.norm(np.array(sample)-np.array(predict))\n",
    "        distances.append([euclidean_distance,count])\n",
    "        count+=1\n",
    "    \n",
    "    votes = [i[1] for i in sorted(distances)[:k]] ##votes is returning indexes of k random samples\n",
    "\n",
    "    #vote_result = Counter(votes).most_common(9)[0][0]\n",
    "    return votes\n",
    "def fair_kSMOTE_algo_2(dmajor,dminor,k,r):\n",
    "    S = []\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    #Ns = 12\n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    \n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    i = 0\n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,k) #from minority-p\n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            \n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / (len(N)-1) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return S\n",
    "\n",
    "def fair_kSMOTE(dmajor,dminor_wg,dminor,k,r):\n",
    "    S = []\n",
    "    #Ns =  int(r*(len(dmajor) - len(dminor)))\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    #Ns = 12\n",
    "    \n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    #if r==-1:\n",
    "    #    Nks = 1\n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    \n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    i = 0\n",
    "    \n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,int(k/2)+1) #from minority-p\n",
    "        N2= k_nearest_neighbors(dminor_wg,xb,int(k/2)) #from minority-np\n",
    "    \n",
    "        N3 = np.hstack((N, N2))\n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                \n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "            j = 0\n",
    "            while(j < len(N2)):\n",
    "                #here on random xb\n",
    "                ind = N2[j]\n",
    "                \n",
    "                x_new = x_new + ((dminor_wg[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1    \n",
    "            x_new = x_new / (len(N3)-1) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "   \n",
    "    return S\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def splitYtrain_sa_value(Xtr,Ytr,minority_lable,majority_label,pp_Group=p_Group,npp_Group=np_Group): #splite Ytrain based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x = []\n",
    "    dmaj_np_x = []\n",
    "    dmin_p_x = []\n",
    "    dmin_np_x = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==pp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x.append(Xtr[i])\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==npp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==pp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==npp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x.append(Xtr[i])\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x\n",
    "\n",
    "\n",
    "\n",
    "#create_synth_data(window, window_label, minority_label,majority_label,5,lambda_score, 'min_p')\n",
    "def create_synth_data(x, y, minority_lable,majority_label,k,r,group,pp_group,npp_group):\n",
    "    \n",
    "    \n",
    "    dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x = splitYtrain_sa_value(x,y,minority_lable,majority_label, pp_group,npp_group)\n",
    "    '''\n",
    "    print(\"length of dmin_p_x: \" + str(len(dmin_p_x)))\n",
    "    print(\"length of dmin_np_x: \" + str(len(dmin_np_x)))\n",
    "    print(\"length of dmaj_p_x: \" + str(len(dmaj_p_x)))\n",
    "    print(\"length of dmaj_np_x: \" + str(len(dmaj_np_x)))\n",
    "    '''\n",
    "    if len(dmin_p_x)<4:\n",
    "        return -1, -1\n",
    "    group_names = ['dmin_p_x', 'dmin_np_x', 'dmaj_p_x', 'dmaj_np_x']\n",
    "    \n",
    "    Xtr_new = []\n",
    "    Ytr_new = []  \n",
    "    \n",
    "    \n",
    "    ##Algo 3:\n",
    "    \n",
    "    if group =='min_p':\n",
    "        dmaj_x = dmaj_p_x\n",
    "        dmin_x = dmin_p_x\n",
    "        \n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE(dmaj_x,dmin_np_x,dmin_x,k,r)\n",
    "        #x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = minority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "        \n",
    "    elif group =='maj_np':\n",
    "        dmaj_x = dmin_np_x\n",
    "        dmin_x = dmaj_np_x\n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        \n",
    "        \n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = majority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return Xtr_new,Ytr_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5fe353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, Y = make_classification(n_samples=50000, n_features=8, n_informative=4, n_redundant=0, n_classes=2,\n",
    "#                           n_clusters_per_class=1, class_sep=3)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, shuffle=True)\n",
    "X_train, y_train= X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc23026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_update_classSize(train_data, label,classSize):\n",
    "    theta = 0.9\n",
    "    if (label not in classSize):\n",
    "        up_dict = {train_data[-1]:0.5}\n",
    "        classSize.update(up_dict)\n",
    "    for classValue in classSize:\n",
    "        if classValue == label:\n",
    "            update = theta * classSize.get(classValue) + (1-theta)\n",
    "            classSize[classValue] = update\n",
    "        else:\n",
    "            update = theta * classSize.get(classValue)\n",
    "            classSize[classValue] = update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3b99015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_clients(instances, labels, num_clients, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(instances, labels))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ea09d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16280\n",
      "16280\n",
      "16280\n",
      "{'client_1': 0, 'client_2': 0, 'client_3': 0}\n"
     ]
    }
   ],
   "source": [
    "from skmultiflow.drift_detection.eddm import EDDM\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "clients = create_clients(X_train, y_train, num_clients=3, initial='client')\n",
    "client_index = {}\n",
    "client_window = {}\n",
    "client_window_label = {}\n",
    "client_eddm = {}\n",
    "\n",
    "for (client_name, data) in clients.items():\n",
    "    data, label = zip(*data)\n",
    "    Y = np.asarray(label)\n",
    "    X = np.asarray(data)\n",
    "    client_index.update({client_name:0})\n",
    "    client_window.update({client_name:[]})\n",
    "    client_window_label.update({client_name:[]})\n",
    "    length = len(data)\n",
    "    print(len(data))\n",
    "#client_eddm.update({'client_1':eddm1})\n",
    "#client_eddm.update({'client_2':eddm2})\n",
    "#client_eddm.update({'client_3':eddm3})\n",
    "print(client_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef314838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14d59b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16280\n",
      "{0: 12333, 1: 3947}\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "client_name = 'client_3'\n",
    "for i in range(len(clients[client_name])):\n",
    "    labels.append(clients[client_name][i][1])\n",
    "print(len(labels))\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "count_ap_dict = dict(zip(unique, counts))\n",
    "print(count_ap_dict)\n",
    "minority_label=1\n",
    "majority_label = 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c4f1d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_1\n",
      "[0 1]\n",
      "client_2\n",
      "[0 1]\n",
      "client_3\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "for (client_name, data) in clients.items():\n",
    "    print(client_name)\n",
    "    data, label = zip(*data)\n",
    "    Y = np.asarray(label)\n",
    "    X = np.asarray(data)\n",
    "    print(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8731e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport warnings\\nwarnings.simplefilter(action=\\'ignore\\', category=FutureWarning)\\nfrom skmultiflow.drift_detection.eddm import EDDM\\nfrom sklearn.utils import shuffle\\nfrom sklearn.utils.class_weight import compute_class_weight\\n\\nnum_clients = 3\\n#adwin = ADWIN(delta =1)\\nwindow=[]\\nwindow_label = []\\nwindow_warning = []\\nwindow_label_warning = []\\npos_assigned=0\\npos_samples = 0\\nneg_samples = 0\\npos_syn_samples = 0\\nneg_syn_samples = 0\\ngenerated_samples_per_sample = 0\\nimbalance_ratio = 0 #of window\\nminority_label=1\\nmajority_label = 0\\nlambda_initial=0.1\\n###\\nocis = 0\\nclassSize  = {}\\nclass_weights_dict = {}\\nlabels = []\\n###\\nj =0\\nchange=0\\nwarning=0\\n\\neddm1 = EDDM()\\neddm2 = EDDM()\\neddm3 = EDDM()\\n\\n#one global network\\nglobal_network = ONN(features_size=107, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\\n\\n#3 networks for three clients\\nonn_network_1 = ONN(features_size=107, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\\nonn_network_2 = ONN(features_size=107, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\\nonn_network_3 = ONN(features_size=107, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\\nweight = 1\\nfor _ in range(length):\\n    sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer = [],[],[],[]\\n    sum_alpha = []\\n\\n    for (client_name, data) in clients.items():\\n        added_points = 0\\n        data, label = zip(*data)\\n        Y = np.asarray(label)\\n        X = np.asarray(data)\\n        \\n        if client_name==\\'client_1\\':\\n            eddm = eddm1\\n            onn_network = onn_network_1\\n        elif client_name==\\'client_2\\':\\n            eddm = eddm2\\n            onn_network = onn_network_2\\n        else:\\n            eddm = eddm3\\n            onn_network = onn_network_3\\n        i = client_index[client_name]\\n        if i ==0:\\n            print(client_index)\\n        else:\\n            if i%1000==0:\\n                galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer = global_network.get_weights(\\'global\\')\\n                onn_network.set_weights(galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer)\\n        \\n        if np.size(client_window[client_name])!=0:\\n            if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\\n                majority_count = list(client_window_label[client_name]).count(0)\\n                minority_count = list(client_window_label[client_name]).count(1)\\n                if majority_count >  minority_count and minority_count!=0:\\n                    weight = int(majority_count/minority_count)\\n        if Y[i]==minority_label:\\n            onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),weight)\\n            \\n        else:\\n            onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),1)\\n        \\n        \\n        \\n        #onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]))\\n    \\n        if np.size(client_window[client_name])==0:\\n            client_window[client_name] = np.array(X[i])\\n            client_window_label[client_name] = np.array(Y[i])\\n        else:\\n            client_window[client_name]=np.vstack((client_window[client_name],np.array(X[i])))\\n            client_window_label[client_name]= np.vstack((client_window_label[client_name],np.array(Y[i])))\\n        eddm.add_element(Y[i])\\n    \\n\\n        if eddm.detected_change():\\n            print(\\'Change has been detected in data: \\' + str(Y[i]) + \\' - of index: \\' + str(i))\\n            change+=1\\n            client_window[client_name] = []\\n            client_window_label[client_name] = []\\n    \\n            \\n        pos_assigned = onn_network.tp+onn_network.fp-0.2\\n        pos_samples = onn_network.tp+onn_network.fn-0.2\\n        \\n        if np.size(client_window[client_name])!=0:\\n            if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\\n                if onn_network.stp_score > 0.05:\\n                    #print(onn_network.stp_score)\\n                    lambda_score = lambda_initial*(1+(onn_network.stp_score/0.2))\\n                    if pos_assigned <= pos_samples:\\n                        X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, \\'min_p\\', p_Group,np_Group)\\n                    else:\\n                        X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, \\'maj_np\\', p_Group,np_Group)\\n                    if X_syn!=-1:\\n                        Y_syn = np.array(Y_syn)\\n                        X_syn = np.array(X_syn)\\n                        X_syn, Y_syn = shuffle(X_syn, Y_syn, random_state=0)\\n                        added_points = len(Y_syn)\\n                        for k in range(len(X_syn)):\\n                            onn_network.partial_fit(np.asarray([X_syn[k, :]]), np.asarray([Y_syn[k]]), 1)\\n        client_alpha, client_w_output_layer, client_b_output_layer, client_w_hidden_layer, client_b_hidden_layer = onn_network.get_weights(\\'client\\')\\n        i=i+1\\n        client_index.update({client_name:i})\\n        #scaling_factor = (client_index[client_name]+added_points)/(sum(client_index.values())+added_points)\\n        scaling_factor = 1/5\\n        print(client_index)\\n        p = 0\\n        if p==0:\\n            if sum_alpha==[]:\\n                sum_alpha = torch.mul(client_alpha, scaling_factor)\\n                sum_w_output_layer = client_w_output_layer\\n                sum_b_output_layer = client_b_output_layer\\n                sum_w_hidden_layer = client_w_hidden_layer\\n                sum_b_hidden_layer = client_b_hidden_layer\\n            \\n            \\n            \\n            \\n                for j in range(onn_network.max_num_hidden_layers):\\n                    sum_w_output_layer[j] = torch.mul(client_w_output_layer[j], scaling_factor)\\n                    sum_b_output_layer[j] = torch.mul(client_b_output_layer[j], scaling_factor)\\n                    sum_w_hidden_layer[j] = torch.mul(client_w_hidden_layer[j], scaling_factor)\\n                    sum_b_hidden_layer[j] = torch.mul(client_b_hidden_layer[j], scaling_factor)\\n\\n            else:\\n                sum_alpha = torch.add(sum_alpha, torch.mul(client_alpha, scaling_factor))\\n                for j in range(onn_network.max_num_hidden_layers):\\n                    sum_w_output_layer[j] = torch.add(sum_w_output_layer[j],torch.mul(client_w_output_layer[j], scaling_factor)) \\n                    sum_b_output_layer[j] = torch.add(sum_b_output_layer[j],torch.mul(client_b_output_layer[j], scaling_factor))\\n                    sum_w_hidden_layer[j] = torch.add(sum_w_hidden_layer[j],torch.mul(client_w_hidden_layer[j], scaling_factor)) \\n                    sum_b_hidden_layer[j] = torch.add(sum_b_hidden_layer[j],torch.mul(client_b_hidden_layer[j], scaling_factor))\\n        \\n    if i%1000==0:    \\n        global_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer)\\nglobal_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer)            \\n        #global_network.set_weights(client_alpha, client_w_output_layer, client_b_output_layer, client_w_hidden_layer, client_b_hidden_layer)\\n        \\n        \\n        #print(\"change\" + str(change))\\n#print(\"warning\" + str(warning))\\n    \\n    \\n    \\n    \\n    \\nprint(\"Balanced accuracy: \" + str(onn_network.bal_acc))\\nprint(\"Sensitivity: \" + str(onn_network.sen))\\nprint(\"Specificity: \" + str(onn_network.spec))\\nprint(\"Stp score: \" + str(onn_network.stp_score))\\nprint(\"Eqop score: \" + str(onn_network.eqop_score))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from skmultiflow.drift_detection.eddm import EDDM\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "num_clients = 3\n",
    "#adwin = ADWIN(delta =1)\n",
    "window=[]\n",
    "window_label = []\n",
    "window_warning = []\n",
    "window_label_warning = []\n",
    "pos_assigned=0\n",
    "pos_samples = 0\n",
    "neg_samples = 0\n",
    "pos_syn_samples = 0\n",
    "neg_syn_samples = 0\n",
    "generated_samples_per_sample = 0\n",
    "imbalance_ratio = 0 #of window\n",
    "minority_label=1\n",
    "majority_label = 0\n",
    "lambda_initial=0.1\n",
    "###\n",
    "ocis = 0\n",
    "classSize  = {}\n",
    "class_weights_dict = {}\n",
    "labels = []\n",
    "###\n",
    "j =0\n",
    "change=0\n",
    "warning=0\n",
    "\n",
    "eddm1 = EDDM()\n",
    "eddm2 = EDDM()\n",
    "eddm3 = EDDM()\n",
    "\n",
    "#one global network\n",
    "global_network = ONN(features_size=107, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "\n",
    "#3 networks for three clients\n",
    "onn_network_1 = ONN(features_size=107, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "onn_network_2 = ONN(features_size=107, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "onn_network_3 = ONN(features_size=107, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "weight = 1\n",
    "for _ in range(length):\n",
    "    sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer = [],[],[],[]\n",
    "    sum_alpha = []\n",
    "\n",
    "    for (client_name, data) in clients.items():\n",
    "        added_points = 0\n",
    "        data, label = zip(*data)\n",
    "        Y = np.asarray(label)\n",
    "        X = np.asarray(data)\n",
    "        \n",
    "        if client_name=='client_1':\n",
    "            eddm = eddm1\n",
    "            onn_network = onn_network_1\n",
    "        elif client_name=='client_2':\n",
    "            eddm = eddm2\n",
    "            onn_network = onn_network_2\n",
    "        else:\n",
    "            eddm = eddm3\n",
    "            onn_network = onn_network_3\n",
    "        i = client_index[client_name]\n",
    "        if i ==0:\n",
    "            print(client_index)\n",
    "        else:\n",
    "            if i%1000==0:\n",
    "                galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer = global_network.get_weights('global')\n",
    "                onn_network.set_weights(galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer)\n",
    "        \n",
    "        if np.size(client_window[client_name])!=0:\n",
    "            if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\n",
    "                majority_count = list(client_window_label[client_name]).count(0)\n",
    "                minority_count = list(client_window_label[client_name]).count(1)\n",
    "                if majority_count >  minority_count and minority_count!=0:\n",
    "                    weight = int(majority_count/minority_count)\n",
    "        if Y[i]==minority_label:\n",
    "            onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),weight)\n",
    "            \n",
    "        else:\n",
    "            onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]))\n",
    "    \n",
    "        if np.size(client_window[client_name])==0:\n",
    "            client_window[client_name] = np.array(X[i])\n",
    "            client_window_label[client_name] = np.array(Y[i])\n",
    "        else:\n",
    "            client_window[client_name]=np.vstack((client_window[client_name],np.array(X[i])))\n",
    "            client_window_label[client_name]= np.vstack((client_window_label[client_name],np.array(Y[i])))\n",
    "        eddm.add_element(Y[i])\n",
    "    \n",
    "\n",
    "        if eddm.detected_change():\n",
    "            print('Change has been detected in data: ' + str(Y[i]) + ' - of index: ' + str(i))\n",
    "            change+=1\n",
    "            client_window[client_name] = []\n",
    "            client_window_label[client_name] = []\n",
    "    \n",
    "            \n",
    "        pos_assigned = onn_network.tp+onn_network.fp-0.2\n",
    "        pos_samples = onn_network.tp+onn_network.fn-0.2\n",
    "        \n",
    "        if np.size(client_window[client_name])!=0:\n",
    "            if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\n",
    "                if onn_network.stp_score > 0.05:\n",
    "                    #print(onn_network.stp_score)\n",
    "                    lambda_score = lambda_initial*(1+(onn_network.stp_score/0.2))\n",
    "                    if pos_assigned <= pos_samples:\n",
    "                        X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, 'min_p', p_Group,np_Group)\n",
    "                    else:\n",
    "                        X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, 'maj_np', p_Group,np_Group)\n",
    "                    if X_syn!=-1:\n",
    "                        Y_syn = np.array(Y_syn)\n",
    "                        X_syn = np.array(X_syn)\n",
    "                        X_syn, Y_syn = shuffle(X_syn, Y_syn, random_state=0)\n",
    "                        added_points = len(Y_syn)\n",
    "                        for k in range(len(X_syn)):\n",
    "                            onn_network.partial_fit(np.asarray([X_syn[k, :]]), np.asarray([Y_syn[k]]), 1)\n",
    "        client_alpha, client_w_output_layer, client_b_output_layer, client_w_hidden_layer, client_b_hidden_layer = onn_network.get_weights('client')\n",
    "        i=i+1\n",
    "        client_index.update({client_name:i})\n",
    "        #scaling_factor = (client_index[client_name]+added_points)/(sum(client_index.values())+added_points)\n",
    "        scaling_factor = 1/5\n",
    "        print(client_index)\n",
    "        p = 0\n",
    "        if p==0:\n",
    "            if sum_alpha==[]:\n",
    "                sum_alpha = torch.mul(client_alpha, scaling_factor)\n",
    "                sum_w_output_layer = client_w_output_layer\n",
    "                sum_b_output_layer = client_b_output_layer\n",
    "                sum_w_hidden_layer = client_w_hidden_layer\n",
    "                sum_b_hidden_layer = client_b_hidden_layer\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                for j in range(onn_network.max_num_hidden_layers):\n",
    "                    sum_w_output_layer[j] = torch.mul(client_w_output_layer[j], scaling_factor)\n",
    "                    sum_b_output_layer[j] = torch.mul(client_b_output_layer[j], scaling_factor)\n",
    "                    sum_w_hidden_layer[j] = torch.mul(client_w_hidden_layer[j], scaling_factor)\n",
    "                    sum_b_hidden_layer[j] = torch.mul(client_b_hidden_layer[j], scaling_factor)\n",
    "\n",
    "            else:\n",
    "                sum_alpha = torch.add(sum_alpha, torch.mul(client_alpha, scaling_factor))\n",
    "                for j in range(onn_network.max_num_hidden_layers):\n",
    "                    sum_w_output_layer[j] = torch.add(sum_w_output_layer[j],torch.mul(client_w_output_layer[j], scaling_factor)) \n",
    "                    sum_b_output_layer[j] = torch.add(sum_b_output_layer[j],torch.mul(client_b_output_layer[j], scaling_factor))\n",
    "                    sum_w_hidden_layer[j] = torch.add(sum_w_hidden_layer[j],torch.mul(client_w_hidden_layer[j], scaling_factor)) \n",
    "                    sum_b_hidden_layer[j] = torch.add(sum_b_hidden_layer[j],torch.mul(client_b_hidden_layer[j], scaling_factor))\n",
    "        \n",
    "    if i%1000==0:    \n",
    "        global_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer)\n",
    "global_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer)            \n",
    "        #global_network.set_weights(client_alpha, client_w_output_layer, client_b_output_layer, client_w_hidden_layer, client_b_hidden_layer)\n",
    "        \n",
    "        \n",
    "        #print(\"change\" + str(change))\n",
    "#print(\"warning\" + str(warning))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Balanced accuracy: \" + str(onn_network.bal_acc))\n",
    "print(\"Sensitivity: \" + str(onn_network.sen))\n",
    "print(\"Specificity: \" + str(onn_network.spec))\n",
    "print(\"Stp score: \" + str(onn_network.stp_score))\n",
    "print(\"Eqop score: \" + str(onn_network.eqop_score))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d95c76a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a341b515ae17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'client_1'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0meddm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meddm1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from skmultiflow.drift_detection.eddm import EDDM\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "num_clients = 3\n",
    "#adwin = ADWIN(delta =1)\n",
    "window=[]\n",
    "window_label = []\n",
    "window_warning = []\n",
    "window_label_warning = []\n",
    "pos_assigned=0\n",
    "pos_samples = 0\n",
    "neg_samples = 0\n",
    "pos_syn_samples = 0\n",
    "neg_syn_samples = 0\n",
    "generated_samples_per_sample = 0\n",
    "imbalance_ratio = 0 #of window\n",
    "minority_label=1\n",
    "majority_label = 0\n",
    "lambda_initial=0.05\n",
    "###\n",
    "ocis = 0\n",
    "classSize  = {}\n",
    "class_weights_dict = {}\n",
    "labels = []\n",
    "###\n",
    "j =0\n",
    "change=0\n",
    "warning=0\n",
    "\n",
    "\n",
    "bal_acc_global=[] \n",
    "stp_score_global=[]\n",
    "eddm1 = EDDM()\n",
    "eddm2 = EDDM()\n",
    "eddm3 = EDDM()\n",
    "\n",
    "#one global network\n",
    "global_network = ONN(features_size=107, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "\n",
    "#3 networks for three clients\n",
    "onn_network_1 = ONN(features_size=107, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "onn_network_2 = ONN(features_size=107, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "onn_network_3 = ONN(features_size=107, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "weight = 1\n",
    "for _ in range(length):\n",
    "    sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer = [],[],[],[]\n",
    "    sum_alpha = []\n",
    "\n",
    "    for (client_name, data) in clients.items():\n",
    "        added_points = 0\n",
    "        data, label = zip(*data)\n",
    "        Y = np.asarray(label)\n",
    "        X = np.asarray(data)\n",
    "        print(m)\n",
    "        if client_name=='client_1':\n",
    "            eddm = eddm1\n",
    "            onn_network = onn_network_1\n",
    "        elif client_name=='client_2':\n",
    "            eddm = eddm2\n",
    "            onn_network = onn_network_2\n",
    "        else:\n",
    "            eddm = eddm3\n",
    "            onn_network = onn_network_3\n",
    "        i = client_index[client_name]\n",
    "        if i ==0:\n",
    "            print(client_index)\n",
    "        else:\n",
    "            if i%200==0:\n",
    "                galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer = global_network.get_weights('global')\n",
    "                onn_network.set_weights(galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer)\n",
    "        \n",
    "        if np.size(client_window[client_name])!=0:\n",
    "            if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\n",
    "                majority_count = list(client_window_label[client_name]).count(0)\n",
    "                minority_count = list(client_window_label[client_name]).count(1)\n",
    "                if majority_count >  minority_count and minority_count!=0:\n",
    "                    weight = int(majority_count/minority_count)\n",
    "        if Y[i]==minority_label:\n",
    "            onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),1)\n",
    "            \n",
    "        else:\n",
    "            onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]))\n",
    "    \n",
    "        if np.size(client_window[client_name])==0:\n",
    "            client_window[client_name] = np.array(X[i])\n",
    "            client_window_label[client_name] = np.array(Y[i])\n",
    "        else:\n",
    "            client_window[client_name]=np.vstack((client_window[client_name],np.array(X[i])))\n",
    "            client_window_label[client_name]= np.vstack((client_window_label[client_name],np.array(Y[i])))\n",
    "        eddm.add_element(Y[i])\n",
    "    \n",
    "\n",
    "        if eddm.detected_change():\n",
    "            print('Change has been detected in data: ' + str(Y[i]) + ' - of index: ' + str(i))\n",
    "            change+=1\n",
    "            client_window[client_name] = []\n",
    "            client_window_label[client_name] = []\n",
    "    \n",
    "            \n",
    "        pos_assigned = onn_network.tp+onn_network.fp-0.2\n",
    "        pos_samples = onn_network.tp+onn_network.fn-0.2\n",
    "        '''\n",
    "        if np.size(client_window[client_name])!=0:\n",
    "            if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\n",
    "                if onn_network.stp_score > 0.005:\n",
    "                    #print(onn_network.stp_score)\n",
    "                    lambda_score = lambda_initial*(1+(onn_network.stp_score/0.2))\n",
    "                    if pos_assigned <= pos_samples:\n",
    "                        X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, 'min_p', p_Group,np_Group)\n",
    "                    else:\n",
    "                        X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, 'maj_np', p_Group,np_Group)\n",
    "                    if X_syn!=-1:\n",
    "                        Y_syn = np.array(Y_syn)\n",
    "                        X_syn = np.array(X_syn)\n",
    "                        X_syn, Y_syn = shuffle(X_syn, Y_syn, random_state=0)\n",
    "                        added_points = len(Y_syn)\n",
    "                        for k in range(len(X_syn)):\n",
    "                            onn_network.partial_fit(np.asarray([X_syn[k, :]]), np.asarray([Y_syn[k]]), 1, test='no')\n",
    "        '''\n",
    "        client_alpha, client_w_output_layer, client_b_output_layer, client_w_hidden_layer, client_b_hidden_layer = onn_network.get_weights('client')\n",
    "        i=i+1\n",
    "        client_index.update({client_name:i})\n",
    "        scaling_factor = (client_index[client_name]+added_points)/(sum(client_index.values())+added_points)\n",
    "        #scaling_factor = 1/5\n",
    "        print(client_index)\n",
    "        p = 0\n",
    "        if p==0:\n",
    "            if sum_alpha==[]:\n",
    "                sum_alpha = torch.mul(client_alpha, scaling_factor)\n",
    "                sum_w_output_layer = client_w_output_layer\n",
    "                sum_b_output_layer = client_b_output_layer\n",
    "                sum_w_hidden_layer = client_w_hidden_layer\n",
    "                sum_b_hidden_layer = client_b_hidden_layer\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                for j in range(onn_network.max_num_hidden_layers):\n",
    "                    sum_w_output_layer[j] = torch.mul(client_w_output_layer[j], scaling_factor)\n",
    "                    sum_b_output_layer[j] = torch.mul(client_b_output_layer[j], scaling_factor)\n",
    "                    sum_w_hidden_layer[j] = torch.mul(client_w_hidden_layer[j], scaling_factor)\n",
    "                    sum_b_hidden_layer[j] = torch.mul(client_b_hidden_layer[j], scaling_factor)\n",
    "\n",
    "            else:\n",
    "                sum_alpha = torch.add(sum_alpha, torch.mul(client_alpha, scaling_factor))\n",
    "                for j in range(onn_network.max_num_hidden_layers):\n",
    "                    sum_w_output_layer[j] = torch.add(sum_w_output_layer[j],torch.mul(client_w_output_layer[j], scaling_factor)) \n",
    "                    sum_b_output_layer[j] = torch.add(sum_b_output_layer[j],torch.mul(client_b_output_layer[j], scaling_factor))\n",
    "                    sum_w_hidden_layer[j] = torch.add(sum_w_hidden_layer[j],torch.mul(client_w_hidden_layer[j], scaling_factor)) \n",
    "                    sum_b_hidden_layer[j] = torch.add(sum_b_hidden_layer[j],torch.mul(client_b_hidden_layer[j], scaling_factor))\n",
    "        \n",
    "    if i%200==0:    \n",
    "        global_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer)\n",
    "\n",
    "        x_test, y_test = shuffle(x_test, y_test, random_state=0)\n",
    "        \n",
    "        for m in range(len(x_test)-1):\n",
    "            prediction_1 = global_network.predict_1(np.asarray([x_test[m, :]]))\n",
    "            global_network.update_eval_metrics(prediction_1,np.asarray([y_test[m]]))\n",
    "            global_network.update_stp_score(prediction_1,np.asarray([x_test[m, :]]))\n",
    "            global_network.update_eqop_score(prediction_1,np.asarray([x_test[m, :]]),np.asarray([y_test[m]]))\n",
    "        \n",
    "        print(\"Balanced accuracy: \" + str(global_network.bal_acc))\n",
    "        print(\"Sensitivity: \" + str(global_network.sen))\n",
    "        print(\"Specificity: \" + str(global_network.spec))\n",
    "        print(\"Stp score: \" + str(global_network.stp_score))\n",
    "        print(\"Eqop score: \" + str(global_network.eqop_score))\n",
    "        \n",
    "        bal_acc_global.append(global_network.bal_acc)\n",
    "        stp_score_global.append(global_network.stp_score)\n",
    "        global_network.reset_eval_metrics()\n",
    "global_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer) \n",
    "for n in range(len(x_test)-1):\n",
    "    prediction_1 = global_network.predict_1(np.asarray([x_test[n, :]]))\n",
    "    global_network.update_eval_metrics(prediction_1,np.asarray([y_test[n]]))\n",
    "    global_network.update_stp_score(prediction_1,np.asarray([x_test[n, :]]))\n",
    "    global_network.update_eqop_score(prediction_1,np.asarray([x_test[n, :]]),np.asarray([y_test[n]]))\n",
    "\n",
    "bal_acc_global.append(global_network.bal_acc)  \n",
    "stp_score_global.append(global_network.stp_score)        #print(\"change\" + str(change))\n",
    "#print(\"warning\" + str(warning))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Balanced accuracy: \" + str(onn_network.bal_acc))\n",
    "print(\"Sensitivity: \" + str(onn_network.sen))\n",
    "print(\"Specificity: \" + str(onn_network.spec))\n",
    "print(\"Stp score: \" + str(onn_network.stp_score))\n",
    "print(\"Eqop score: \" + str(onn_network.eqop_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fbbc37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.26593338  0.          0.          0.          0.          1.\n",
      "  0.          0.          0.          0.         -0.68924342  0.\n",
      "  0.          0.          0.          1.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         -2.75310618  0.          0.\n",
      "  1.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  1.          1.         -0.14480353 -0.2171271  -0.03408696  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64a94b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy: 0.5000194276619242\n",
      "Sensitivity: 5.694112287894318e-05\n",
      "Specificity: 0.9999819142009694\n",
      "Gmean: 0.007545865957810158\n",
      "Stp score: -2.0594776046961945e-05\n",
      "Eqop score: -0.0002958592655860427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'BalanceACC')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "onn_network = global_network\n",
    "print(\"Balanced accuracy: \" + str(onn_network.bal_acc))\n",
    "print(\"Sensitivity: \" + str(onn_network.sen))\n",
    "print(\"Specificity: \" + str(onn_network.spec))\n",
    "print(\"Gmean: \" + str(math.sqrt(onn_network.sen*onn_network.spec)))\n",
    "print(\"Stp score: \" + str(onn_network.stp_score))\n",
    "print(\"Eqop score: \" + str(onn_network.eqop_score))\n",
    "plt.plot(bal_acc_global)\n",
    "plt.title('Global Server')\n",
    "plt.xlabel('Instances')\n",
    "plt.ylabel('BalanceACC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "483aa71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9769\n",
      "9769\n"
     ]
    }
   ],
   "source": [
    "print(len(x_test))\n",
    "print(len(y_test))\n",
    "x_test, y_test = shuffle(x_test, y_test, random_state=0)\n",
    "for i in range(len(x_test)-1):\n",
    "    prediction_1 = global_network.predict_1(np.asarray([x_test[i, :]]))\n",
    "    global_network.update_eval_metrics(prediction_1,np.asarray([y_test[i]]))\n",
    "    global_network.update_stp_score(prediction_1,np.asarray([x_test[i, :]]))\n",
    "    global_network.update_eqop_score(prediction_1,np.asarray([x_test[i, :]]),np.asarray([y_test[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca4283bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy: 0.6479251805102212\n",
      "Sensitivity: 0.3225054531457166\n",
      "Specificity: 0.9733449078747257\n",
      "Gmean: 0.5602758611445028\n",
      "Stp score: -0.0461729828398381\n",
      "Eqop score: -0.44564886632353884\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "onn_network = global_network\n",
    "print(\"Balanced accuracy: \" + str(onn_network.bal_acc))\n",
    "print(\"Sensitivity: \" + str(onn_network.sen))\n",
    "print(\"Specificity: \" + str(onn_network.spec))\n",
    "print(\"Gmean: \" + str(math.sqrt(onn_network.sen*onn_network.spec)))\n",
    "print(\"Stp score: \" + str(onn_network.stp_score))\n",
    "print(\"Eqop score: \" + str(onn_network.eqop_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54fe8488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy: 0.6479251805102212\n",
      "Sensitivity: 0.3225054531457166\n",
      "Specificity: 0.9733449078747257\n",
      "Gmean: 0.5602758611445028\n",
      "Stp score: -0.0461729828398381\n",
      "Eqop score: -0.44564886632353884\n"
     ]
    }
   ],
   "source": [
    "print(\"Balanced accuracy: \" + str(onn_network.bal_acc))\n",
    "print(\"Sensitivity: \" + str(onn_network.sen))\n",
    "print(\"Specificity: \" + str(onn_network.spec))\n",
    "print(\"Gmean: \" + str(math.sqrt(onn_network.sen*onn_network.spec)))\n",
    "print(\"Stp score: \" + str(onn_network.stp_score))\n",
    "print(\"Eqop score: \" + str(onn_network.eqop_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e8039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9db17b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer = [],[],[],[]\n",
    "sum_alpha = []\n",
    "scaling_factor = 1/5\n",
    "global_network2 = ONN(features_size=46, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "for m in range(3):\n",
    "    if m==0:\n",
    "        onn_network=onn_network_1\n",
    "    elif m==1:\n",
    "        onn_network=onn_network_2\n",
    "    else:\n",
    "        onn_network=onn_network_3\n",
    "    if sum_alpha==[]:\n",
    "        sum_alpha = torch.mul(client_alpha, scaling_factor)\n",
    "        sum_w_output_layer = client_w_output_layer\n",
    "        sum_b_output_layer = client_b_output_layer\n",
    "        sum_w_hidden_layer = client_w_hidden_layer\n",
    "        sum_b_hidden_layer = client_b_hidden_layer\n",
    "            \n",
    "        for j in range(onn_network.max_num_hidden_layers):\n",
    "            sum_w_output_layer[j] = torch.mul(client_w_output_layer[j], scaling_factor)\n",
    "            sum_b_output_layer[j] = torch.mul(client_b_output_layer[j], scaling_factor)\n",
    "            sum_w_hidden_layer[j] = torch.mul(client_w_hidden_layer[j], scaling_factor)\n",
    "            sum_b_hidden_layer[j] = torch.mul(client_b_hidden_layer[j], scaling_factor)\n",
    "\n",
    "    else:\n",
    "        sum_alpha = torch.add(sum_alpha, torch.mul(client_alpha, scaling_factor))\n",
    "        for j in range(onn_network.max_num_hidden_layers):\n",
    "            sum_w_output_layer[j] = torch.add(sum_w_output_layer[j],torch.mul(client_w_output_layer[j], scaling_factor)) \n",
    "            sum_b_output_layer[j] = torch.add(sum_b_output_layer[j],torch.mul(client_b_output_layer[j], scaling_factor))\n",
    "            sum_w_hidden_layer[j] = torch.add(sum_w_hidden_layer[j],torch.mul(client_w_hidden_layer[j], scaling_factor)) \n",
    "            sum_b_hidden_layer[j] = torch.add(sum_b_hidden_layer[j],torch.mul(client_b_hidden_layer[j], scaling_factor))\n",
    "        \n",
    "        \n",
    "global_network2.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cd9a0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "onn_network_4 = ONN(features_size=46, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer = onn_network_3.get_weights('local')\n",
    "onn_network_4.set_weights(galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbd622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
