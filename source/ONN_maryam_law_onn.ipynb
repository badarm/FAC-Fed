{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140e5580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from mab import algs\n",
    "\n",
    "\n",
    "class ONN(nn.Module):\n",
    "    def __init__(self, features_size, max_num_hidden_layers, qtd_neuron_per_hidden_layer, n_classes, batch_size=1,\n",
    "                 b=0.99, n=0.01, s=0.2, use_cuda=False):\n",
    "        super(ONN, self).__init__()\n",
    "\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            print(\"Using CUDA :]\")\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "\n",
    "        self.features_size = features_size\n",
    "        self.max_num_hidden_layers = max_num_hidden_layers\n",
    "        self.qtd_neuron_per_hidden_layer = qtd_neuron_per_hidden_layer\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.b = Parameter(torch.tensor(\n",
    "            b), requires_grad=False).to(self.device)\n",
    "        self.n = Parameter(torch.tensor(\n",
    "            n), requires_grad=False).to(self.device)\n",
    "        self.s = Parameter(torch.tensor(\n",
    "            s), requires_grad=False).to(self.device)\n",
    "\n",
    "        self.hidden_layers = []\n",
    "        self.output_layers = []\n",
    "\n",
    "        self.hidden_layers.append(\n",
    "            nn.Linear(features_size, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        for i in range(max_num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(\n",
    "                nn.Linear(qtd_neuron_per_hidden_layer, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        for i in range(max_num_hidden_layers):\n",
    "            self.output_layers.append(\n",
    "                nn.Linear(qtd_neuron_per_hidden_layer, n_classes))\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList(self.hidden_layers).to(self.device)\n",
    "        self.output_layers = nn.ModuleList(self.output_layers).to(self.device)\n",
    "\n",
    "        self.alpha = Parameter(torch.Tensor(self.max_num_hidden_layers).fill_(1 / (self.max_num_hidden_layers + 1)),\n",
    "                               requires_grad=False).to(\n",
    "            self.device)\n",
    "\n",
    "        self.loss_array = []\n",
    "        self.tp=0.1\n",
    "        self.tn=0.1\n",
    "        self.fp=0.1\n",
    "        self.fn=0.1\n",
    "        self.bal_acc = 0\n",
    "        self.sen = 0\n",
    "        self.spec = 0\n",
    "        \n",
    "        self.protected_pos = 0.1\n",
    "        self.protected_neg = 0.1\n",
    "        self.non_protected_pos = 0.1\n",
    "        self.non_protected_neg = 0.1\n",
    "        self.stp_score=0\n",
    "        \n",
    "        self.tp_protected = 0.1\n",
    "        self.fn_protected = 0.1\n",
    "        self.tp_non_protected = 0.1\n",
    "        self.fn_non_protected = 0.1\n",
    "        self.eqop_score=0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            self.output_layers[i].weight.grad.data.fill_(0)\n",
    "            self.output_layers[i].bias.grad.data.fill_(0)\n",
    "            self.hidden_layers[i].weight.grad.data.fill_(0)\n",
    "            self.hidden_layers[i].bias.grad.data.fill_(0)\n",
    "\n",
    "    def update_weights(self, X, Y, weight, show_loss):\n",
    "        Y = torch.from_numpy(Y).to(self.device)\n",
    "        prediction_1 = self.predict_1(X)\n",
    "        predictions_per_layer = self.forward(X)\n",
    "        \n",
    "        \n",
    "        self.update_eval_metrics(prediction_1,Y)\n",
    "        self.update_stp_score(prediction_1,X)\n",
    "        self.update_eqop_score(prediction_1,X,Y)\n",
    "        \n",
    "        losses_per_layer = []\n",
    "\n",
    "        for out in predictions_per_layer:\n",
    "            criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "            loss = criterion(out.view(self.batch_size, self.n_classes), Y.view(\n",
    "                self.batch_size).long())\n",
    "            losses_per_layer.append(loss*weight)\n",
    "\n",
    "        w = [None] * len(losses_per_layer)\n",
    "        b = [None] * len(losses_per_layer)\n",
    "        \n",
    "        with torch.no_grad():     #Context-manager that disabled gradient calculation\n",
    "\n",
    "          for i in range(len(losses_per_layer)):\n",
    "              losses_per_layer[i].backward(retain_graph=True)\n",
    "              self.output_layers[i].weight.data -= self.n * \\\n",
    "                                                   self.alpha[i] * self.output_layers[i].weight.grad.data\n",
    "              self.output_layers[i].bias.data -= self.n * \\\n",
    "                                                 self.alpha[i] * self.output_layers[i].bias.grad.data\n",
    "\n",
    "              for j in range(i + 1):\n",
    "                  if w[j] is None:\n",
    "                      w[j] = self.alpha[i] * self.hidden_layers[j].weight.grad.data\n",
    "                      b[j] = self.alpha[i] * self.hidden_layers[j].bias.grad.data\n",
    "                  else:\n",
    "                      w[j] += self.alpha[i] * self.hidden_layers[j].weight.grad.data\n",
    "                      b[j] += self.alpha[i] * self.hidden_layers[j].bias.grad.data\n",
    "\n",
    "              self.zero_grad()\n",
    "\n",
    "          for i in range(len(losses_per_layer)):\n",
    "              self.hidden_layers[i].weight.data -= self.n * w[i]\n",
    "              self.hidden_layers[i].bias.data -= self.n * b[i]\n",
    "\n",
    "          for i in range(len(losses_per_layer)):\n",
    "              self.alpha[i] *= torch.pow(self.b, losses_per_layer[i])\n",
    "              self.alpha[i] = torch.max(\n",
    "                  self.alpha[i], self.s / self.max_num_hidden_layers)\n",
    "\n",
    "        z_t = torch.sum(self.alpha)\n",
    "\n",
    "        self.alpha = Parameter(\n",
    "            self.alpha / z_t, requires_grad=False).to(self.device)\n",
    "\n",
    "        if show_loss:\n",
    "            \n",
    "            real_output = torch.sum(torch.mul(\n",
    "                self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, self.batch_size).view(\n",
    "                    self.max_num_hidden_layers, self.batch_size, 1), predictions_per_layer), 0)\n",
    "            criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "            loss = criterion(real_output.view(self.batch_size, self.n_classes), Y.view(self.batch_size).long())\n",
    "            self.loss_array.append(loss)\n",
    "            \n",
    "            if (len(self.loss_array) % 1000) == 0:\n",
    "                #print(\"WARNING: Set 'show_loss' to 'False' when not debugging. \"\n",
    "                #      \"It will deteriorate the fitting performance.\")\n",
    "                loss = torch.Tensor(self.loss_array).mean().cpu().numpy()\n",
    "                #print(\"Alpha:\" + str(self.alpha.data.cpu().numpy()))\n",
    "                print(\"Training Loss: \" + str(loss))\n",
    "                print(\"Balanced accuracy: \" + str(self.bal_acc))\n",
    "                print(\"Sensitivity: \" + str(self.sen))\n",
    "                print(\"Specificity: \" + str(self.spec))\n",
    "                print(\"Stp score: \" + str(self.stp_score))\n",
    "                print(\"Eqop score: \" + str(self.eqop_score))\n",
    "                self.loss_array.clear()\n",
    "\n",
    "    def forward(self, X):\n",
    "        hidden_connections = []\n",
    "\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "\n",
    "        x = F.relu(self.hidden_layers[0](X))\n",
    "        hidden_connections.append(x)\n",
    "\n",
    "        for i in range(1, self.max_num_hidden_layers):\n",
    "            hidden_connections.append(\n",
    "                F.relu(self.hidden_layers[i](hidden_connections[i - 1])))\n",
    "\n",
    "        output_class = []\n",
    "\n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            output_class.append(self.output_layers[i](hidden_connections[i]))\n",
    "\n",
    "        pred_per_layer = torch.stack(output_class)\n",
    "\n",
    "        return pred_per_layer\n",
    "\n",
    "    def validate_input_X(self, data):\n",
    "        \n",
    "        if len(data.shape) != 2:\n",
    "            raise Exception(\n",
    "                \"Wrong dimension for this X data. It should have only two dimensions.\")\n",
    "\n",
    "    def validate_input_Y(self, data):\n",
    "        if len(data.shape) != 1:\n",
    "            raise Exception(\n",
    "                \"Wrong dimension for this Y data. It should have only one dimensions.\")\n",
    "\n",
    "    def partial_fit_(self, X_data, Y_data, weight, show_loss=True):\n",
    "        \n",
    "        self.validate_input_X(X_data)\n",
    "        self.validate_input_Y(Y_data)\n",
    "        self.update_weights(X_data, Y_data, weight, show_loss)\n",
    "\n",
    "    def partial_fit(self, X_data, Y_data, weight, show_loss=True):\n",
    "        \n",
    "        self.partial_fit_(X_data, Y_data, weight, show_loss)\n",
    "\n",
    "    def update_eval_metrics(self,prediction_1,Y):\n",
    "        if prediction_1==1 and Y==1:\n",
    "            self.tp+=1\n",
    "        elif prediction_1==1 and Y==0:\n",
    "            self.fp+=1\n",
    "        elif prediction_1==0 and Y==1:\n",
    "            self.fn+=1\n",
    "        else:\n",
    "            self.tn+=1\n",
    "        \n",
    "        self.sen = self.tp/(self.tp + self.fn)\n",
    "        self.spec= self.tn/(self.tn + self.fp)\n",
    "        self.bal_acc = (self.sen + self.spec)/2\n",
    "            \n",
    "    def update_stp_score(self,prediction_1,X):\n",
    "        if X[0][sa_index] == p_Group:\n",
    "            if prediction_1 == 1:\n",
    "                self.protected_pos += 1.\n",
    "            else:\n",
    "                self.protected_neg += 1.\n",
    "        else:\n",
    "            if prediction_1 == 1:\n",
    "                self.non_protected_pos += 1.\n",
    "            else:\n",
    "                self.non_protected_neg += 1.\n",
    "            \n",
    "        C_prot = (self.protected_pos) / (self.protected_pos + self.protected_neg)\n",
    "        C_non_prot = (self.non_protected_pos) / (self.non_protected_pos + self.non_protected_neg)\n",
    "\n",
    "        self.stp_score = C_non_prot - C_prot\n",
    "    \n",
    "    def update_eqop_score(self,prediction_1,X,Y):\n",
    "        if X[0][sa_index] == p_Group:\n",
    "            if prediction_1==1 and Y==1:\n",
    "                self.tp_protected += 1.\n",
    "            elif prediction_1==0 and Y==1:\n",
    "                self.fn_protected += 1.\n",
    "        else:\n",
    "            if prediction_1==1 and Y==1:\n",
    "                self.tp_non_protected += 1.\n",
    "            elif prediction_1==0 and Y==1:\n",
    "                self.fn_non_protected += 1.\n",
    "            \n",
    "        tpr_protected = self.tp_protected / (self.tp_protected + self.fn_protected)\n",
    "        tpr_non_protected = self.tp_non_protected / (self.tp_non_protected + self.fn_non_protected)\n",
    "        self.eqop_score = tpr_non_protected - tpr_protected\n",
    "    \n",
    "    def predict_1(self, X_data):\n",
    "        self.validate_input_X(X_data)\n",
    "        return torch.argmax(torch.sum(torch.mul(\n",
    "            self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, 1).view(\n",
    "                self.max_num_hidden_layers, len(X_data), 1), self.forward(X_data)), 0), dim=1).cpu().numpy()\n",
    "    def predict_(self, X_data):\n",
    "        self.validate_input_X(X_data)\n",
    "        return torch.argmax(torch.sum(torch.mul(\n",
    "            self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, len(X_data)).view(\n",
    "                self.max_num_hidden_layers, len(X_data), 1), self.forward(X_data)), 0), dim=1).cpu().numpy()\n",
    "\n",
    "    def predict(self, X_data):\n",
    "        pred = self.predict_(X_data)\n",
    "        return pred\n",
    "\n",
    "    def export_params_to_json(self):\n",
    "        state_dict = self.state_dict()\n",
    "        params_gp = {}\n",
    "        for key, tensor in state_dict.items():\n",
    "            params_gp[key] = tensor.cpu().numpy().tolist()\n",
    "\n",
    "        return json.dumps(params_gp)\n",
    "\n",
    "    def load_params_from_json(self, json_data):\n",
    "        params = json.loads(json_data)\n",
    "        o_dict = collections.OrderedDict()\n",
    "        for key, tensor in params.items():\n",
    "            o_dict[key] = torch.tensor(tensor).to(self.device)\n",
    "        self.load_state_dict(o_dict)\n",
    "\n",
    "\n",
    "class ONN_THS(ONN):\n",
    "    def __init__(self, features_size, max_num_hidden_layers, qtd_neuron_per_hidden_layer, n_classes, b=0.99, n=0.01,\n",
    "                 s=0.2, e=[0.5, 0.35, 0.2, 0.1, 0.05], use_cuda=False):\n",
    "        super().__init__(features_size, max_num_hidden_layers, qtd_neuron_per_hidden_layer, n_classes, b=b, n=n, s=s,\n",
    "                         use_cuda=use_cuda)\n",
    "        self.e = Parameter(torch.tensor(e), requires_grad=False)\n",
    "        self.arms_values = Parameter(\n",
    "            torch.arange(n_classes), requires_grad=False)\n",
    "        self.explorations_mab = []\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            self.explorations_mab.append(algs.ThompsomSampling(len(e)))\n",
    "\n",
    "    def partial_fit(self, X_data, Y_data, exp_factor, show_loss=True):\n",
    "        self.partial_fit_(X_data, Y_data, show_loss)\n",
    "        self.explorations_mab[Y_data[0]].reward(exp_factor)\n",
    "\n",
    "    def predict(self, X_data):\n",
    "        pred = self.predict_(X_data)[0]\n",
    "        exp_factor = self.explorations_mab[pred].select()[0]\n",
    "        if np.random.uniform() < self.e[exp_factor]:\n",
    "            removed_arms = self.arms_values.clone().numpy().tolist()\n",
    "            removed_arms.remove(pred)\n",
    "            return random.choice(removed_arms), exp_factor\n",
    "\n",
    "        return pred, exp_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30aa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from imblearn.datasets import make_imbalance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e47b4179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['decile1b', 'decile3', 'lsat', 'ugpa', 'zfygpa', 'zgpa', 'fulltime', 'fam_inc', 'sex', 'race', 'tier', 'y'])\n",
      "['decile1b_1', 'decile1b_2', 'decile1b_3', 'decile1b_4', 'decile1b_5', 'decile1b_6', 'decile1b_7', 'decile1b_8', 'decile1b_9', 'decile1b_10', 'decile3_1', 'decile3_2', 'decile3_3', 'decile3_4', 'decile3_5', 'decile3_6', 'decile3_7', 'decile3_8', 'decile3_9', 'decile3_10', 'lsat', 'ugpa', 'zfygpa', 'zgpa', 'fulltime', 'fam_inc_1', 'fam_inc_2', 'fam_inc_3', 'fam_inc_4', 'fam_inc_5', 'sex', 'race', 'tier_1', 'tier_2', 'tier_3', 'tier_4', 'tier_5', 'tier_6', 'target']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18692, 38)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import urllib2\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import preprocessing\n",
    "from random import seed, shuffle\n",
    "\n",
    "# import utils as ut\n",
    "\n",
    "SEED = 1234\n",
    "seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "def load_law():\n",
    "    FEATURES_CLASSIFICATION = [\"decile1b\", \"decile3\", \"lsat\", \"ugpa\", \"zfygpa\",\"zgpa\", \"fulltime\", \"fam_inc\", \"sex\", \"race\", \"tier\"]  # features to be used for classification\n",
    "    CONT_VARIABLES = [\"lsat\", \"ugpa\", \"zfygpa\", \"zgpa\"]  # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"y\"  # the decision variable\n",
    "    SENSITIVE_ATTRS = [\"sex\"]\n",
    "    CAT_VARIABLES = [\"decile1b\", \"decile3\", \"fulltime\", \"fam_inc\", \"sex\", \"race\", \"tier\"]\n",
    "    CAT_VARIABLES_INDICES = [1,2,7,8,9,10,11]\n",
    "    # COMPAS_INPUT_FILE = \"bank-full.csv\"\n",
    "    INPUT_FILE = \"./datasets/law.csv\"\n",
    "\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # convert to np array\n",
    "    data = df.to_dict('list')\n",
    "    print(data.keys())\n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "\n",
    "    # convert class label 0 to -1\n",
    "    y = data[CLASS_FEATURE]\n",
    "    #print(np.unique(y))\n",
    "    #y[y == 0] = 1\n",
    "    #y[y==0] = -1\n",
    "    #y[y == 1] = -1\n",
    "    y = np.array([int(k) for k in y])\n",
    "\n",
    "    X = np.array([]).reshape(len(y), 0)  # empty array with num rows same as num examples, will hstack the features to it\n",
    "    \n",
    "    x_control = defaultdict(list)\n",
    "    i=0\n",
    "    feature_names = []\n",
    "    \n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        vals = data[attr]\n",
    "        \n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            vals = preprocessing.scale(vals)  # 0 mean and 1 variance\n",
    "            vals = np.reshape(vals, (len(y), -1))  # convert from 1-d arr to a 2-d arr with one col\n",
    "            \n",
    "\n",
    "        else:  # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            lb = preprocessing.LabelBinarizer()\n",
    "            lb.fit(vals)\n",
    "            vals = lb.transform(vals)\n",
    "           \n",
    "            #if attr == 'job':\n",
    "            #    print(lb.classes_)\n",
    "            #    print(lb.transform(lb.classes_))\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "        # add to sensitive features dict\n",
    "        if attr in SENSITIVE_ATTRS:\n",
    "            x_control[attr] = vals\n",
    "            \n",
    "\n",
    "        # add to learnable features\n",
    "        X = np.hstack((X, vals))\n",
    "        \n",
    "        if attr in CONT_VARIABLES:  # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else:  # categorical features\n",
    "            if vals.shape[1] == 1:  # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in lb.classes_:  # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "    # convert the sensitive feature to 1-d array\n",
    "    \n",
    "    x_control = dict(x_control)\n",
    "    \n",
    "    for k in x_control.keys():\n",
    "        assert (x_control[k].shape[1] == 1)  # make sure that the sensitive feature is binary after one hot encoding\n",
    "        x_control[k] = np.array(x_control[k]).flatten()\n",
    "    \n",
    "    feature_names.append('target')\n",
    "    \n",
    "   \n",
    "    print(feature_names)\n",
    "    return X, y, feature_names.index(SENSITIVE_ATTRS[0]), 1, x_control\n",
    "\n",
    "X,y, sa_index, p_Group, x_control= load_law()\n",
    "\n",
    "print(X)\n",
    "#print(X[0])\n",
    "#print(X[0][1])\n",
    "#print(sa_index)\n",
    "np_Group = 0 #non-protected group's sa_value\n",
    "Y = []\n",
    "for i in y:\n",
    "    if (i == 0):\n",
    "        Y.append(1)\n",
    "    else:\n",
    "        Y.append(0)\n",
    "        \n",
    "Y = np.array(Y)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520200fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(data, predict, k):\n",
    "    #k=8\n",
    "    #if len(data) >= k:\n",
    "    #    warnings.warn('K is set to a value less than total voting groups!')\n",
    "\n",
    "    distances = []\n",
    "    count = 0\n",
    "    for sample in data:\n",
    "        euclidean_distance = np.linalg.norm(np.array(sample)-np.array(predict))\n",
    "        distances.append([euclidean_distance,count])\n",
    "        count+=1\n",
    "    \n",
    "    votes = [i[1] for i in sorted(distances)[:k]] ##votes is returning indexes of k random samples\n",
    "\n",
    "    #vote_result = Counter(votes).most_common(9)[0][0]\n",
    "    return votes\n",
    "def fair_kSMOTE_algo_2(dmajor,dminor,k,r):\n",
    "    S = []\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    #Ns = 12\n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    \n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    i = 0\n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,k) #from minority-p\n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            \n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / (len(N)-1) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return S\n",
    "\n",
    "def fair_kSMOTE(dmajor,dminor_wg,dminor,k,r):\n",
    "    S = []\n",
    "    #Ns =  int(r*(len(dmajor) - len(dminor)))\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    #Ns = 12\n",
    "    \n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    #if r==-1:\n",
    "    #    Nks = 1\n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    \n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    i = 0\n",
    "    \n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,int(k/2)+1) #from minority-p\n",
    "        N2= k_nearest_neighbors(dminor_wg,xb,int(k/2)) #from minority-np\n",
    "    \n",
    "        N3 = np.hstack((N, N2))\n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                \n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "            j = 0\n",
    "            while(j < len(N2)):\n",
    "                #here on random xb\n",
    "                ind = N2[j]\n",
    "                \n",
    "                x_new = x_new + ((dminor_wg[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1    \n",
    "            x_new = x_new / (len(N3)-1) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "   \n",
    "    return S\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def splitYtrain_sa_value(Xtr,Ytr,minority_lable,majority_label,pp_Group=p_Group,npp_Group=np_Group): #splite Ytrain based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x = []\n",
    "    dmaj_np_x = []\n",
    "    dmin_p_x = []\n",
    "    dmin_np_x = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==pp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x.append(Xtr[i])\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==npp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==pp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==npp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x.append(Xtr[i])\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x\n",
    "\n",
    "\n",
    "\n",
    "#create_synth_data(window, window_label, minority_label,majority_label,5,lambda_score, 'min_p')\n",
    "def create_synth_data(x, y, minority_lable,majority_label,k,r,group,pp_group,npp_group):\n",
    "    \n",
    "    \n",
    "    dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x = splitYtrain_sa_value(x,y,minority_lable,majority_label, pp_group,npp_group)\n",
    "    '''\n",
    "    print(\"length of dmin_p_x: \" + str(len(dmin_p_x)))\n",
    "    print(\"length of dmin_np_x: \" + str(len(dmin_np_x)))\n",
    "    print(\"length of dmaj_p_x: \" + str(len(dmaj_p_x)))\n",
    "    print(\"length of dmaj_np_x: \" + str(len(dmaj_np_x)))\n",
    "    '''\n",
    "    if len(dmin_p_x)<4:\n",
    "        return -1, -1\n",
    "    group_names = ['dmin_p_x', 'dmin_np_x', 'dmaj_p_x', 'dmaj_np_x']\n",
    "    \n",
    "    Xtr_new = []\n",
    "    Ytr_new = []  \n",
    "    \n",
    "    \n",
    "    ##Algo 3:\n",
    "    \n",
    "    if group =='min_p':\n",
    "        dmaj_x = dmaj_p_x\n",
    "        dmin_x = dmin_p_x\n",
    "        \n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE(dmaj_x,dmin_np_x,dmin_x,k,r)\n",
    "        #x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = minority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "        \n",
    "    elif group =='maj_np':\n",
    "        dmaj_x = dmin_np_x\n",
    "        dmin_x = dmaj_np_x\n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        \n",
    "        \n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = majority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return Xtr_new,Ytr_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f5fe353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, Y = make_classification(n_samples=50000, n_features=8, n_informative=4, n_redundant=0, n_classes=2,\n",
    "#                           n_clusters_per_class=1, class_sep=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, shuffle=True)\n",
    "X_train, y_train= X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba89634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a,b = create_synth_data(X, Y, 1,0,5,0.1, 'min_p', p_Group,np_Group)\n",
    "#print(len(a))\n",
    "onn_network = ONN(features_size=38, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c77da331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_update_classSize(train_data, label,classSize):\n",
    "    theta = 0.9\n",
    "    if (label not in classSize):\n",
    "        up_dict = {train_data[-1]:0.5}\n",
    "        classSize.update(up_dict)\n",
    "    for classValue in classSize:\n",
    "        if classValue == label:\n",
    "            update = theta * classSize.get(classValue) + (1-theta)\n",
    "            classSize[classValue] = update\n",
    "        else:\n",
    "            update = theta * classSize.get(classValue)\n",
    "            classSize[classValue] = update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8731e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.62309223\n",
      "Balanced accuracy: 0.7242493882219261\n",
      "Sensitivity: 0.7888655462184875\n",
      "Specificity: 0.6596332302253646\n",
      "Stp score: -0.006750325894557874\n",
      "Eqop score: -0.05879876759932645\n",
      "Training Loss: 0.5286569\n",
      "Balanced accuracy: 0.7460961400058046\n",
      "Sensitivity: 0.8041294642857143\n",
      "Specificity: 0.688062815725895\n",
      "Stp score: -0.00025266839488069515\n",
      "Eqop score: -0.02650040329406611\n",
      "Training Loss: 0.48049706\n",
      "Balanced accuracy: 0.7669719093690942\n",
      "Sensitivity: 0.824768022840828\n",
      "Specificity: 0.7091757958973605\n",
      "Stp score: 0.01725744666570944\n",
      "Eqop score: -0.01041462658310488\n",
      "Training Loss: 0.4952991\n",
      "Balanced accuracy: 0.7718105565032964\n",
      "Sensitivity: 0.8284764826175869\n",
      "Specificity: 0.7151446303890059\n",
      "Stp score: 0.011447675844421745\n",
      "Eqop score: -0.03563096600555882\n",
      "Training Loss: 0.5112487\n",
      "Balanced accuracy: 0.7689016236777342\n",
      "Sensitivity: 0.8198356807511736\n",
      "Specificity: 0.7179675666042947\n",
      "Stp score: 0.011956463129892458\n",
      "Eqop score: -0.004819536661621604\n",
      "Training Loss: 0.368743\n",
      "Balanced accuracy: 0.7712330034952022\n",
      "Sensitivity: 0.8080448065173116\n",
      "Specificity: 0.7344212004730929\n",
      "Stp score: 0.0028139001100704886\n",
      "Eqop score: -0.015792232725019506\n",
      "Training Loss: 0.4416216\n",
      "Balanced accuracy: 0.7775559975655891\n",
      "Sensitivity: 0.8162650602409638\n",
      "Specificity: 0.7388469348902145\n",
      "Stp score: -0.0015960268684099277\n",
      "Eqop score: -0.006997542326597439\n",
      "Training Loss: 0.25119987\n",
      "Balanced accuracy: 0.7864481489897854\n",
      "Sensitivity: 0.8122432210353328\n",
      "Specificity: 0.7606530769442381\n",
      "Stp score: -0.036416036726003076\n",
      "Eqop score: 0.0040820918600019684\n",
      "Training Loss: 0.05839081\n",
      "Balanced accuracy: 0.798655090991892\n",
      "Sensitivity: 0.8078180332334514\n",
      "Specificity: 0.7894921487503327\n",
      "Stp score: -0.0874095353609273\n",
      "Eqop score: 0.0035764272384507523\n",
      "Training Loss: 0.12852627\n",
      "Balanced accuracy: 0.8023429987582449\n",
      "Sensitivity: 0.7962672931349517\n",
      "Specificity: 0.8084187043815382\n",
      "Stp score: -0.11240338887392531\n",
      "Eqop score: 0.0006676622890412487\n",
      "Training Loss: 0.09606279\n",
      "Balanced accuracy: 0.8090992642000444\n",
      "Sensitivity: 0.7929384376616658\n",
      "Specificity: 0.825260090738423\n",
      "Stp score: -0.13384369562455592\n",
      "Eqop score: 0.0008706747105660106\n",
      "Training Loss: 0.03374488\n",
      "Balanced accuracy: 0.8161839079238841\n",
      "Sensitivity: 0.7919142340480496\n",
      "Specificity: 0.8404535817997185\n",
      "Stp score: -0.1514212204638275\n",
      "Eqop score: 0.0028215564775314617\n",
      "Training Loss: 0.24088302\n",
      "Balanced accuracy: 0.8145384285879191\n",
      "Sensitivity: 0.7838454152664426\n",
      "Specificity: 0.8452314419093957\n",
      "Stp score: -0.145969563777587\n",
      "Eqop score: -0.003769053537752587\n",
      "Training Loss: 0.15768842\n",
      "Balanced accuracy: 0.8169872419551982\n",
      "Sensitivity: 0.7813674812030075\n",
      "Specificity: 0.852607002707389\n",
      "Stp score: -0.15236012530759752\n",
      "Eqop score: 0.0028704791197095014\n",
      "Training Loss: 0.052788273\n",
      "Balanced accuracy: 0.8193614055177021\n",
      "Sensitivity: 0.7758982734484368\n",
      "Specificity: 0.8628245375869675\n",
      "Stp score: -0.16270501198771048\n",
      "Eqop score: 0.009706370439875323\n",
      "Training Loss: 0.04502984\n",
      "Balanced accuracy: 0.8203883435619723\n",
      "Sensitivity: 0.769613515389956\n",
      "Specificity: 0.8711631717339887\n",
      "Stp score: -0.17047603414907936\n",
      "Eqop score: 0.0037664674395161413\n",
      "Training Loss: 0.06063368\n",
      "Balanced accuracy: 0.8224376642586345\n",
      "Sensitivity: 0.7660677263303386\n",
      "Specificity: 0.8788076021869304\n",
      "Stp score: -0.17707723566535793\n",
      "Eqop score: 0.0034874770573299907\n",
      "Training Loss: 0.07257402\n",
      "Balanced accuracy: 0.8225140174955382\n",
      "Sensitivity: 0.7596219539968116\n",
      "Specificity: 0.8854060809942648\n",
      "Stp score: -0.18272334483640312\n",
      "Eqop score: 0.00534655390298433\n",
      "Training Loss: 0.07828182\n",
      "Balanced accuracy: 0.8216578914585593\n",
      "Sensitivity: 0.7519161406672678\n",
      "Specificity: 0.8913996422498509\n",
      "Stp score: -0.18665434525932273\n",
      "Eqop score: 0.0029685072666615353\n",
      "Training Loss: 0.07266317\n",
      "Balanced accuracy: 0.8209783227883946\n",
      "Sensitivity: 0.7449343130705856\n",
      "Specificity: 0.8970223325062036\n",
      "Stp score: -0.1905735432833931\n",
      "Eqop score: 0.0007265810134852169\n",
      "Training Loss: 0.06622396\n",
      "Balanced accuracy: 0.8211670235269521\n",
      "Sensitivity: 0.7408104517271922\n",
      "Specificity: 0.9015235953267122\n",
      "Stp score: -0.19311844270060882\n",
      "Eqop score: 0.0020542232473867994\n",
      "Training Loss: 0.053823452\n",
      "Balanced accuracy: 0.821427160894387\n",
      "Sensitivity: 0.7370215574131104\n",
      "Specificity: 0.9058327643756638\n",
      "Stp score: -0.19607945433180052\n",
      "Eqop score: 0.0027929870010072744\n",
      "Training Loss: 0.045391735\n",
      "Balanced accuracy: 0.8219892031758201\n",
      "Sensitivity: 0.7340844454167578\n",
      "Specificity: 0.9098939609348825\n",
      "Stp score: -0.19812593752799607\n",
      "Eqop score: 0.0031220676712480078\n",
      "Training Loss: 0.03073627\n",
      "Balanced accuracy: 0.8229794256692161\n",
      "Sensitivity: 0.7322671888598782\n",
      "Specificity: 0.913691662478554\n",
      "Stp score: -0.200520931807049\n",
      "Eqop score: 0.001367182983201709\n",
      "Training Loss: 0.31690297\n",
      "Balanced accuracy: 0.8221872772294965\n",
      "Sensitivity: 0.7329476248477467\n",
      "Specificity: 0.9114269296112463\n",
      "Stp score: -0.19981098135253322\n",
      "Eqop score: 9.94499140521965e-05\n",
      "Training Loss: 0.066076644\n",
      "Balanced accuracy: 0.8220386829497338\n",
      "Sensitivity: 0.7292466168450817\n",
      "Specificity: 0.9148307490543859\n",
      "Stp score: -0.2011122934284217\n",
      "Eqop score: 0.004218370718459985\n",
      "Training Loss: 0.049102347\n",
      "Balanced accuracy: 0.8207410662187684\n",
      "Sensitivity: 0.7234021238228812\n",
      "Specificity: 0.9180800086146558\n",
      "Stp score: -0.20300133607824086\n",
      "Eqop score: 0.00657332697850066\n",
      "Training Loss: 0.0288515\n",
      "Balanced accuracy: 0.8215132819540936\n",
      "Sensitivity: 0.7219556088782243\n",
      "Specificity: 0.9210709550299628\n",
      "Stp score: -0.20456389190758856\n",
      "Eqop score: 0.006426856715140095\n",
      "Training Loss: 0.029994378\n",
      "Balanced accuracy: 0.8224573677844899\n",
      "Sensitivity: 0.7210714997012547\n",
      "Specificity: 0.923843235867725\n",
      "Stp score: -0.20574849469902534\n",
      "Eqop score: 0.006320034533310759\n",
      "Training Loss: 0.029041208\n",
      "Balanced accuracy: 0.8223351014900133\n",
      "Sensitivity: 0.7182106724856179\n",
      "Specificity: 0.9264595304944089\n",
      "Stp score: -0.20654017176896644\n",
      "Eqop score: 0.011747186353673333\n",
      "Training Loss: 0.049122892\n",
      "Balanced accuracy: 0.8222006540974272\n",
      "Sensitivity: 0.7155087581184806\n",
      "Specificity: 0.9288925500763736\n",
      "Stp score: -0.2080285436116916\n",
      "Eqop score: 0.01111990203862534\n",
      "Training Loss: 0.04448358\n",
      "Balanced accuracy: 0.8222272016800749\n",
      "Sensitivity: 0.7134026687598116\n",
      "Specificity: 0.9310517346003383\n",
      "Stp score: -0.20916040091489127\n",
      "Eqop score: 0.006650746550641462\n",
      "Training Loss: 0.04195913\n",
      "Balanced accuracy: 0.82189088397786\n",
      "Sensitivity: 0.7106137607505864\n",
      "Specificity: 0.9331680072051337\n",
      "Stp score: -0.2101088475748372\n",
      "Eqop score: 0.003561139050019979\n",
      "Training Loss: 0.02418955\n",
      "Balanced accuracy: 0.82219391537529\n",
      "Sensitivity: 0.7092274678111588\n",
      "Specificity: 0.9351603629394212\n",
      "Stp score: -0.21070461446130323\n",
      "Eqop score: 0.0006484717415763397\n",
      "Training Loss: 0.24748924\n",
      "Balanced accuracy: 0.8203612460185161\n",
      "Sensitivity: 0.70617123795404\n",
      "Specificity: 0.9345512540829923\n",
      "Stp score: -0.20948664005748097\n",
      "Eqop score: -0.012645333798562919\n",
      "Training Loss: 0.5011144\n",
      "Balanced accuracy: 0.8203060238414459\n",
      "Sensitivity: 0.7099396580623534\n",
      "Specificity: 0.9306723896205383\n",
      "Stp score: -0.21039449040188268\n",
      "Eqop score: -0.021908641345289248\n",
      "Training Loss: 0.45259523\n",
      "Balanced accuracy: 0.8223204849939558\n",
      "Sensitivity: 0.7171379738968303\n",
      "Specificity: 0.9275029960910812\n",
      "Stp score: -0.20605574876554558\n",
      "Eqop score: -0.02736770788760523\n",
      "Training Loss: 0.43413627\n",
      "Balanced accuracy: 0.8237470628739041\n",
      "Sensitivity: 0.7227115218019059\n",
      "Specificity: 0.9247826039459023\n",
      "Stp score: -0.2033146395437119\n",
      "Eqop score: -0.02590164812626139\n",
      "Training Loss: 0.45308807\n",
      "Balanced accuracy: 0.8200609823179377\n",
      "Sensitivity: 0.7181276249830647\n",
      "Specificity: 0.9219943396528107\n",
      "Stp score: -0.19774176996429524\n",
      "Eqop score: -0.025365317571264034\n",
      "Training Loss: 0.48244178\n",
      "Balanced accuracy: 0.8200097112044487\n",
      "Sensitivity: 0.7217716385755925\n",
      "Specificity: 0.9182477838333047\n",
      "Stp score: -0.19661430297262245\n",
      "Eqop score: -0.0280594021585876\n",
      "Training Loss: 0.46524328\n",
      "Balanced accuracy: 0.8204319974272214\n",
      "Sensitivity: 0.7261066698484668\n",
      "Specificity: 0.914757325005976\n",
      "Stp score: -0.19543855287782502\n",
      "Eqop score: -0.02120248790286361\n",
      "Training Loss: 0.41521546\n",
      "Balanced accuracy: 0.8195673317770387\n",
      "Sensitivity: 0.7269889910339349\n",
      "Specificity: 0.9121456725201426\n",
      "Stp score: -0.1913237490362437\n",
      "Eqop score: -0.015382882664727204\n",
      "change7\n",
      "warning0\n",
      "Balanced accuracy: 0.8215263066611932\n",
      "Sensitivity: 0.7336346803180481\n",
      "Specificity: 0.9094179330043383\n",
      "Stp score: -0.19003793002214187\n",
      "Eqop score: -0.019819136347721145\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from skmultiflow.drift_detection.eddm import EDDM\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "eddm = EDDM()\n",
    "#adwin = ADWIN(delta =1)\n",
    "window=[]\n",
    "window_label = []\n",
    "window_warning = []\n",
    "window_label_warning = []\n",
    "X, Y = shuffle(X, Y, random_state=0)\n",
    "pos_assigned=0\n",
    "pos_samples = 0\n",
    "neg_samples = 0\n",
    "pos_syn_samples = 0\n",
    "neg_syn_samples = 0\n",
    "generated_samples_per_sample = 0\n",
    "imbalance_ratio = 0 #of window\n",
    "minority_label=1\n",
    "majority_label = 0\n",
    "lambda_initial=0.05\n",
    "###\n",
    "ocis = 0\n",
    "classSize  = {}\n",
    "class_weights_dict = {}\n",
    "###\n",
    "j =0\n",
    "change=0\n",
    "warning=0\n",
    "bal_acc_list=[]\n",
    "stp_list = []\n",
    "    \n",
    "labels = []\n",
    "weight = 1\n",
    "for i in range(len(X)):\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    labels.append(Y[i])\n",
    "    online_update_classSize(X[i], Y[i], classSize)\n",
    "    class_weights = compute_class_weight('balanced', np.unique(labels), labels)\n",
    "    unique_labels = np.unique(labels)\n",
    "    for n in range(len(unique_labels)):\n",
    "        up_dict = {unique_labels[n]:class_weights[n]}\n",
    "        class_weights_dict.update(up_dict)\n",
    "    ocis = classSize.get(minority_label,0) - classSize.get(majority_label,0)\n",
    "    if Y[i] == majority_label and ocis>0:\n",
    "        weight = class_weights_dict[majority_label]/(1-ocis)\n",
    "    if Y[i] == minority_label and ocis<0:\n",
    "        weight = class_weights_dict[minority_label]/(1+ocis)\n",
    "    \n",
    "    ###\n",
    "\n",
    "    \n",
    "    if np.size(window)!=0:\n",
    "        if window.ndim==2 and len(window)>30:\n",
    "            majority_count = list(window_label).count(0)\n",
    "            minority_count = list(window_label).count(1)\n",
    "            if majority_count >  minority_count:\n",
    "                weight = int(majority_count/minority_count)\n",
    "    if Y[i]==minority_label:\n",
    "        onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),weight)\n",
    "    else:\n",
    "    \n",
    "        onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),1)\n",
    "    bal_acc_list.append(onn_network.bal_acc)\n",
    "    stp_list.append(onn_network.eqop_score)\n",
    "    if np.size(window)==0:\n",
    "        window = np.array(X[i])\n",
    "        window_label = np.array(Y[i])\n",
    "    else:\n",
    "        window=np.vstack((window,np.array(X[i])))\n",
    "        window_label= np.vstack((window_label,np.array(Y[i])))\n",
    "    eddm.add_element(Y[i])\n",
    "    \n",
    "    '''\n",
    "    if eddm.detected_warning_zone():\n",
    "        #print('Warning zone has been detected in data: ' + str(Y[i]) + ' - of index: ' + str(i))\n",
    "        warning+=1\n",
    "        if np.size(window_warning)==0:\n",
    "            window_warning = np.array(X[i])\n",
    "            window_label_warning = np.array(Y[i])\n",
    "        else:\n",
    "            window_warning=np.vstack((window_warning,np.array(X[i])))\n",
    "            window_label_warning= np.vstack((window_label_warning,np.array(Y[i])))\n",
    "    '''\n",
    "    if eddm.detected_change():\n",
    "        #print('Change has been detected in data: ' + str(Y[i]) + ' - of index: ' + str(i))\n",
    "        change+=1\n",
    "        window = []\n",
    "        window_label = []\n",
    "    \n",
    "        '''\n",
    "        if np.size(window_warning)==0:\n",
    "            window=[]\n",
    "            window_label=[]\n",
    "        else:\n",
    "            window = np.copy(window_warning)\n",
    "            window_label = np.copy(window_label_warning)\n",
    "            window_warning = []\n",
    "            window_label_warning = []\n",
    "        '''\n",
    "    pos_assigned = onn_network.tp+onn_network.fp-0.2\n",
    "    \n",
    "    if Y[i] == 1:\n",
    "        pos_samples += 1\n",
    "    else:\n",
    "        neg_samples += 1 \n",
    "    if np.size(window)!=0:\n",
    "        if window.ndim==2 and len(window)>30:\n",
    "            if onn_network.eqop_score > 0.000005:\n",
    "                #print(onn_network.stp_score)\n",
    "                lambda_score = lambda_initial*(1+(onn_network.eqop_score/0.2))\n",
    "                if pos_assigned <= pos_samples:\n",
    "                    X_syn,Y_syn = create_synth_data(window, window_label, minority_label,majority_label,4,lambda_score, 'min_p', p_Group,np_Group)\n",
    "                else:\n",
    "                    X_syn,Y_syn = create_synth_data(window, window_label, minority_label,majority_label,4,lambda_score, 'maj_np', p_Group,np_Group)\n",
    "                if X_syn!=-1:\n",
    "                    Y_syn = np.array(Y_syn)\n",
    "                    X_syn = np.array(X_syn)\n",
    "                    X_syn, Y_syn = shuffle(X_syn, Y_syn, random_state=0)\n",
    "                    for k in range(len(X_syn)):\n",
    "                        '''\n",
    "                        labels.append(Y[i])\n",
    "                        online_update_classSize(X[i], Y[i], classSize)\n",
    "                        class_weights = compute_class_weight('balanced', np.unique(labels), labels)\n",
    "                        unique_labels = np.unique(labels)\n",
    "                        for n in range(len(unique_labels)):\n",
    "                            up_dict = {unique_labels[n]:class_weights[n]}\n",
    "                            class_weights_dict.update(up_dict)\n",
    "                        ocis = classSize.get(minority_label,0) - classSize.get(majority_label,0)\n",
    "                        if Y[i] == majority_label and ocis>0:\n",
    "                            weight = class_weights_dict[majority_label]/(1-ocis)\n",
    "                        if Y[i] == minority_label and ocis<0:\n",
    "                            weight = class_weights_dict[minority_label]/(1+ocis)\n",
    "                        '''\n",
    "                        weight = 1\n",
    "                        onn_network.partial_fit(np.asarray([X_syn[k, :]]), np.asarray([Y_syn[k]]),weight)\n",
    "                 \n",
    "print(\"change\" + str(change))\n",
    "print(\"warning\" + str(warning))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Balanced accuracy: \" + str(onn_network.bal_acc))\n",
    "print(\"Sensitivity: \" + str(onn_network.sen))\n",
    "print(\"Specificity: \" + str(onn_network.spec))\n",
    "print(\"Stp score: \" + str(onn_network.stp_score))\n",
    "print(\"Eqop score: \" + str(onn_network.eqop_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a94b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "print(\"Balanced accuracy: \" + str(onn_network.bal_acc))\n",
    "print(\"Sensitivity: \" + str(onn_network.sen))\n",
    "print(\"Specificity: \" + str(onn_network.spec))\n",
    "print(\"Gmean: \" + str(math.sqrt(onn_network.sen*onn_network.spec)))\n",
    "print(\"Stp score: \" + str(onn_network.stp_score))\n",
    "print(\"Eqop score: \" + str(onn_network.eqop_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "483aa71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8215263066611932\n",
      "0.7336346803180481\n",
      "0.9094179330043383\n",
      "0.8168111988428769\n",
      "-0.19003793002214187\n",
      "-0.019819136347721145\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(str(onn_network.bal_acc))\n",
    "print(str(onn_network.sen))\n",
    "print( str(onn_network.spec))\n",
    "print( str(math.sqrt(onn_network.sen*onn_network.spec)))\n",
    "print( str(onn_network.stp_score))\n",
    "print( str(onn_network.eqop_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5332786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
