{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140e5580",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import collections\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from mab import algs\n",
    "\n",
    "\n",
    "class ONN(nn.Module):\n",
    "    def __init__(self, features_size, max_num_hidden_layers, qtd_neuron_per_hidden_layer, n_classes, batch_size=1,\n",
    "                 b=0.99, n=0.01, s=0.2, use_cuda=False):\n",
    "        super(ONN, self).__init__()\n",
    "\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            print(\"Using CUDA :]\")\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "\n",
    "        self.features_size = features_size\n",
    "        self.max_num_hidden_layers = max_num_hidden_layers\n",
    "        self.qtd_neuron_per_hidden_layer = qtd_neuron_per_hidden_layer\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.b = Parameter(torch.tensor(\n",
    "            b), requires_grad=False).to(self.device)\n",
    "        self.n = Parameter(torch.tensor(\n",
    "            n), requires_grad=False).to(self.device)\n",
    "        self.s = Parameter(torch.tensor(\n",
    "            s), requires_grad=False).to(self.device)\n",
    "\n",
    "        self.hidden_layers = []\n",
    "        self.output_layers = []\n",
    "\n",
    "        self.hidden_layers.append(\n",
    "            nn.Linear(features_size, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        for i in range(max_num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(\n",
    "                nn.Linear(qtd_neuron_per_hidden_layer, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        for i in range(max_num_hidden_layers):\n",
    "            self.output_layers.append(\n",
    "                nn.Linear(qtd_neuron_per_hidden_layer, n_classes))\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList(self.hidden_layers).to(self.device)\n",
    "        self.output_layers = nn.ModuleList(self.output_layers).to(self.device)\n",
    "\n",
    "        self.alpha = Parameter(torch.Tensor(self.max_num_hidden_layers).fill_(1 / (self.max_num_hidden_layers + 1)),\n",
    "                               requires_grad=False).to(\n",
    "            self.device)\n",
    "\n",
    "        self.loss_array = []\n",
    "        self.tp=0.1\n",
    "        self.tn=0.1\n",
    "        self.fp=0.1\n",
    "        self.fn=0.1\n",
    "        self.bal_acc = 0\n",
    "        self.bal_acc_list = []\n",
    "        self.sen = 0\n",
    "        self.spec = 0\n",
    "        \n",
    "        self.protected_pos = 0.1\n",
    "        self.protected_neg = 0.1\n",
    "        self.non_protected_pos = 0.1\n",
    "        self.non_protected_neg = 0.1\n",
    "        self.stp_score=0\n",
    "        \n",
    "        self.tp_protected = 0.1\n",
    "        self.fn_protected = 0.1\n",
    "        self.tp_non_protected = 0.1\n",
    "        self.fn_non_protected = 0.1\n",
    "        self.eqop_score=0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            self.output_layers[i].weight.grad.data.fill_(0)\n",
    "            self.output_layers[i].bias.grad.data.fill_(0)\n",
    "            self.hidden_layers[i].weight.grad.data.fill_(0)\n",
    "            self.hidden_layers[i].bias.grad.data.fill_(0)\n",
    "\n",
    "    def update_weights(self, X, Y, weight, show_loss, test):\n",
    "        Y = torch.from_numpy(Y).to(self.device)\n",
    "        if test=='yes':\n",
    "            prediction_1 = self.predict_1(X)\n",
    "            self.update_eval_metrics(prediction_1,Y)\n",
    "            self.update_stp_score(prediction_1,X)\n",
    "            self.update_eqop_score(prediction_1,X,Y)\n",
    "        \n",
    "        predictions_per_layer = self.forward(X)\n",
    "        losses_per_layer = []\n",
    "\n",
    "        for out in predictions_per_layer:\n",
    "            criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "            loss = criterion(out.view(self.batch_size, self.n_classes), Y.view(\n",
    "                self.batch_size).long())\n",
    "            losses_per_layer.append(loss*weight)\n",
    "\n",
    "        w = [None] * len(losses_per_layer)\n",
    "        b = [None] * len(losses_per_layer)\n",
    "        \n",
    "        with torch.no_grad():     #Context-manager that disabled gradient calculation\n",
    "            self.weights_output_layers = list()\n",
    "            self.biases_output_layers = list()\n",
    "            for i in range(len(losses_per_layer)):\n",
    "                losses_per_layer[i].backward(retain_graph=True)\n",
    "                self.output_layers[i].weight.data -= self.n * \\\n",
    "                                                   self.alpha[i] * self.output_layers[i].weight.grad.data\n",
    "                self.output_layers[i].bias.data -= self.n * \\\n",
    "                                                 self.alpha[i] * self.output_layers[i].bias.grad.data\n",
    "                self.weights_output_layers.append(self.output_layers[i].weight.data) \n",
    "                self.biases_output_layers.append(self.output_layers[i].bias.data)\n",
    "                \n",
    "                for j in range(i + 1):\n",
    "                    if w[j] is None:\n",
    "                        w[j] = self.alpha[i] * self.hidden_layers[j].weight.grad.data\n",
    "                        b[j] = self.alpha[i] * self.hidden_layers[j].bias.grad.data\n",
    "                    else:\n",
    "                        w[j] += self.alpha[i] * self.hidden_layers[j].weight.grad.data\n",
    "                        b[j] += self.alpha[i] * self.hidden_layers[j].bias.grad.data\n",
    "                self.zero_grad()\n",
    "            self.weights_hidden_layers= list()\n",
    "            self.biases_hidden_layers = list()\n",
    "            \n",
    "            for i in range(len(losses_per_layer)):\n",
    "                self.hidden_layers[i].weight.data -= self.n * w[i]\n",
    "                self.hidden_layers[i].bias.data -= self.n * b[i]\n",
    "                self.weights_hidden_layers.append(self.hidden_layers[i].weight.data)\n",
    "                self.biases_hidden_layers.append(self.hidden_layers[i].bias.data)\n",
    "            for i in range(len(losses_per_layer)):\n",
    "                self.alpha[i] *= torch.pow(self.b, losses_per_layer[i])\n",
    "                self.alpha[i] = torch.max(\n",
    "                  self.alpha[i], self.s / self.max_num_hidden_layers)\n",
    "\n",
    "        z_t = torch.sum(self.alpha)\n",
    "\n",
    "        self.alpha = Parameter(\n",
    "            self.alpha / z_t, requires_grad=False).to(self.device)\n",
    "        if show_loss:\n",
    "            \n",
    "            real_output = torch.sum(torch.mul(\n",
    "                self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, self.batch_size).view(\n",
    "                    self.max_num_hidden_layers, self.batch_size, 1), predictions_per_layer), 0)\n",
    "            criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "            loss = criterion(real_output.view(self.batch_size, self.n_classes), Y.view(self.batch_size).long())\n",
    "            self.loss_array.append(loss)\n",
    "            if (len(self.loss_array) % 1000) == 0:\n",
    "                #print(\"WARNING: Set 'show_loss' to 'False' when not debugging. \"\n",
    "                #      \"It will deteriorate the fitting performance.\")\n",
    "                loss = torch.Tensor(self.loss_array).mean().cpu().numpy()\n",
    "                #print(\"Alpha:\" + str(self.alpha.data.cpu().numpy()))\n",
    "                print(\"Training Loss: \" + str(loss))\n",
    "                print(\"Balanced accuracy: \" + str(self.bal_acc))\n",
    "                print(\"Sensitivity: \" + str(self.sen))\n",
    "                print(\"Specificity: \" + str(self.spec))\n",
    "                print(\"Stp score: \" + str(self.stp_score))\n",
    "                print(\"Eqop score: \" + str(self.eqop_score))\n",
    "                self.loss_array.clear()\n",
    "\n",
    "    def get_weights(self,network_name):\n",
    "        \n",
    "        if network_name == 'global':\n",
    "            weights_output_layers = list()\n",
    "            biases_output_layers = list()\n",
    "            weights_hidden_layers = list()\n",
    "            biases_hidden_layers= list()\n",
    "            for i in range(self.max_num_hidden_layers):\n",
    "                weights_output_layers.append(self.output_layers[i].weight.data) \n",
    "                biases_output_layers.append(self.output_layers[i].bias.data)\n",
    "                weights_hidden_layers.append(self.hidden_layers[i].weight.data)\n",
    "                biases_hidden_layers.append(self.hidden_layers[i].bias.data)\n",
    "               \n",
    "            return self.alpha, weights_output_layers, biases_output_layers, weights_hidden_layers, biases_hidden_layers\n",
    "        else:\n",
    "            return self.alpha, self.weights_output_layers, self.biases_output_layers, self.weights_hidden_layers, self.biases_hidden_layers\n",
    "    \n",
    "    def set_weights(self, alpha, w_output_layer, b_output_layer, w_hidden_layer, b_hidden_layer):\n",
    "        \n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            self.alpha[i] = alpha[i].clone().detach()\n",
    "            self.output_layers[i].weight.data = w_output_layer[i].clone().detach()\n",
    "            self.output_layers[i].bias.data = b_output_layer [i].clone().detach()\n",
    "            self.hidden_layers[i].weight.data =  w_hidden_layer [i].clone().detach()\n",
    "            self.hidden_layers[i].bias.data = b_hidden_layer [i].clone().detach()    \n",
    "    def forward(self, X):\n",
    "        hidden_connections = []\n",
    "\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "\n",
    "        x = F.relu(self.hidden_layers[0](X))\n",
    "        hidden_connections.append(x)\n",
    "\n",
    "        for i in range(1, self.max_num_hidden_layers):\n",
    "            hidden_connections.append(\n",
    "                F.relu(self.hidden_layers[i](hidden_connections[i - 1])))\n",
    "\n",
    "        output_class = []\n",
    "\n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            output_class.append(self.output_layers[i](hidden_connections[i]))\n",
    "\n",
    "        pred_per_layer = torch.stack(output_class)\n",
    "\n",
    "        return pred_per_layer\n",
    "\n",
    "    def validate_input_X(self, data):\n",
    "        \n",
    "        if len(data.shape) != 2:\n",
    "            raise Exception(\n",
    "                \"Wrong dimension for this X data. It should have only two dimensions.\")\n",
    "\n",
    "    def validate_input_Y(self, data):\n",
    "        if len(data.shape) != 1:\n",
    "            raise Exception(\n",
    "                \"Wrong dimension for this Y data. It should have only one dimensions.\")\n",
    "\n",
    "    def partial_fit_(self, X_data, Y_data, weight, show_loss=True, test='yes'):\n",
    "        \n",
    "        self.validate_input_X(X_data)\n",
    "        self.validate_input_Y(Y_data)\n",
    "        self.update_weights(X_data, Y_data, weight, show_loss, test)\n",
    "\n",
    "    def partial_fit(self, X_data, Y_data, weight, show_loss=True, test='yes'):\n",
    "        \n",
    "        self.partial_fit_(X_data, Y_data, weight, show_loss, test)\n",
    "\n",
    "    def reset_eval_metrics(self):\n",
    "        self.tp=0.1\n",
    "        self.tn=0.1\n",
    "        self.fp=0.1\n",
    "        self.fn=0.1\n",
    "        self.bal_acc = 0\n",
    "        self.bal_acc_list = []\n",
    "        self.sen = 0\n",
    "        self.spec = 0\n",
    "        \n",
    "        self.protected_pos = 0.1\n",
    "        self.protected_neg = 0.1\n",
    "        self.non_protected_pos = 0.1\n",
    "        self.non_protected_neg = 0.1\n",
    "        self.stp_score=0\n",
    "        \n",
    "        self.tp_protected = 0.1\n",
    "        self.fn_protected = 0.1\n",
    "        self.tp_non_protected = 0.1\n",
    "        self.fn_non_protected = 0.1\n",
    "        self.eqop_score=0\n",
    "    def update_eval_metrics(self,prediction_1,Y):\n",
    "        if prediction_1==1 and Y==1:\n",
    "            self.tp+=1\n",
    "        elif prediction_1==1 and Y==0:\n",
    "            self.fp+=1\n",
    "        elif prediction_1==0 and Y==1:\n",
    "            self.fn+=1\n",
    "        else:\n",
    "            self.tn+=1\n",
    "        \n",
    "        self.sen = self.tp/(self.tp + self.fn)\n",
    "        self.spec= self.tn/(self.tn + self.fp)\n",
    "        self.bal_acc = (self.sen + self.spec)/2\n",
    "        self.bal_acc_list.append(self.bal_acc)\n",
    "            \n",
    "    def update_stp_score(self,prediction_1,X):\n",
    "        if X[0][sa_index] == p_Group:\n",
    "            if prediction_1 == 1:\n",
    "                self.protected_pos += 1.\n",
    "            else:\n",
    "                self.protected_neg += 1.\n",
    "        else:\n",
    "            if prediction_1 == 1:\n",
    "                self.non_protected_pos += 1.\n",
    "            else:\n",
    "                self.non_protected_neg += 1.\n",
    "            \n",
    "        C_prot = (self.protected_pos) / (self.protected_pos + self.protected_neg)\n",
    "        C_non_prot = (self.non_protected_pos) / (self.non_protected_pos + self.non_protected_neg)\n",
    "\n",
    "        self.stp_score = C_non_prot - C_prot\n",
    "    \n",
    "    def update_eqop_score(self,prediction_1,X,Y):\n",
    "        if X[0][sa_index] == p_Group:\n",
    "            if prediction_1==1 and Y==1:\n",
    "                self.tp_protected += 1.\n",
    "            elif prediction_1==0 and Y==1:\n",
    "                self.fn_protected += 1.\n",
    "        else:\n",
    "            if prediction_1==1 and Y==1:\n",
    "                self.tp_non_protected += 1.\n",
    "            elif prediction_1==0 and Y==1:\n",
    "                self.fn_non_protected += 1.\n",
    "            \n",
    "        tpr_protected = self.tp_protected / (self.tp_protected + self.fn_protected)\n",
    "        tpr_non_protected = self.tp_non_protected / (self.tp_non_protected + self.fn_non_protected)\n",
    "        self.eqop_score = tpr_non_protected - tpr_protected\n",
    "    \n",
    "    def predict_1(self, X_data):\n",
    "        self.validate_input_X(X_data)\n",
    "        return torch.argmax(torch.sum(torch.mul(\n",
    "            self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, 1).view(\n",
    "                self.max_num_hidden_layers, len(X_data), 1), self.forward(X_data)), 0), dim=1).cpu().numpy()\n",
    "    def predict_(self, X_data):\n",
    "        self.validate_input_X(X_data)\n",
    "        return torch.argmax(torch.sum(torch.mul(\n",
    "            self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, len(X_data)).view(\n",
    "                self.max_num_hidden_layers, len(X_data), 1), self.forward(X_data)), 0), dim=1).cpu().numpy()\n",
    "\n",
    "    def predict(self, X_data):\n",
    "        pred = self.predict_(X_data)\n",
    "        return pred\n",
    "\n",
    "    def export_params_to_json(self):\n",
    "        state_dict = self.state_dict()\n",
    "        params_gp = {}\n",
    "        for key, tensor in state_dict.items():\n",
    "            params_gp[key] = tensor.cpu().numpy().tolist()\n",
    "\n",
    "        return json.dumps(params_gp)\n",
    "\n",
    "    def load_params_from_json(self, json_data):\n",
    "        params = json.loads(json_data)\n",
    "        o_dict = collections.OrderedDict()\n",
    "        for key, tensor in params.items():\n",
    "            o_dict[key] = torch.tensor(tensor).to(self.device)\n",
    "        self.load_state_dict(o_dict)\n",
    "\n",
    "\n",
    "class ONN_THS(ONN):\n",
    "    def __init__(self, features_size, max_num_hidden_layers, qtd_neuron_per_hidden_layer, n_classes, b=0.99, n=0.01,\n",
    "                 s=0.2, e=[0.5, 0.35, 0.2, 0.1, 0.05], use_cuda=False):\n",
    "        super().__init__(features_size, max_num_hidden_layers, qtd_neuron_per_hidden_layer, n_classes, b=b, n=n, s=s,\n",
    "                         use_cuda=use_cuda)\n",
    "        self.e = Parameter(torch.tensor(e), requires_grad=False)\n",
    "        self.arms_values = Parameter(\n",
    "            torch.arange(n_classes), requires_grad=False)\n",
    "        self.explorations_mab = []\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            self.explorations_mab.append(algs.ThompsomSampling(len(e)))\n",
    "\n",
    "    def partial_fit(self, X_data, Y_data, exp_factor, show_loss=True):\n",
    "        self.partial_fit_(X_data, Y_data, show_loss)\n",
    "        self.explorations_mab[Y_data[0]].reward(exp_factor)\n",
    "\n",
    "    def predict(self, X_data):\n",
    "        pred = self.predict_(X_data)[0]\n",
    "        exp_factor = self.explorations_mab[pred].select()[0]\n",
    "        if np.random.uniform() < self.e[exp_factor]:\n",
    "            removed_arms = self.arms_values.clone().numpy().tolist()\n",
    "            removed_arms.remove(pred)\n",
    "            return random.choice(removed_arms), exp_factor\n",
    "\n",
    "        return pred, exp_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30aa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from imblearn.datasets import make_imbalance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e47b4179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['decile1b', 'decile3', 'lsat', 'ugpa', 'zfygpa', 'zgpa', 'fulltime', 'fam_inc', 'sex', 'race', 'tier', 'y'])\n",
      "['decile1b_1', 'decile1b_2', 'decile1b_3', 'decile1b_4', 'decile1b_5', 'decile1b_6', 'decile1b_7', 'decile1b_8', 'decile1b_9', 'decile1b_10', 'decile3_1', 'decile3_2', 'decile3_3', 'decile3_4', 'decile3_5', 'decile3_6', 'decile3_7', 'decile3_8', 'decile3_9', 'decile3_10', 'lsat', 'ugpa', 'zfygpa', 'zgpa', 'fulltime', 'fam_inc_1', 'fam_inc_2', 'fam_inc_3', 'fam_inc_4', 'fam_inc_5', 'sex', 'race', 'tier_1', 'tier_2', 'tier_3', 'tier_4', 'tier_5', 'tier_6', 'target']\n"
     ]
    }
   ],
   "source": [
    "# import urllib2\n",
    "# import urllib2\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import preprocessing\n",
    "from random import seed, shuffle\n",
    "\n",
    "# import utils as ut\n",
    "\n",
    "SEED = 1234\n",
    "seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "def load_law():\n",
    "    FEATURES_CLASSIFICATION = [\"decile1b\", \"decile3\", \"lsat\", \"ugpa\", \"zfygpa\",\"zgpa\", \"fulltime\", \"fam_inc\", \"sex\", \"race\", \"tier\"]  # features to be used for classification\n",
    "    CONT_VARIABLES = [\"lsat\", \"ugpa\", \"zfygpa\", \"zgpa\"]  # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"y\"  # the decision variable\n",
    "    SENSITIVE_ATTRS = [\"sex\"]\n",
    "    CAT_VARIABLES = [\"decile1b\", \"decile3\", \"fulltime\", \"fam_inc\", \"sex\", \"race\", \"tier\"]\n",
    "    CAT_VARIABLES_INDICES = [1,2,7,8,9,10,11]\n",
    "    # COMPAS_INPUT_FILE = \"bank-full.csv\"\n",
    "    INPUT_FILE = \"./datasets/law.csv\"\n",
    "\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # convert to np array\n",
    "    data = df.to_dict('list')\n",
    "    print(data.keys())\n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "\n",
    "    # convert class label 0 to -1\n",
    "    y = data[CLASS_FEATURE]\n",
    "    #print(np.unique(y))\n",
    "    #y[y == 0] = 1\n",
    "    #y[y==0] = -1\n",
    "    #y[y == 1] = -1\n",
    "    y = np.array([int(k) for k in y])\n",
    "\n",
    "    X = np.array([]).reshape(len(y), 0)  # empty array with num rows same as num examples, will hstack the features to it\n",
    "    \n",
    "    x_control = defaultdict(list)\n",
    "    i=0\n",
    "    feature_names = []\n",
    "    \n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        vals = data[attr]\n",
    "        \n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            vals = preprocessing.scale(vals)  # 0 mean and 1 variance\n",
    "            vals = np.reshape(vals, (len(y), -1))  # convert from 1-d arr to a 2-d arr with one col\n",
    "            \n",
    "\n",
    "        else:  # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            lb = preprocessing.LabelBinarizer()\n",
    "            lb.fit(vals)\n",
    "            vals = lb.transform(vals)\n",
    "           \n",
    "            #if attr == 'job':\n",
    "            #    print(lb.classes_)\n",
    "            #    print(lb.transform(lb.classes_))\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "        # add to sensitive features dict\n",
    "        if attr in SENSITIVE_ATTRS:\n",
    "            x_control[attr] = vals\n",
    "            \n",
    "\n",
    "        # add to learnable features\n",
    "        X = np.hstack((X, vals))\n",
    "        \n",
    "        if attr in CONT_VARIABLES:  # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else:  # categorical features\n",
    "            if vals.shape[1] == 1:  # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in lb.classes_:  # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "    # convert the sensitive feature to 1-d array\n",
    "    \n",
    "    x_control = dict(x_control)\n",
    "    \n",
    "    for k in x_control.keys():\n",
    "        assert (x_control[k].shape[1] == 1)  # make sure that the sensitive feature is binary after one hot encoding\n",
    "        x_control[k] = np.array(x_control[k]).flatten()\n",
    "    \n",
    "    feature_names.append('target')\n",
    "    \n",
    "   \n",
    "    print(feature_names)\n",
    "    return X, y, feature_names.index(SENSITIVE_ATTRS[0]), 1, x_control\n",
    "\n",
    "X_1,y, sa_index, p_Group, x_control= load_law()\n",
    "\n",
    "#print(X[0])\n",
    "#print(X[0][1])\n",
    "#print(sa_index)\n",
    "np_Group = 0 #non-protected group's sa_value\n",
    "Y = []\n",
    "for i in y:\n",
    "    if (i == 0):\n",
    "        Y.append(1)\n",
    "    else:\n",
    "        Y.append(0)\n",
    "        \n",
    "Y_1 = np.array(Y)\n",
    "from sklearn.utils import shuffle\n",
    "X_1, Y_1 = shuffle(X_1, Y_1)\n",
    "X, x_test, Y, y_test = train_test_split(X_1,Y_1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be6c708b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1836"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520200fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(data, predict, k):\n",
    "    #k=8\n",
    "    #if len(data) >= k:\n",
    "    #    warnings.warn('K is set to a value less than total voting groups!')\n",
    "\n",
    "    distances = []\n",
    "    count = 0\n",
    "    for sample in data:\n",
    "        euclidean_distance = np.linalg.norm(np.array(sample)-np.array(predict))\n",
    "        distances.append([euclidean_distance,count])\n",
    "        count+=1\n",
    "    \n",
    "    votes = [i[1] for i in sorted(distances)[:k]] ##votes is returning indexes of k random samples\n",
    "\n",
    "    #vote_result = Counter(votes).most_common(9)[0][0]\n",
    "    return votes\n",
    "def fair_kSMOTE_algo_2(dmajor,dminor,k,r):\n",
    "    S = []\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    #Ns = 12\n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    \n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    i = 0\n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,k) #from minority-p\n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            \n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / (len(N)-1) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return S\n",
    "\n",
    "def fair_kSMOTE(dmajor,dminor_wg,dminor,k,r):\n",
    "    S = []\n",
    "    #Ns =  int(r*(len(dmajor) - len(dminor)))\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    #Ns = 12\n",
    "    \n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    #if r==-1:\n",
    "    #    Nks = 1\n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    \n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    i = 0\n",
    "    \n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,int(k/2)+1) #from minority-p\n",
    "        N2= k_nearest_neighbors(dminor_wg,xb,int(k/2)) #from minority-np\n",
    "    \n",
    "        N3 = np.hstack((N, N2))\n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                \n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "            j = 0\n",
    "            while(j < len(N2)):\n",
    "                #here on random xb\n",
    "                ind = N2[j]\n",
    "                \n",
    "                x_new = x_new + ((dminor_wg[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1    \n",
    "            x_new = x_new / (len(N3)-1) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "   \n",
    "    return S\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def splitYtrain_sa_value(Xtr,Ytr,minority_lable,majority_label,pp_Group=p_Group,npp_Group=np_Group): #splite Ytrain based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x = []\n",
    "    dmaj_np_x = []\n",
    "    dmin_p_x = []\n",
    "    dmin_np_x = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==pp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x.append(Xtr[i])\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==npp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==pp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==npp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x.append(Xtr[i])\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x\n",
    "\n",
    "\n",
    "\n",
    "#create_synth_data(window, window_label, minority_label,majority_label,5,lambda_score, 'min_p')\n",
    "def create_synth_data(x, y, minority_lable,majority_label,k,r,group,pp_group,npp_group):\n",
    "    \n",
    "    \n",
    "    dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x = splitYtrain_sa_value(x,y,minority_lable,majority_label, pp_group,npp_group)\n",
    "    '''\n",
    "    print(\"length of dmin_p_x: \" + str(len(dmin_p_x)))\n",
    "    print(\"length of dmin_np_x: \" + str(len(dmin_np_x)))\n",
    "    print(\"length of dmaj_p_x: \" + str(len(dmaj_p_x)))\n",
    "    print(\"length of dmaj_np_x: \" + str(len(dmaj_np_x)))\n",
    "    '''\n",
    "    if len(dmin_p_x)<4:\n",
    "        return -1, -1\n",
    "    group_names = ['dmin_p_x', 'dmin_np_x', 'dmaj_p_x', 'dmaj_np_x']\n",
    "    \n",
    "    Xtr_new = []\n",
    "    Ytr_new = []  \n",
    "    \n",
    "    \n",
    "    ##Algo 3:\n",
    "    \n",
    "    if group =='min_p':\n",
    "        dmaj_x = dmaj_p_x\n",
    "        dmin_x = dmin_p_x\n",
    "        \n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE(dmaj_x,dmin_np_x,dmin_x,k,r)\n",
    "        #x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = minority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "        \n",
    "    elif group =='maj_np':\n",
    "        dmaj_x = dmin_np_x\n",
    "        dmin_x = dmaj_np_x\n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        \n",
    "        \n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = majority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return Xtr_new,Ytr_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f5fe353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, Y = make_classification(n_samples=50000, n_features=8, n_informative=4, n_redundant=0, n_classes=2,\n",
    "#                           n_clusters_per_class=1, class_sep=3)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, shuffle=True)\n",
    "X_train, y_train= X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc23026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_update_classSize(train_data, label,classSize):\n",
    "    theta = 0.9\n",
    "    if (label not in classSize):\n",
    "        up_dict = {train_data[-1]:0.5}\n",
    "        classSize.update(up_dict)\n",
    "    for classValue in classSize:\n",
    "        if classValue == label:\n",
    "            update = theta * classSize.get(classValue) + (1-theta)\n",
    "            classSize[classValue] = update\n",
    "        else:\n",
    "            update = theta * classSize.get(classValue)\n",
    "            classSize[classValue] = update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b99015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_clients(instances, labels, num_clients, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(instances, labels))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ea09d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4984\n",
      "4984\n",
      "4984\n",
      "{'client_1': 0, 'client_2': 0, 'client_3': 0}\n"
     ]
    }
   ],
   "source": [
    "from skmultiflow.drift_detection.eddm import EDDM\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "clients = create_clients(X_train, y_train, num_clients=3, initial='client')\n",
    "client_index = {}\n",
    "client_window = {}\n",
    "client_window_label = {}\n",
    "client_eddm = {}\n",
    "\n",
    "for (client_name, data) in clients.items():\n",
    "    data, label = zip(*data)\n",
    "    Y = np.asarray(label)\n",
    "    X = np.asarray(data)\n",
    "    client_index.update({client_name:0})\n",
    "    client_window.update({client_name:[]})\n",
    "    client_window_label.update({client_name:[]})\n",
    "    length = len(data)\n",
    "    print(len(data))\n",
    "#client_eddm.update({'client_1':eddm1})\n",
    "#client_eddm.update({'client_2':eddm2})\n",
    "#client_eddm.update({'client_3':eddm3})\n",
    "print(client_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e713d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4505, 1: 479}\n"
     ]
    }
   ],
   "source": [
    "labels =[]\n",
    "for i in range(len(clients['client_3'])):\n",
    "    labels.append(clients['client_3'][i][1])\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "count_ap_dict = dict(zip(unique, counts))\n",
    "print(count_ap_dict)\n",
    "minority_label=1\n",
    "majority_label = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7334a2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_1\n",
      "[0 1]\n",
      "client_2\n",
      "[0 1]\n",
      "client_3\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "for (client_name, data) in clients.items():\n",
    "    print(client_name)\n",
    "    data, label = zip(*data)\n",
    "    Y = np.asarray(label)\n",
    "    X = np.asarray(data)\n",
    "    print(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8731e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'client_1': 0, 'client_2': 0, 'client_3': 0}\n",
      "{'client_1': 1, 'client_2': 0, 'client_3': 0}\n",
      "{'client_1': 1, 'client_2': 1, 'client_3': 0}\n",
      "{'client_1': 200, 'client_2': 200, 'client_3': 200}\n",
      "Change has been detected in data: 1 - of index: 242\n",
      "Change has been detected in data: 1 - of index: 259\n",
      "Change has been detected in data: 1 - of index: 321\n",
      "{'client_1': 400, 'client_2': 400, 'client_3': 400}\n",
      "Change has been detected in data: 1 - of index: 564\n",
      "{'client_1': 600, 'client_2': 600, 'client_3': 600}\n",
      "Change has been detected in data: 1 - of index: 605\n",
      "Change has been detected in data: 1 - of index: 610\n",
      "{'client_1': 800, 'client_2': 800, 'client_3': 800}\n",
      "Training Loss: 0.67026865\n",
      "Balanced accuracy: 0.5994406803293411\n",
      "Sensitivity: 0.7595338983050848\n",
      "Specificity: 0.43934746235359734\n",
      "Stp score: -0.09027318711517385\n",
      "Eqop score: 0.11919892036372215\n",
      "Change has been detected in data: 1 - of index: 868\n",
      "Change has been detected in data: 1 - of index: 924\n",
      "Training Loss: 0.68478954\n",
      "Balanced accuracy: 0.6015637491783183\n",
      "Sensitivity: 0.7150974025974026\n",
      "Specificity: 0.48803009575923395\n",
      "Stp score: 0.02410192382068488\n",
      "Eqop score: 0.21977853157338445\n",
      "Training Loss: 0.6978081\n",
      "Balanced accuracy: 0.6217542098200078\n",
      "Sensitivity: 0.9030710172744723\n",
      "Specificity: 0.34043740236554343\n",
      "Stp score: 0.01688115434309234\n",
      "Eqop score: -0.016066407818985162\n",
      "{'client_1': 1000, 'client_2': 1000, 'client_3': 1000}\n",
      "Change has been detected in data: 1 - of index: 1076\n",
      "Change has been detected in data: 1 - of index: 1169\n",
      "{'client_1': 1200, 'client_2': 1200, 'client_3': 1200}\n",
      "Change has been detected in data: 1 - of index: 1241\n",
      "{'client_1': 1400, 'client_2': 1400, 'client_3': 1400}\n",
      "Change has been detected in data: 1 - of index: 1417\n",
      "Change has been detected in data: 1 - of index: 1563\n",
      "Change has been detected in data: 1 - of index: 1575\n",
      "{'client_1': 1600, 'client_2': 1600, 'client_3': 1600}\n",
      "Training Loss: 0.5900506\n",
      "Balanced accuracy: 0.6781773488137527\n",
      "Sensitivity: 0.7785181236673774\n",
      "Specificity: 0.577836573960128\n",
      "Stp score: -0.07166045069204585\n",
      "Eqop score: 0.1333506437198888\n",
      "{'client_1': 1800, 'client_2': 1800, 'client_3': 1800}\n",
      "Change has been detected in data: 1 - of index: 1840\n",
      "Training Loss: 0.4852245\n",
      "Balanced accuracy: 0.6791756913032597\n",
      "Sensitivity: 0.7321763602251408\n",
      "Specificity: 0.6261750223813787\n",
      "Stp score: 0.009474317436825141\n",
      "Eqop score: 0.17225542728959709\n",
      "Change has been detected in data: 1 - of index: 1871\n",
      "Training Loss: 0.594482\n",
      "Balanced accuracy: 0.691202664775127\n",
      "Sensitivity: 0.8715551181102362\n",
      "Specificity: 0.5108502114400179\n",
      "Stp score: -0.0010736793794393629\n",
      "Eqop score: 0.016532953292222596\n",
      "{'client_1': 2000, 'client_2': 2000, 'client_3': 2000}\n",
      "Change has been detected in data: 1 - of index: 2015\n",
      "Change has been detected in data: 1 - of index: 2174\n",
      "Change has been detected in data: 1 - of index: 2176\n",
      "{'client_1': 2200, 'client_2': 2200, 'client_3': 2200}\n",
      "{'client_1': 2400, 'client_2': 2400, 'client_3': 2400}\n",
      "Change has been detected in data: 1 - of index: 2455\n",
      "Change has been detected in data: 1 - of index: 2474\n",
      "Training Loss: 0.42717168\n",
      "Balanced accuracy: 0.7102546771569527\n",
      "Sensitivity: 0.741519174041298\n",
      "Specificity: 0.6789901802726074\n",
      "Stp score: -0.029963880077151728\n",
      "Eqop score: 0.18113629645553975\n",
      "{'client_1': 2600, 'client_2': 2600, 'client_3': 2600}\n",
      "Change has been detected in data: 1 - of index: 2725\n",
      "Change has been detected in data: 1 - of index: 2744\n",
      "Training Loss: 0.5389533\n",
      "Balanced accuracy: 0.7022687703573711\n",
      "Sensitivity: 0.7836658354114713\n",
      "Specificity: 0.6208717053032708\n",
      "Stp score: -0.046940870972732585\n",
      "Eqop score: 0.09425452674686108\n",
      "{'client_1': 2800, 'client_2': 2800, 'client_3': 2800}\n",
      "Training Loss: 0.46441853\n",
      "Balanced accuracy: 0.730832978267089\n",
      "Sensitivity: 0.8751728907330567\n",
      "Specificity: 0.5864930658011213\n",
      "Stp score: -0.015098514735100432\n",
      "Eqop score: 0.03037724771487116\n",
      "{'client_1': 3000, 'client_2': 3000, 'client_3': 3000}\n",
      "Change has been detected in data: 1 - of index: 3078\n",
      "Change has been detected in data: 1 - of index: 3122\n",
      "Change has been detected in data: 1 - of index: 3146\n",
      "{'client_1': 3200, 'client_2': 3200, 'client_3': 3200}\n",
      "Change has been detected in data: 1 - of index: 3252\n",
      "Training Loss: 0.4188348\n",
      "Balanced accuracy: 0.7365297015700396\n",
      "Sensitivity: 0.7667040988208871\n",
      "Specificity: 0.7063553043191921\n",
      "Stp score: -0.03755773174724336\n",
      "Eqop score: 0.15966623176735606\n",
      "Change has been detected in data: 1 - of index: 3395\n",
      "{'client_1': 3400, 'client_2': 3400, 'client_3': 3400}\n",
      "Change has been detected in data: 1 - of index: 3489\n",
      "Change has been detected in data: 1 - of index: 3592\n",
      "{'client_1': 3600, 'client_2': 3600, 'client_3': 3600}\n",
      "Training Loss: 0.43098968\n",
      "Balanced accuracy: 0.7482145573896679\n",
      "Sensitivity: 0.8527525387493319\n",
      "Specificity: 0.6436765760300038\n",
      "Stp score: -0.03309046461183007\n",
      "Eqop score: 0.01797606850117095\n",
      "Change has been detected in data: 1 - of index: 3778\n",
      "Training Loss: 0.53849894\n",
      "Balanced accuracy: 0.7217084632582175\n",
      "Sensitivity: 0.7914164606146238\n",
      "Specificity: 0.6520004659018112\n",
      "Stp score: -0.03832890619638557\n",
      "Eqop score: 0.07819284533714321\n",
      "{'client_1': 3800, 'client_2': 3800, 'client_3': 3800}\n",
      "Change has been detected in data: 1 - of index: 3886\n",
      "{'client_1': 4000, 'client_2': 4000, 'client_3': 4000}\n",
      "Change has been detected in data: 1 - of index: 4052\n",
      "Change has been detected in data: 1 - of index: 4129\n",
      "{'client_1': 4200, 'client_2': 4200, 'client_3': 4200}\n",
      "Training Loss: 0.50849944\n",
      "Balanced accuracy: 0.7501642502960539\n",
      "Sensitivity: 0.7847960992907801\n",
      "Specificity: 0.7155324013013277\n",
      "Stp score: -0.029103043359835434\n",
      "Eqop score: 0.12058324558324562\n",
      "Change has been detected in data: 1 - of index: 4352\n",
      "{'client_1': 4400, 'client_2': 4400, 'client_3': 4400}\n",
      "Change has been detected in data: 1 - of index: 4401\n",
      "Change has been detected in data: 1 - of index: 4588\n",
      "{'client_1': 4600, 'client_2': 4600, 'client_3': 4600}\n",
      "Training Loss: 0.44075277\n",
      "Balanced accuracy: 0.7631894594280658\n",
      "Sensitivity: 0.8496190049305244\n",
      "Specificity: 0.6767599139256072\n",
      "Stp score: -0.025710021163092456\n",
      "Eqop score: 0.018005464392947146\n",
      "Change has been detected in data: 1 - of index: 4706\n",
      "Change has been detected in data: 1 - of index: 4709\n",
      "Training Loss: 0.48355415\n",
      "Balanced accuracy: 0.7375237736588791\n",
      "Sensitivity: 0.7999111374407583\n",
      "Specificity: 0.675136409877\n",
      "Stp score: -0.02447160001826526\n",
      "Eqop score: 0.06924422315123113\n",
      "{'client_1': 4800, 'client_2': 4800, 'client_3': 4800}\n",
      "Change has been detected in data: 1 - of index: 4941\n",
      "Balanced accuracy: 0.762827529533632\n",
      "Sensitivity: 0.8432804674457429\n",
      "Specificity: 0.682374591621521\n",
      "Stp score: -0.020093990458764643\n",
      "Eqop score: 0.002055998756951838\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from skmultiflow.drift_detection.eddm import EDDM\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "num_clients = 3\n",
    "#adwin = ADWIN(delta =1)\n",
    "window=[]\n",
    "window_label = []\n",
    "window_warning = []\n",
    "window_label_warning = []\n",
    "pos_assigned=0\n",
    "pos_samples = 0\n",
    "neg_samples = 0\n",
    "pos_syn_samples = 0\n",
    "neg_syn_samples = 0\n",
    "generated_samples_per_sample = 0\n",
    "imbalance_ratio = 0 #of window\n",
    "minority_label=1\n",
    "majority_label = 0\n",
    "lambda_initial=0.05\n",
    "###\n",
    "ocis = 0\n",
    "classSize  = {}\n",
    "class_weights_dict = {}\n",
    "labels = []\n",
    "###\n",
    "j =0\n",
    "change=0\n",
    "warning=0\n",
    "\n",
    "disc_tol = 0.9 #0.2 original\n",
    "bal_acc_global=[] \n",
    "stp_score_global=[]\n",
    "eddm1 = EDDM()\n",
    "eddm2 = EDDM()\n",
    "eddm3 = EDDM()\n",
    "\n",
    "#one global network\n",
    "global_network = ONN(features_size=38, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "\n",
    "#3 networks for three clients\n",
    "onn_network_1 = ONN(features_size=38, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "onn_network_2 = ONN(features_size=38, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "onn_network_3 = ONN(features_size=38, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "weight = 1\n",
    "for _ in range(length):\n",
    "    sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer = [],[],[],[]\n",
    "    sum_alpha = []\n",
    "\n",
    "    for (client_name, data) in clients.items():\n",
    "        added_points = 0\n",
    "        data, label = zip(*data)\n",
    "        Y = np.asarray(label)\n",
    "        X = np.asarray(data)\n",
    "        \n",
    "        if client_name=='client_1':\n",
    "            eddm = eddm1\n",
    "            onn_network = onn_network_1\n",
    "        elif client_name=='client_2':\n",
    "            eddm = eddm2\n",
    "            onn_network = onn_network_2\n",
    "        else:\n",
    "            eddm = eddm3\n",
    "            onn_network = onn_network_3\n",
    "        i = client_index[client_name]\n",
    "        if i ==0:\n",
    "            print(client_index)\n",
    "        else:\n",
    "            if i%200==0:\n",
    "                galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer = global_network.get_weights('global')\n",
    "                onn_network.set_weights(galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer)\n",
    "        \n",
    "        if np.size(client_window[client_name])!=0:\n",
    "            if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\n",
    "                majority_count = list(client_window_label[client_name]).count(0)\n",
    "                minority_count = list(client_window_label[client_name]).count(1)\n",
    "                if majority_count >  minority_count and minority_count!=0:\n",
    "                    weight = int(majority_count/minority_count)\n",
    "        if Y[i]==minority_label:\n",
    "            onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),weight)\n",
    "            \n",
    "        else:\n",
    "            onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]))\n",
    "    \n",
    "        if np.size(client_window[client_name])==0:\n",
    "            client_window[client_name] = np.array(X[i])\n",
    "            client_window_label[client_name] = np.array(Y[i])\n",
    "        else:\n",
    "            client_window[client_name]=np.vstack((client_window[client_name],np.array(X[i])))\n",
    "            client_window_label[client_name]= np.vstack((client_window_label[client_name],np.array(Y[i])))\n",
    "        eddm.add_element(Y[i])\n",
    "    \n",
    "\n",
    "        if eddm.detected_change():\n",
    "            print('Change has been detected in data: ' + str(Y[i]) + ' - of index: ' + str(i))\n",
    "            change+=1\n",
    "            client_window[client_name] = []\n",
    "            client_window_label[client_name] = []\n",
    "    \n",
    "            \n",
    "        pos_assigned = onn_network.tp+onn_network.fp-0.2\n",
    "        pos_samples = onn_network.tp+onn_network.fn-0.2\n",
    "        \n",
    "        if np.size(client_window[client_name])!=0:\n",
    "            if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\n",
    "                if onn_network.eqop_score > 0.01:\n",
    "                    #print(onn_network.stp_score)\n",
    "                    lambda_score = lambda_initial*(1+(onn_network.eqop_score/disc_tol))\n",
    "                    if pos_assigned <= pos_samples:\n",
    "                        X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, 'min_p', p_Group,np_Group)\n",
    "                    else:\n",
    "                        X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, 'maj_np', p_Group,np_Group)\n",
    "                    if X_syn!=-1:\n",
    "                        Y_syn = np.array(Y_syn)\n",
    "                        X_syn = np.array(X_syn)\n",
    "                        X_syn, Y_syn = shuffle(X_syn, Y_syn, random_state=0)\n",
    "                        added_points = len(Y_syn)\n",
    "                        for k in range(len(X_syn)):\n",
    "                            onn_network.partial_fit(np.asarray([X_syn[k, :]]), np.asarray([Y_syn[k]]), 1)\n",
    "        \n",
    "        client_alpha, client_w_output_layer, client_b_output_layer, client_w_hidden_layer, client_b_hidden_layer = onn_network.get_weights('client')\n",
    "        i=i+1\n",
    "        client_index.update({client_name:i})\n",
    "        scaling_factor = (client_index[client_name]+added_points)/(sum(client_index.values())+added_points)\n",
    "        #scaling_factor = 1/5\n",
    "        \n",
    "        p = 0\n",
    "        if p==0:\n",
    "            if sum_alpha==[]:\n",
    "                sum_alpha = torch.mul(client_alpha, scaling_factor)\n",
    "                sum_w_output_layer = client_w_output_layer\n",
    "                sum_b_output_layer = client_b_output_layer\n",
    "                sum_w_hidden_layer = client_w_hidden_layer\n",
    "                sum_b_hidden_layer = client_b_hidden_layer\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                for j in range(onn_network.max_num_hidden_layers):\n",
    "                    sum_w_output_layer[j] = torch.mul(client_w_output_layer[j], scaling_factor)\n",
    "                    sum_b_output_layer[j] = torch.mul(client_b_output_layer[j], scaling_factor)\n",
    "                    sum_w_hidden_layer[j] = torch.mul(client_w_hidden_layer[j], scaling_factor)\n",
    "                    sum_b_hidden_layer[j] = torch.mul(client_b_hidden_layer[j], scaling_factor)\n",
    "\n",
    "            else:\n",
    "                sum_alpha = torch.add(sum_alpha, torch.mul(client_alpha, scaling_factor))\n",
    "                for j in range(onn_network.max_num_hidden_layers):\n",
    "                    sum_w_output_layer[j] = torch.add(sum_w_output_layer[j],torch.mul(client_w_output_layer[j], scaling_factor)) \n",
    "                    sum_b_output_layer[j] = torch.add(sum_b_output_layer[j],torch.mul(client_b_output_layer[j], scaling_factor))\n",
    "                    sum_w_hidden_layer[j] = torch.add(sum_w_hidden_layer[j],torch.mul(client_w_hidden_layer[j], scaling_factor)) \n",
    "                    sum_b_hidden_layer[j] = torch.add(sum_b_hidden_layer[j],torch.mul(client_b_hidden_layer[j], scaling_factor))\n",
    "        \n",
    "    if i%200==0:   \n",
    "        print(client_index)\n",
    "        global_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer)\n",
    "\n",
    "        x_test, y_test = shuffle(x_test, y_test, random_state=0)\n",
    "        \n",
    "        for m in range(len(x_test)-1):\n",
    "            prediction_1 = global_network.predict_1(np.asarray([x_test[m, :]]))\n",
    "            global_network.update_eval_metrics(prediction_1,np.asarray([y_test[m]]))\n",
    "            global_network.update_stp_score(prediction_1,np.asarray([x_test[m, :]]))\n",
    "            global_network.update_eqop_score(prediction_1,np.asarray([x_test[m, :]]),np.asarray([y_test[m]]))\n",
    "        \n",
    "        bal_acc_global.append(global_network.bal_acc)\n",
    "        stp_score_global.append(global_network.eqop_score)\n",
    "        global_network.reset_eval_metrics()\n",
    "global_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer) \n",
    "for n in range(len(x_test)-1):\n",
    "    prediction_1 = global_network.predict_1(np.asarray([x_test[n, :]]))\n",
    "    global_network.update_eval_metrics(prediction_1,np.asarray([y_test[n]]))\n",
    "    global_network.update_stp_score(prediction_1,np.asarray([x_test[n, :]]))\n",
    "    global_network.update_eqop_score(prediction_1,np.asarray([x_test[n, :]]),np.asarray([y_test[n]]))\n",
    "\n",
    "bal_acc_global.append(global_network.bal_acc)  \n",
    "stp_score_global.append(global_network.eqop_score)        #print(\"change\" + str(change))\n",
    "#print(\"warning\" + str(warning))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Balanced accuracy: \" + str(onn_network.bal_acc))\n",
    "print(\"Sensitivity: \" + str(onn_network.sen))\n",
    "print(\"Specificity: \" + str(onn_network.spec))\n",
    "print(\"Stp score: \" + str(onn_network.stp_score))\n",
    "print(\"Eqop score: \" + str(onn_network.eqop_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17fa255e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.645446917346142e-05, -5.645446917346142e-05, -0.0010904626414035556, 0.014532568880395003, -0.0024988530828894406, 0.0444593801180434, 0.005969317293132992, -0.003158478985863855, 0.031189608574423655, 0.014532568880395003, 0.01914400762551316, 0.015661658263864675, -0.004757031849828786, -0.00033575552718967394, -0.00024067431595020672, -0.008429543633956316, 0.015061753188071125, 0.008227496060072337, 0.0272377957322798, 0.007758032579576968, 0.016885828858573815, 0.04568355071275243, 0.0003238703757846295, 0.010485674827011682, 0.022061812295426808]\n"
     ]
    }
   ],
   "source": [
    "print(stp_score_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b13f68a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport warnings\\nwarnings.simplefilter(action=\\'ignore\\', category=FutureWarning)\\nfrom skmultiflow.drift_detection.eddm import EDDM\\nfrom sklearn.utils import shuffle\\nfrom sklearn.utils.class_weight import compute_class_weight\\n\\nnum_clients = 3\\n#adwin = ADWIN(delta =1)\\nwindow=[]\\nwindow_label = []\\nwindow_warning = []\\nwindow_label_warning = []\\npos_assigned=0\\npos_samples = 0\\nneg_samples = 0\\npos_syn_samples = 0\\nneg_syn_samples = 0\\ngenerated_samples_per_sample = 0\\nimbalance_ratio = 0 #of window\\nminority_label=1\\nmajority_label = 0\\nlambda_initial=0.05\\n###\\nocis = 0\\nclassSize  = {}\\nclass_weights_dict = {}\\nlabels = []\\n###\\nj =0\\nchange=0\\nwarning=0\\n\\nbal_acc_global = []  \\nstp_score_global = []  \\n\\neddm1 = EDDM()\\neddm2 = EDDM()\\neddm3 = EDDM()\\n\\n#one global network\\nglobal_network = ONN(features_size=38, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\\n\\n#3 networks for three clients\\nonn_network_1 = ONN(features_size=38, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\\nonn_network_2 = ONN(features_size=38, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\\nonn_network_3 = ONN(features_size=38, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\\n\\nweight = 1\\nfor _ in range(length):\\n    sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer = [],[],[],[]\\n    sum_alpha = []\\n    \\n    for (client_name, data) in clients.items():\\n        np_Group = 1\\n        p_Group = 0\\n        added_points = 0\\n        data, label = zip(*data)\\n        Y = np.asarray(label)\\n        X = np.asarray(data)\\n        \\n        if client_name==\\'client_1\\':\\n            eddm = eddm1\\n            onn_network = onn_network_1\\n        elif client_name==\\'client_2\\':\\n            eddm = eddm2\\n            onn_network = onn_network_2\\n        else:\\n            eddm = eddm3\\n            onn_network = onn_network_3\\n            \\n        i = client_index[client_name]\\n        if i ==0:\\n            print(client_index)\\n        else:\\n            if i%200==0:\\n                galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer = global_network.get_weights(\\'global\\')\\n                onn_network.set_weights(galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer)\\n        \\n        if np.size(client_window[client_name])!=0:\\n            if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\\n                majority_count = list(client_window_label[client_name]).count(0)\\n                minority_count = list(client_window_label[client_name]).count(1)\\n                if majority_count >  minority_count and minority_count!=0:\\n                    weight = int(majority_count/minority_count)\\n        if Y[i]==minority_label:\\n            onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),weight)\\n            \\n        else:\\n            onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),1)\\n        \\n        \\n        \\n        #onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]))\\n    \\n        if np.size(client_window[client_name])==0:\\n            client_window[client_name] = np.array(X[i])\\n            client_window_label[client_name] = np.array(Y[i])\\n        else:\\n            client_window[client_name]=np.vstack((client_window[client_name],np.array(X[i])))\\n            client_window_label[client_name]= np.vstack((client_window_label[client_name],np.array(Y[i])))\\n        eddm.add_element(Y[i])\\n    \\n\\n        if eddm.detected_change():\\n            print(\\'Change has been detected in data: \\' + str(Y[i]) + \\' - of index: \\' + str(i))\\n            change+=1\\n            client_window[client_name] = []\\n            client_window_label[client_name] = []\\n            onn_network.reset_eval_metrics()\\n            \\n        pos_assigned = onn_network.tp+onn_network.fp-0.2\\n        pos_samples = onn_network.tp+onn_network.fn-0.2\\n        \\n        if np.size(client_window[client_name])!=0:\\n            if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\\n                if onn_network.stp_score < 0:\\n                    pp_Group = np_Group\\n                    npp_Group = p_Group\\n                else:\\n                    pp_Group = p_Group\\n                    npp_Group = np_Group\\n                disc_score = abs(onn_network.stp_score)    \\n                if disc_score > 0.005:\\n                    #print(onn_network.stp_score)\\n                    lambda_score = lambda_initial*(1+(onn_network.stp_score/0.2))\\n                    if pos_assigned <= pos_samples:\\n                        X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, \\'min_p\\', pp_Group,npp_Group)\\n                    else:\\n                        X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, \\'maj_np\\', pp_Group,npp_Group)\\n                    if X_syn!=-1:\\n                        Y_syn = np.array(Y_syn)\\n                        X_syn = np.array(X_syn)\\n                        X_syn, Y_syn = shuffle(X_syn, Y_syn, random_state=0)\\n                        added_points = len(Y_syn)\\n                        for k in range(len(X_syn)):\\n                            onn_network.partial_fit(np.asarray([X_syn[k, :]]), np.asarray([Y_syn[k]]), 1, test=\\'no\\')\\n        client_alpha, client_w_output_layer, client_b_output_layer, client_w_hidden_layer, client_b_hidden_layer = onn_network.get_weights(\\'client\\')\\n        i=i+1\\n        client_index.update({client_name:i})\\n        scaling_factor = (client_index[client_name]+added_points)/(sum(client_index.values())+added_points)\\n        #scaling_factor = 1/5\\n        print(client_index)\\n        p = 0\\n        if p==0:\\n            if sum_alpha==[]:\\n                sum_alpha = torch.mul(client_alpha, scaling_factor)\\n                sum_w_output_layer = client_w_output_layer\\n                sum_b_output_layer = client_b_output_layer\\n                sum_w_hidden_layer = client_w_hidden_layer\\n                sum_b_hidden_layer = client_b_hidden_layer\\n            \\n            \\n            \\n            \\n                for j in range(onn_network.max_num_hidden_layers):\\n                    sum_w_output_layer[j] = torch.mul(client_w_output_layer[j], scaling_factor)\\n                    sum_b_output_layer[j] = torch.mul(client_b_output_layer[j], scaling_factor)\\n                    sum_w_hidden_layer[j] = torch.mul(client_w_hidden_layer[j], scaling_factor)\\n                    sum_b_hidden_layer[j] = torch.mul(client_b_hidden_layer[j], scaling_factor)\\n\\n            else:\\n                sum_alpha = torch.add(sum_alpha, torch.mul(client_alpha, scaling_factor))\\n                for j in range(onn_network.max_num_hidden_layers):\\n                    sum_w_output_layer[j] = torch.add(sum_w_output_layer[j],torch.mul(client_w_output_layer[j], scaling_factor)) \\n                    sum_b_output_layer[j] = torch.add(sum_b_output_layer[j],torch.mul(client_b_output_layer[j], scaling_factor))\\n                    sum_w_hidden_layer[j] = torch.add(sum_w_hidden_layer[j],torch.mul(client_w_hidden_layer[j], scaling_factor)) \\n                    sum_b_hidden_layer[j] = torch.add(sum_b_hidden_layer[j],torch.mul(client_b_hidden_layer[j], scaling_factor))\\n        \\n    if i%200==0:    \\n        global_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer)\\n\\n        x_test, y_test = shuffle(x_test, y_test, random_state=0)\\n        \\n        for m in range(len(x_test)-1):\\n            prediction_1 = global_network.predict_1(np.asarray([x_test[m, :]]))\\n            global_network.update_eval_metrics(prediction_1,np.asarray([y_test[m]]))\\n            global_network.update_stp_score(prediction_1,np.asarray([x_test[m, :]]))\\n            global_network.update_eqop_score(prediction_1,np.asarray([x_test[m, :]]),np.asarray([y_test[m]]))\\n        \\n        bal_acc_global.append(global_network.bal_acc)\\n        stp_score_global.append(global_network.stp_score)\\n        global_network.reset_eval_metrics()\\nglobal_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer) \\nfor n in range(len(x_test)-1):\\n    prediction_1 = global_network.predict_1(np.asarray([x_test[n, :]]))\\n    global_network.update_eval_metrics(prediction_1,np.asarray([y_test[n]]))\\n    global_network.update_stp_score(prediction_1,np.asarray([x_test[n, :]]))\\n    global_network.update_eqop_score(prediction_1,np.asarray([x_test[n, :]]),np.asarray([y_test[n]]))\\n\\nbal_acc_global.append(global_network.bal_acc)  \\nstp_score_global.append(global_network.stp_score)  \\n        \\n        #print(\"change\" + str(change))\\n#print(\"warning\" + str(warning))\\n    \\n    \\n    \\n    \\n    \\nprint(\"Balanced accuracy: \" + str(onn_network.bal_acc))\\nprint(\"Sensitivity: \" + str(onn_network.sen))\\nprint(\"Specificity: \" + str(onn_network.spec))\\nprint(\"Stp score: \" + str(onn_network.stp_score))\\nprint(\"Eqop score: \" + str(onn_network.eqop_score))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from skmultiflow.drift_detection.eddm import EDDM\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "num_clients = 3\n",
    "#adwin = ADWIN(delta =1)\n",
    "window=[]\n",
    "window_label = []\n",
    "window_warning = []\n",
    "window_label_warning = []\n",
    "pos_assigned=0\n",
    "pos_samples = 0\n",
    "neg_samples = 0\n",
    "pos_syn_samples = 0\n",
    "neg_syn_samples = 0\n",
    "generated_samples_per_sample = 0\n",
    "imbalance_ratio = 0 #of window\n",
    "minority_label=1\n",
    "majority_label = 0\n",
    "lambda_initial=0.05\n",
    "###\n",
    "ocis = 0\n",
    "classSize  = {}\n",
    "class_weights_dict = {}\n",
    "labels = []\n",
    "###\n",
    "j =0\n",
    "change=0\n",
    "warning=0\n",
    "\n",
    "bal_acc_global = []  \n",
    "stp_score_global = []  \n",
    "\n",
    "eddm1 = EDDM()\n",
    "eddm2 = EDDM()\n",
    "eddm3 = EDDM()\n",
    "\n",
    "#one global network\n",
    "global_network = ONN(features_size=38, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "\n",
    "#3 networks for three clients\n",
    "onn_network_1 = ONN(features_size=38, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "onn_network_2 = ONN(features_size=38, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "onn_network_3 = ONN(features_size=38, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "\n",
    "weight = 1\n",
    "for _ in range(length):\n",
    "    sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer = [],[],[],[]\n",
    "    sum_alpha = []\n",
    "    \n",
    "    for (client_name, data) in clients.items():\n",
    "        np_Group = 1\n",
    "        p_Group = 0\n",
    "        added_points = 0\n",
    "        data, label = zip(*data)\n",
    "        Y = np.asarray(label)\n",
    "        X = np.asarray(data)\n",
    "        \n",
    "        if client_name=='client_1':\n",
    "            eddm = eddm1\n",
    "            onn_network = onn_network_1\n",
    "        elif client_name=='client_2':\n",
    "            eddm = eddm2\n",
    "            onn_network = onn_network_2\n",
    "        else:\n",
    "            eddm = eddm3\n",
    "            onn_network = onn_network_3\n",
    "            \n",
    "        i = client_index[client_name]\n",
    "        if i ==0:\n",
    "            print(client_index)\n",
    "        else:\n",
    "            if i%200==0:\n",
    "                galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer = global_network.get_weights('global')\n",
    "                onn_network.set_weights(galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer)\n",
    "        \n",
    "        if np.size(client_window[client_name])!=0:\n",
    "            if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\n",
    "                majority_count = list(client_window_label[client_name]).count(0)\n",
    "                minority_count = list(client_window_label[client_name]).count(1)\n",
    "                if majority_count >  minority_count and minority_count!=0:\n",
    "                    weight = int(majority_count/minority_count)\n",
    "        if Y[i]==minority_label:\n",
    "            onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),weight)\n",
    "            \n",
    "        else:\n",
    "            onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]))\n",
    "    \n",
    "        if np.size(client_window[client_name])==0:\n",
    "            client_window[client_name] = np.array(X[i])\n",
    "            client_window_label[client_name] = np.array(Y[i])\n",
    "        else:\n",
    "            client_window[client_name]=np.vstack((client_window[client_name],np.array(X[i])))\n",
    "            client_window_label[client_name]= np.vstack((client_window_label[client_name],np.array(Y[i])))\n",
    "        eddm.add_element(Y[i])\n",
    "    \n",
    "\n",
    "        if eddm.detected_change():\n",
    "            print('Change has been detected in data: ' + str(Y[i]) + ' - of index: ' + str(i))\n",
    "            change+=1\n",
    "            client_window[client_name] = []\n",
    "            client_window_label[client_name] = []\n",
    "            onn_network.reset_eval_metrics()\n",
    "            \n",
    "        pos_assigned = onn_network.tp+onn_network.fp-0.2\n",
    "        pos_samples = onn_network.tp+onn_network.fn-0.2\n",
    "        \n",
    "        if np.size(client_window[client_name])!=0:\n",
    "            if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\n",
    "                if onn_network.stp_score < 0:\n",
    "                    pp_Group = np_Group\n",
    "                    npp_Group = p_Group\n",
    "                else:\n",
    "                    pp_Group = p_Group\n",
    "                    npp_Group = np_Group\n",
    "                disc_score = abs(onn_network.stp_score)    \n",
    "                if disc_score > 0.005:\n",
    "                    #print(onn_network.stp_score)\n",
    "                    lambda_score = lambda_initial*(1+(onn_network.stp_score/0.2))\n",
    "                    if pos_assigned <= pos_samples:\n",
    "                        X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, 'min_p', pp_Group,npp_Group)\n",
    "                    else:\n",
    "                        X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, 'maj_np', pp_Group,npp_Group)\n",
    "                    if X_syn!=-1:\n",
    "                        Y_syn = np.array(Y_syn)\n",
    "                        X_syn = np.array(X_syn)\n",
    "                        X_syn, Y_syn = shuffle(X_syn, Y_syn, random_state=0)\n",
    "                        added_points = len(Y_syn)\n",
    "                        for k in range(len(X_syn)):\n",
    "                            onn_network.partial_fit(np.asarray([X_syn[k, :]]), np.asarray([Y_syn[k]]), 1, test='no')\n",
    "        client_alpha, client_w_output_layer, client_b_output_layer, client_w_hidden_layer, client_b_hidden_layer = onn_network.get_weights('client')\n",
    "        i=i+1\n",
    "        client_index.update({client_name:i})\n",
    "        scaling_factor = (client_index[client_name]+added_points)/(sum(client_index.values())+added_points)\n",
    "        #scaling_factor = 1/5\n",
    "        print(client_index)\n",
    "        p = 0\n",
    "        if p==0:\n",
    "            if sum_alpha==[]:\n",
    "                sum_alpha = torch.mul(client_alpha, scaling_factor)\n",
    "                sum_w_output_layer = client_w_output_layer\n",
    "                sum_b_output_layer = client_b_output_layer\n",
    "                sum_w_hidden_layer = client_w_hidden_layer\n",
    "                sum_b_hidden_layer = client_b_hidden_layer\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                for j in range(onn_network.max_num_hidden_layers):\n",
    "                    sum_w_output_layer[j] = torch.mul(client_w_output_layer[j], scaling_factor)\n",
    "                    sum_b_output_layer[j] = torch.mul(client_b_output_layer[j], scaling_factor)\n",
    "                    sum_w_hidden_layer[j] = torch.mul(client_w_hidden_layer[j], scaling_factor)\n",
    "                    sum_b_hidden_layer[j] = torch.mul(client_b_hidden_layer[j], scaling_factor)\n",
    "\n",
    "            else:\n",
    "                sum_alpha = torch.add(sum_alpha, torch.mul(client_alpha, scaling_factor))\n",
    "                for j in range(onn_network.max_num_hidden_layers):\n",
    "                    sum_w_output_layer[j] = torch.add(sum_w_output_layer[j],torch.mul(client_w_output_layer[j], scaling_factor)) \n",
    "                    sum_b_output_layer[j] = torch.add(sum_b_output_layer[j],torch.mul(client_b_output_layer[j], scaling_factor))\n",
    "                    sum_w_hidden_layer[j] = torch.add(sum_w_hidden_layer[j],torch.mul(client_w_hidden_layer[j], scaling_factor)) \n",
    "                    sum_b_hidden_layer[j] = torch.add(sum_b_hidden_layer[j],torch.mul(client_b_hidden_layer[j], scaling_factor))\n",
    "        \n",
    "    if i%200==0:    \n",
    "        global_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer)\n",
    "\n",
    "        x_test, y_test = shuffle(x_test, y_test, random_state=0)\n",
    "        \n",
    "        for m in range(len(x_test)-1):\n",
    "            prediction_1 = global_network.predict_1(np.asarray([x_test[m, :]]))\n",
    "            global_network.update_eval_metrics(prediction_1,np.asarray([y_test[m]]))\n",
    "            global_network.update_stp_score(prediction_1,np.asarray([x_test[m, :]]))\n",
    "            global_network.update_eqop_score(prediction_1,np.asarray([x_test[m, :]]),np.asarray([y_test[m]]))\n",
    "        \n",
    "        bal_acc_global.append(global_network.bal_acc)\n",
    "        stp_score_global.append(global_network.stp_score)\n",
    "        global_network.reset_eval_metrics()\n",
    "global_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer) \n",
    "for n in range(len(x_test)-1):\n",
    "    prediction_1 = global_network.predict_1(np.asarray([x_test[n, :]]))\n",
    "    global_network.update_eval_metrics(prediction_1,np.asarray([y_test[n]]))\n",
    "    global_network.update_stp_score(prediction_1,np.asarray([x_test[n, :]]))\n",
    "    global_network.update_eqop_score(prediction_1,np.asarray([x_test[n, :]]),np.asarray([y_test[n]]))\n",
    "\n",
    "bal_acc_global.append(global_network.bal_acc)  \n",
    "stp_score_global.append(global_network.stp_score)  \n",
    "        \n",
    "        #print(\"change\" + str(change))\n",
    "#print(\"warning\" + str(warning))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Balanced accuracy: \" + str(onn_network.bal_acc))\n",
    "print(\"Sensitivity: \" + str(onn_network.sen))\n",
    "print(\"Specificity: \" + str(onn_network.spec))\n",
    "print(\"Stp score: \" + str(onn_network.stp_score))\n",
    "print(\"Eqop score: \" + str(onn_network.eqop_score))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "483aa71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy: 0.792393467763441\n",
      "Sensitivity: 0.8255243802778535\n",
      "Specificity: 0.7592625552490285\n",
      "Gmean: 0.7917005433812293\n",
      "Stp score: 0.016178132185273364\n",
      "Eqop score: 0.003731553054831638\n"
     ]
    }
   ],
   "source": [
    "print(\"Balanced accuracy: \" + str(onn_network.bal_acc))\n",
    "print(\"Sensitivity: \" + str(onn_network.sen))\n",
    "print(\"Specificity: \" + str(onn_network.spec))\n",
    "print(\"Gmean: \" + str(math.sqrt(onn_network.sen*onn_network.spec)))\n",
    "print(\"Stp score: \" + str(onn_network.stp_score))\n",
    "print(\"Eqop score: \" + str(onn_network.eqop_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e8039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbd622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
