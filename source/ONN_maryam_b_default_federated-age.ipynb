{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140e5580",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import collections\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from mab import algs\n",
    "\n",
    "\n",
    "class ONN(nn.Module):\n",
    "    def __init__(self, features_size, max_num_hidden_layers, qtd_neuron_per_hidden_layer, n_classes, batch_size=1,\n",
    "                 b=0.99, n=0.01, s=0.2, use_cuda=False):\n",
    "        super(ONN, self).__init__()\n",
    "\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            print(\"Using CUDA :]\")\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "\n",
    "        self.features_size = features_size\n",
    "        self.max_num_hidden_layers = max_num_hidden_layers\n",
    "        self.qtd_neuron_per_hidden_layer = qtd_neuron_per_hidden_layer\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.b = Parameter(torch.tensor(\n",
    "            b), requires_grad=False).to(self.device)\n",
    "        self.n = Parameter(torch.tensor(\n",
    "            n), requires_grad=False).to(self.device)\n",
    "        self.s = Parameter(torch.tensor(\n",
    "            s), requires_grad=False).to(self.device)\n",
    "\n",
    "        self.hidden_layers = []\n",
    "        self.output_layers = []\n",
    "\n",
    "        self.hidden_layers.append(\n",
    "            nn.Linear(features_size, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        for i in range(max_num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(\n",
    "                nn.Linear(qtd_neuron_per_hidden_layer, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        for i in range(max_num_hidden_layers):\n",
    "            self.output_layers.append(\n",
    "                nn.Linear(qtd_neuron_per_hidden_layer, n_classes))\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList(self.hidden_layers).to(self.device)\n",
    "        self.output_layers = nn.ModuleList(self.output_layers).to(self.device)\n",
    "\n",
    "        self.alpha = Parameter(torch.Tensor(self.max_num_hidden_layers).fill_(1 / (self.max_num_hidden_layers + 1)),\n",
    "                               requires_grad=False).to(\n",
    "            self.device)\n",
    "\n",
    "        self.loss_array = []\n",
    "        self.tp=0.1\n",
    "        self.tn=0.1\n",
    "        self.fp=0.1\n",
    "        self.fn=0.1\n",
    "        self.bal_acc = 0\n",
    "        self.bal_acc_list = []\n",
    "        self.sen = 0\n",
    "        self.spec = 0\n",
    "        \n",
    "        self.protected_pos = 0.1\n",
    "        self.protected_neg = 0.1\n",
    "        self.non_protected_pos = 0.1\n",
    "        self.non_protected_neg = 0.1\n",
    "        self.stp_score=0\n",
    "        \n",
    "        self.tp_protected = 0.1\n",
    "        self.fn_protected = 0.1\n",
    "        self.tp_non_protected = 0.1\n",
    "        self.fn_non_protected = 0.1\n",
    "        self.eqop_score=0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            self.output_layers[i].weight.grad.data.fill_(0)\n",
    "            self.output_layers[i].bias.grad.data.fill_(0)\n",
    "            self.hidden_layers[i].weight.grad.data.fill_(0)\n",
    "            self.hidden_layers[i].bias.grad.data.fill_(0)\n",
    "\n",
    "    def update_weights(self, X, Y, weight, show_loss, test):\n",
    "        Y = torch.from_numpy(Y).to(self.device)\n",
    "        if test=='yes':\n",
    "            prediction_1 = self.predict_1(X)\n",
    "            self.update_eval_metrics(prediction_1,Y)\n",
    "            self.update_stp_score(prediction_1,X)\n",
    "            self.update_eqop_score(prediction_1,X,Y)\n",
    "        \n",
    "        predictions_per_layer = self.forward(X)\n",
    "        losses_per_layer = []\n",
    "\n",
    "        for out in predictions_per_layer:\n",
    "            criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "            loss = criterion(out.view(self.batch_size, self.n_classes), Y.view(\n",
    "                self.batch_size).long())\n",
    "            losses_per_layer.append(loss*weight)\n",
    "\n",
    "        w = [None] * len(losses_per_layer)\n",
    "        b = [None] * len(losses_per_layer)\n",
    "        \n",
    "        with torch.no_grad():     #Context-manager that disabled gradient calculation\n",
    "            self.weights_output_layers = list()\n",
    "            self.biases_output_layers = list()\n",
    "            for i in range(len(losses_per_layer)):\n",
    "                losses_per_layer[i].backward(retain_graph=True)\n",
    "                self.output_layers[i].weight.data -= self.n * \\\n",
    "                                                   self.alpha[i] * self.output_layers[i].weight.grad.data\n",
    "                self.output_layers[i].bias.data -= self.n * \\\n",
    "                                                 self.alpha[i] * self.output_layers[i].bias.grad.data\n",
    "                self.weights_output_layers.append(self.output_layers[i].weight.data) \n",
    "                self.biases_output_layers.append(self.output_layers[i].bias.data)\n",
    "                \n",
    "                for j in range(i + 1):\n",
    "                    if w[j] is None:\n",
    "                        w[j] = self.alpha[i] * self.hidden_layers[j].weight.grad.data\n",
    "                        b[j] = self.alpha[i] * self.hidden_layers[j].bias.grad.data\n",
    "                    else:\n",
    "                        w[j] += self.alpha[i] * self.hidden_layers[j].weight.grad.data\n",
    "                        b[j] += self.alpha[i] * self.hidden_layers[j].bias.grad.data\n",
    "                self.zero_grad()\n",
    "            self.weights_hidden_layers= list()\n",
    "            self.biases_hidden_layers = list()\n",
    "            \n",
    "            for i in range(len(losses_per_layer)):\n",
    "                self.hidden_layers[i].weight.data -= self.n * w[i]\n",
    "                self.hidden_layers[i].bias.data -= self.n * b[i]\n",
    "                self.weights_hidden_layers.append(self.hidden_layers[i].weight.data)\n",
    "                self.biases_hidden_layers.append(self.hidden_layers[i].bias.data)\n",
    "            for i in range(len(losses_per_layer)):\n",
    "                self.alpha[i] *= torch.pow(self.b, losses_per_layer[i])\n",
    "                self.alpha[i] = torch.max(\n",
    "                  self.alpha[i], self.s / self.max_num_hidden_layers)\n",
    "\n",
    "        z_t = torch.sum(self.alpha)\n",
    "\n",
    "        self.alpha = Parameter(\n",
    "            self.alpha / z_t, requires_grad=False).to(self.device)\n",
    "        if show_loss:\n",
    "            \n",
    "            real_output = torch.sum(torch.mul(\n",
    "                self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, self.batch_size).view(\n",
    "                    self.max_num_hidden_layers, self.batch_size, 1), predictions_per_layer), 0)\n",
    "            criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "            loss = criterion(real_output.view(self.batch_size, self.n_classes), Y.view(self.batch_size).long())\n",
    "            self.loss_array.append(loss)\n",
    "            if (len(self.loss_array) % 1000) == 0:\n",
    "                #print(\"WARNING: Set 'show_loss' to 'False' when not debugging. \"\n",
    "                #      \"It will deteriorate the fitting performance.\")\n",
    "                loss = torch.Tensor(self.loss_array).mean().cpu().numpy()\n",
    "                #print(\"Alpha:\" + str(self.alpha.data.cpu().numpy()))\n",
    "                print(\"Training Loss: \" + str(loss))\n",
    "                print(\"Balanced accuracy: \" + str(self.bal_acc))\n",
    "                print(\"Sensitivity: \" + str(self.sen))\n",
    "                print(\"Specificity: \" + str(self.spec))\n",
    "                print(\"Stp score: \" + str(self.stp_score))\n",
    "                print(\"Eqop score: \" + str(self.eqop_score))\n",
    "                self.loss_array.clear()\n",
    "\n",
    "    def get_weights(self,network_name):\n",
    "        \n",
    "        if network_name == 'global':\n",
    "            weights_output_layers = list()\n",
    "            biases_output_layers = list()\n",
    "            weights_hidden_layers = list()\n",
    "            biases_hidden_layers= list()\n",
    "            for i in range(self.max_num_hidden_layers):\n",
    "                weights_output_layers.append(self.output_layers[i].weight.data) \n",
    "                biases_output_layers.append(self.output_layers[i].bias.data)\n",
    "                weights_hidden_layers.append(self.hidden_layers[i].weight.data)\n",
    "                biases_hidden_layers.append(self.hidden_layers[i].bias.data)\n",
    "               \n",
    "            return self.alpha, weights_output_layers, biases_output_layers, weights_hidden_layers, biases_hidden_layers\n",
    "        else:\n",
    "            return self.alpha, self.weights_output_layers, self.biases_output_layers, self.weights_hidden_layers, self.biases_hidden_layers\n",
    "    \n",
    "    def set_weights(self, alpha, w_output_layer, b_output_layer, w_hidden_layer, b_hidden_layer):\n",
    "        \n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            self.alpha[i] = alpha[i].clone().detach()\n",
    "            self.output_layers[i].weight.data = w_output_layer[i].clone().detach()\n",
    "            self.output_layers[i].bias.data = b_output_layer [i].clone().detach()\n",
    "            self.hidden_layers[i].weight.data =  w_hidden_layer [i].clone().detach()\n",
    "            self.hidden_layers[i].bias.data = b_hidden_layer [i].clone().detach()    \n",
    "    def forward(self, X):\n",
    "        hidden_connections = []\n",
    "\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "\n",
    "        x = F.relu(self.hidden_layers[0](X))\n",
    "        hidden_connections.append(x)\n",
    "\n",
    "        for i in range(1, self.max_num_hidden_layers):\n",
    "            hidden_connections.append(\n",
    "                F.relu(self.hidden_layers[i](hidden_connections[i - 1])))\n",
    "\n",
    "        output_class = []\n",
    "\n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            output_class.append(self.output_layers[i](hidden_connections[i]))\n",
    "\n",
    "        pred_per_layer = torch.stack(output_class)\n",
    "\n",
    "        return pred_per_layer\n",
    "\n",
    "    def validate_input_X(self, data):\n",
    "        \n",
    "        if len(data.shape) != 2:\n",
    "            raise Exception(\n",
    "                \"Wrong dimension for this X data. It should have only two dimensions.\")\n",
    "\n",
    "    def validate_input_Y(self, data):\n",
    "        if len(data.shape) != 1:\n",
    "            raise Exception(\n",
    "                \"Wrong dimension for this Y data. It should have only one dimensions.\")\n",
    "\n",
    "    def partial_fit_(self, X_data, Y_data, weight, show_loss=True, test='yes'):\n",
    "        \n",
    "        self.validate_input_X(X_data)\n",
    "        self.validate_input_Y(Y_data)\n",
    "        self.update_weights(X_data, Y_data, weight, show_loss, test)\n",
    "\n",
    "    def partial_fit(self, X_data, Y_data, weight, show_loss=True, test='yes'):\n",
    "        \n",
    "        self.partial_fit_(X_data, Y_data, weight, show_loss, test)\n",
    "\n",
    "    def reset_eval_metrics(self):\n",
    "        self.tp=0.1\n",
    "        self.tn=0.1\n",
    "        self.fp=0.1\n",
    "        self.fn=0.1\n",
    "        self.bal_acc = 0\n",
    "        self.bal_acc_list = []\n",
    "        self.sen = 0\n",
    "        self.spec = 0\n",
    "        \n",
    "        self.protected_pos = 0.1\n",
    "        self.protected_neg = 0.1\n",
    "        self.non_protected_pos = 0.1\n",
    "        self.non_protected_neg = 0.1\n",
    "        self.stp_score=0\n",
    "        \n",
    "        self.tp_protected = 0.1\n",
    "        self.fn_protected = 0.1\n",
    "        self.tp_non_protected = 0.1\n",
    "        self.fn_non_protected = 0.1\n",
    "        self.eqop_score=0\n",
    "    def update_eval_metrics(self,prediction_1,Y):\n",
    "        if prediction_1==1 and Y==1:\n",
    "            self.tp+=1\n",
    "        elif prediction_1==1 and Y==0:\n",
    "            self.fp+=1\n",
    "        elif prediction_1==0 and Y==1:\n",
    "            self.fn+=1\n",
    "        else:\n",
    "            self.tn+=1\n",
    "        \n",
    "        self.sen = self.tp/(self.tp + self.fn)\n",
    "        self.spec= self.tn/(self.tn + self.fp)\n",
    "        self.bal_acc = (self.sen + self.spec)/2\n",
    "        self.bal_acc_list.append(self.bal_acc)\n",
    "            \n",
    "    def update_stp_score(self,prediction_1,X):\n",
    "        if X[0][sa_index] == p_Group:\n",
    "            if prediction_1 == 1:\n",
    "                self.protected_pos += 1.\n",
    "            else:\n",
    "                self.protected_neg += 1.\n",
    "        else:\n",
    "            if prediction_1 == 1:\n",
    "                self.non_protected_pos += 1.\n",
    "            else:\n",
    "                self.non_protected_neg += 1.\n",
    "            \n",
    "        C_prot = (self.protected_pos) / (self.protected_pos + self.protected_neg)\n",
    "        C_non_prot = (self.non_protected_pos) / (self.non_protected_pos + self.non_protected_neg)\n",
    "\n",
    "        self.stp_score = C_non_prot - C_prot\n",
    "    \n",
    "    def update_eqop_score(self,prediction_1,X,Y):\n",
    "        if X[0][sa_index] == p_Group:\n",
    "            if prediction_1==1 and Y==1:\n",
    "                self.tp_protected += 1.\n",
    "            elif prediction_1==0 and Y==1:\n",
    "                self.fn_protected += 1.\n",
    "        else:\n",
    "            if prediction_1==1 and Y==1:\n",
    "                self.tp_non_protected += 1.\n",
    "            elif prediction_1==0 and Y==1:\n",
    "                self.fn_non_protected += 1.\n",
    "            \n",
    "        tpr_protected = self.tp_protected / (self.tp_protected + self.fn_protected)\n",
    "        tpr_non_protected = self.tp_non_protected / (self.tp_non_protected + self.fn_non_protected)\n",
    "        self.eqop_score = tpr_non_protected - tpr_protected\n",
    "    \n",
    "    def predict_1(self, X_data):\n",
    "        self.validate_input_X(X_data)\n",
    "        return torch.argmax(torch.sum(torch.mul(\n",
    "            self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, 1).view(\n",
    "                self.max_num_hidden_layers, len(X_data), 1), self.forward(X_data)), 0), dim=1).cpu().numpy()\n",
    "    def predict_(self, X_data):\n",
    "        self.validate_input_X(X_data)\n",
    "        return torch.argmax(torch.sum(torch.mul(\n",
    "            self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, len(X_data)).view(\n",
    "                self.max_num_hidden_layers, len(X_data), 1), self.forward(X_data)), 0), dim=1).cpu().numpy()\n",
    "\n",
    "    def predict(self, X_data):\n",
    "        pred = self.predict_(X_data)\n",
    "        return pred\n",
    "\n",
    "    def export_params_to_json(self):\n",
    "        state_dict = self.state_dict()\n",
    "        params_gp = {}\n",
    "        for key, tensor in state_dict.items():\n",
    "            params_gp[key] = tensor.cpu().numpy().tolist()\n",
    "\n",
    "        return json.dumps(params_gp)\n",
    "\n",
    "    def load_params_from_json(self, json_data):\n",
    "        params = json.loads(json_data)\n",
    "        o_dict = collections.OrderedDict()\n",
    "        for key, tensor in params.items():\n",
    "            o_dict[key] = torch.tensor(tensor).to(self.device)\n",
    "        self.load_state_dict(o_dict)\n",
    "\n",
    "\n",
    "class ONN_THS(ONN):\n",
    "    def __init__(self, features_size, max_num_hidden_layers, qtd_neuron_per_hidden_layer, n_classes, b=0.99, n=0.01,\n",
    "                 s=0.2, e=[0.5, 0.35, 0.2, 0.1, 0.05], use_cuda=False):\n",
    "        super().__init__(features_size, max_num_hidden_layers, qtd_neuron_per_hidden_layer, n_classes, b=b, n=n, s=s,\n",
    "                         use_cuda=use_cuda)\n",
    "        self.e = Parameter(torch.tensor(e), requires_grad=False)\n",
    "        self.arms_values = Parameter(\n",
    "            torch.arange(n_classes), requires_grad=False)\n",
    "        self.explorations_mab = []\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            self.explorations_mab.append(algs.ThompsomSampling(len(e)))\n",
    "\n",
    "    def partial_fit(self, X_data, Y_data, exp_factor, show_loss=True):\n",
    "        self.partial_fit_(X_data, Y_data, show_loss)\n",
    "        self.explorations_mab[Y_data[0]].reward(exp_factor)\n",
    "\n",
    "    def predict(self, X_data):\n",
    "        pred = self.predict_(X_data)[0]\n",
    "        exp_factor = self.explorations_mab[pred].select()[0]\n",
    "        if np.random.uniform() < self.e[exp_factor]:\n",
    "            removed_arms = self.arms_values.clone().numpy().tolist()\n",
    "            removed_arms.remove(pred)\n",
    "            return random.choice(removed_arms), exp_factor\n",
    "\n",
    "        return pred, exp_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30aa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from imblearn.datasets import make_imbalance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e47b4179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib2\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import preprocessing\n",
    "from random import seed, shuffle\n",
    "\n",
    "# import utils as ut\n",
    "\n",
    "SEED = 1234\n",
    "seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "def load_default():\n",
    "    FEATURES_CLASSIFICATION = [\"LIMIT_BAL\",\"SEX\",\"EDUCATION\",\"MARRIAGE\",\"AGE\",\"PAY_0\",\"PAY_2\",\"PAY_3\",\"PAY_4\",\"PAY_5\",\"PAY_6\",\"BILL_AMT1\",\"BILL_AMT2\",\"BILL_AMT3\",\"BILL_AMT4\",\"BILL_AMT5\",\"BILL_AMT6\",\"PAY_AMT1\",\"PAY_AMT2\",\"PAY_AMT3\",\"PAY_AMT4\",\"PAY_AMT5\",\"PAY_AMT6\"]  # features to be used for classification\n",
    "    CONT_VARIABLES = [\"AGE\",\"BILL_AMT1\",\"BILL_AMT2\",\"BILL_AMT3\",\"BILL_AMT4\",\"BILL_AMT5\",\"BILL_AMT6\",\"PAY_AMT1\",\"PAY_AMT2\",\"PAY_AMT3\",\"PAY_AMT4\",\"PAY_AMT5\",\"PAY_AMT6\"]  # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"y\"  # the decision variable\n",
    "    SENSITIVE_ATTRS = [\"SEX\"]\n",
    "    CAT_VARIABLES = [\"LIMIT_BAL\",\"SEX\",\"EDUCATION\",\"MARRIAGE\",\"PAY_0\",\"PAY_2\",\"PAY_3\",\"PAY_4\",\"PAY_5\",\"PAY_6\"]\n",
    "    CAT_VARIABLES_INDICES = [0,2,3,5,6,7,8,9,10]\n",
    "    \n",
    "    INPUT_FILE = \"./datasets/default.csv\"\n",
    "\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # convert to np array\n",
    "    data = df.to_dict('list')\n",
    "    \n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "\n",
    "    # convert class label 0 to -1\n",
    "    y = data[CLASS_FEATURE]\n",
    "    #y[y == \"yes\"] = 1\n",
    "    #y[y == 'no'] = -1\n",
    "    y = np.array([int(k) for k in y])\n",
    "\n",
    "    X = np.array([]).reshape(len(y), 0)  # empty array with num rows same as num examples, will hstack the features to it\n",
    "    \n",
    "    x_control = defaultdict(list)\n",
    "    i=0\n",
    "    feature_names = []\n",
    "    \n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        vals = data[attr]\n",
    "        \n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            vals = preprocessing.scale(vals)  # 0 mean and 1 variance\n",
    "            vals = np.reshape(vals, (len(y), -1))  # convert from 1-d arr to a 2-d arr with one col\n",
    "            \n",
    "\n",
    "        else:  # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            lb = preprocessing.LabelBinarizer()\n",
    "            lb.fit(vals)\n",
    "            vals = lb.transform(vals)\n",
    "            '''\n",
    "            if attr == 'SEX':\n",
    "                print(lb.classes_)\n",
    "                print(lb.transform(lb.classes_))\n",
    "            '''\n",
    "            #if attr == 'job':\n",
    "            #    print(lb.classes_)\n",
    "            #    print(lb.transform(lb.classes_))\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "        # add to sensitive features dict\n",
    "        if attr in SENSITIVE_ATTRS:\n",
    "            x_control[attr] = vals\n",
    "            \n",
    "\n",
    "        # add to learnable features\n",
    "        X = np.hstack((X, vals))\n",
    "        \n",
    "        if attr in CONT_VARIABLES:  # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else:  # categorical features\n",
    "            if vals.shape[1] == 1:  # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in lb.classes_:  # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "    # convert the sensitive feature to 1-d array\n",
    "    \n",
    "    x_control = dict(x_control)\n",
    "    \n",
    "    for k in x_control.keys():\n",
    "        assert (x_control[k].shape[1] == 1)  # make sure that the sensitive feature is binary after one hot encoding\n",
    "        x_control[k] = np.array(x_control[k]).flatten()\n",
    "    \n",
    "    feature_names.append('target')\n",
    "    # '0' is 'female'\n",
    "   \n",
    "    \n",
    "    return X, y, feature_names.index(SENSITIVE_ATTRS[0]), 0, x_control\n",
    "\n",
    "X,y, sa_index, p_Group, x_control= load_default()\n",
    "\n",
    "np_Group = 1 #non-protected group's sa_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be6c708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPAS_INPUT_FILE = \"./datasets/default.csv\"\n",
    "\n",
    "df = pd.read_csv(COMPAS_INPUT_FILE)\n",
    "age_group_1 = []\n",
    "age_group_2 = []\n",
    "age_group_3 = []\n",
    "for i in range(len(df)):\n",
    "    if df['AGE'].iloc[i]>0 and df['AGE'].iloc[i]<30:\n",
    "        age_group_1.append(i)\n",
    "    elif df['AGE'].iloc[i]>29 and df['AGE'].iloc[i]<40:\n",
    "        age_group_2.append(i)\n",
    "    elif df['AGE'].iloc[i]>39:\n",
    "        age_group_3.append(i)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4109a1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bismillah\n",
      "bismillah\n",
      "bismillah\n",
      "9144\n",
      "9144\n",
      "[0 0 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "Xtr1 = np.empty((0,0))\n",
    "Ytr1 = np.empty(0)\n",
    "for i in age_group_1:\n",
    "    if np.size(Xtr1)==0:\n",
    "        print(\"bismillah\")\n",
    "        Xtr1 = X[i]\n",
    "        Ytr1 = y[i]\n",
    "    else:\n",
    "        Xtr1 = np.vstack((Xtr1,X[i]))\n",
    "        Ytr1 = np.append(Ytr1,y[i])\n",
    "\n",
    "Xtr2 = np.empty((0,0))\n",
    "Ytr2 = np.empty(0)\n",
    "for i in age_group_2:\n",
    "    if np.size(Xtr2)==0:\n",
    "        print(\"bismillah\")\n",
    "        Xtr2 = X[i]\n",
    "        Ytr2 = y[i]\n",
    "    else:\n",
    "        Xtr2 = np.vstack((Xtr2,X[i]))\n",
    "        Ytr2 = np.append(Ytr2,y[i])\n",
    "        \n",
    "Xtr3 = np.empty((0,0))\n",
    "Ytr3 = np.empty(0)\n",
    "for i in age_group_3:\n",
    "    if np.size(Xtr3)==0:\n",
    "        print(\"bismillah\")\n",
    "        Xtr3 = X[i]\n",
    "        Ytr3 = y[i]\n",
    "    else:\n",
    "        Xtr3 = np.vstack((Xtr3,X[i]))\n",
    "        Ytr3 = np.append(Ytr3,y[i])\n",
    "print(len(Xtr3))\n",
    "print(len(Ytr3))\n",
    "print(Ytr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84fb6256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 6981, 1: 2163}\n"
     ]
    }
   ],
   "source": [
    "labels = Ytr3\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "count_ap_dict = dict(zip(unique, counts))\n",
    "print(count_ap_dict)\n",
    "minority_label=1\n",
    "majority_label = 0        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c677ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = {}\n",
    "client_data_testx = []\n",
    "client_data_testy = []\n",
    "x_train, x_test, y_train, y_test = train_test_split(Xtr1,Ytr1,test_size=0.2)\n",
    "Xtr1 = x_train\n",
    "Xte1 = x_test\n",
    "Ytr1 = y_train\n",
    "Yte1 = y_test\n",
    "Xtr = x_train\n",
    "client_data_testx.append(Xte1)\n",
    "client_data_testy.append(Yte1)\n",
    "####\n",
    "x_train, x_test, y_train, y_test = train_test_split(Xtr2,Ytr2,test_size=0.2)\n",
    "Xtr2 = x_train\n",
    "Xte2 = x_test\n",
    "Ytr2 = y_train\n",
    "Yte2 = y_test\n",
    "client_data_testx.append(Xte2)\n",
    "client_data_testy.append(Yte2)\n",
    "####\n",
    "x_train, x_test, y_train, y_test = train_test_split(Xtr3,Ytr3,test_size=0.2)\n",
    "Xtr3 = x_train\n",
    "Xte3 = x_test\n",
    "Ytr3 = y_train\n",
    "Yte3 = y_test\n",
    "client_data_testx.append(Xte3)\n",
    "client_data_testy.append(Yte3)\n",
    "####\n",
    "\n",
    "#client_data_testx.append(Xte4)\n",
    "#client_data_testy.append(Yte4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1424d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatnate teset data\n",
    "x_test_new = np.concatenate((client_data_testx[0], client_data_testx[1]), axis=0)\n",
    "x_test_new = np.concatenate((x_test_new, client_data_testx[2]), axis=0)\n",
    "y_test_new = np.concatenate((client_data_testy[0], client_data_testy[1]), axis=0)\n",
    "y_test_new = np.concatenate((y_test_new, client_data_testy[2]), axis=0)\n",
    "#test_batched1 = tf.data.Dataset.from_tensor_slices((x_test_new, y_test_new)).batch(len(y_test_new))\n",
    "x_test = x_test_new\n",
    "y_test = y_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "520200fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(data, predict, k):\n",
    "    #k=8\n",
    "    #if len(data) >= k:\n",
    "    #    warnings.warn('K is set to a value less than total voting groups!')\n",
    "\n",
    "    distances = []\n",
    "    count = 0\n",
    "    for sample in data:\n",
    "        euclidean_distance = np.linalg.norm(np.array(sample)-np.array(predict))\n",
    "        distances.append([euclidean_distance,count])\n",
    "        count+=1\n",
    "    \n",
    "    votes = [i[1] for i in sorted(distances)[:k]] ##votes is returning indexes of k random samples\n",
    "\n",
    "    #vote_result = Counter(votes).most_common(9)[0][0]\n",
    "    return votes\n",
    "def fair_kSMOTE_algo_2(dmajor,dminor,k,r):\n",
    "    S = []\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    #Ns = 12\n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    \n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    i = 0\n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,k) #from minority-p\n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            \n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / (len(N)-1) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return S\n",
    "\n",
    "def fair_kSMOTE(dmajor,dminor_wg,dminor,k,r):\n",
    "    S = []\n",
    "    #Ns =  int(r*(len(dmajor) - len(dminor)))\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    #Ns = 12\n",
    "    \n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    #if r==-1:\n",
    "    #    Nks = 1\n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    \n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    i = 0\n",
    "    \n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,int(k/2)+1) #from minority-p\n",
    "        N2= k_nearest_neighbors(dminor_wg,xb,int(k/2)) #from minority-np\n",
    "    \n",
    "        N3 = np.hstack((N, N2))\n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                \n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "            j = 0\n",
    "            while(j < len(N2)):\n",
    "                #here on random xb\n",
    "                ind = N2[j]\n",
    "                \n",
    "                x_new = x_new + ((dminor_wg[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1    \n",
    "            x_new = x_new / (len(N3)-1) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "   \n",
    "    return S\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def splitYtrain_sa_value(Xtr,Ytr,minority_lable,majority_label,pp_Group=p_Group,npp_Group=np_Group): #splite Ytrain based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x = []\n",
    "    dmaj_np_x = []\n",
    "    dmin_p_x = []\n",
    "    dmin_np_x = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==pp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x.append(Xtr[i])\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==npp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==pp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==npp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x.append(Xtr[i])\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x\n",
    "\n",
    "\n",
    "\n",
    "#create_synth_data(window, window_label, minority_label,majority_label,5,lambda_score, 'min_p')\n",
    "def create_synth_data(x, y, minority_lable,majority_label,k,r,group,pp_group,npp_group):\n",
    "    \n",
    "    \n",
    "    dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x = splitYtrain_sa_value(x,y,minority_lable,majority_label, pp_group,npp_group)\n",
    "    '''\n",
    "    print(\"length of dmin_p_x: \" + str(len(dmin_p_x)))\n",
    "    print(\"length of dmin_np_x: \" + str(len(dmin_np_x)))\n",
    "    print(\"length of dmaj_p_x: \" + str(len(dmaj_p_x)))\n",
    "    print(\"length of dmaj_np_x: \" + str(len(dmaj_np_x)))\n",
    "    '''\n",
    "    if len(dmin_p_x)<4:\n",
    "        return -1, -1\n",
    "    group_names = ['dmin_p_x', 'dmin_np_x', 'dmaj_p_x', 'dmaj_np_x']\n",
    "    \n",
    "    Xtr_new = []\n",
    "    Ytr_new = []  \n",
    "    \n",
    "    \n",
    "    ##Algo 3:\n",
    "    \n",
    "    if group =='min_p':\n",
    "        dmaj_x = dmaj_p_x\n",
    "        dmin_x = dmin_p_x\n",
    "        \n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE(dmaj_x,dmin_np_x,dmin_x,k,r)\n",
    "        #x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = minority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "        \n",
    "    elif group =='maj_np':\n",
    "        dmaj_x = dmin_np_x\n",
    "        dmin_x = dmaj_np_x\n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        \n",
    "        \n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = majority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return Xtr_new,Ytr_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5fe353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc23026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_update_classSize(train_data, label,classSize):\n",
    "    theta = 0.9\n",
    "    if (label not in classSize):\n",
    "        up_dict = {train_data[-1]:0.5}\n",
    "        classSize.update(up_dict)\n",
    "    for classValue in classSize:\n",
    "        if classValue == label:\n",
    "            update = theta * classSize.get(classValue) + (1-theta)\n",
    "            classSize[classValue] = update\n",
    "        else:\n",
    "            update = theta * classSize.get(classValue)\n",
    "            classSize[classValue] = update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3b99015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(Xtr1, Ytr1,Xtr2,Ytr2,Xtr3,Ytr3,num_clients,initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "    clients = {}\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "    \n",
    "    data = list(zip(Xtr1, Ytr1))\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    clients.update({client_names[0] :data})\n",
    "    data = list(zip(Xtr2, Ytr2))\n",
    "    clients.update({client_names[1] :data})\n",
    "    data = list(zip(Xtr3, Ytr3))\n",
    "    clients.update({client_names[2] :data})\n",
    "    #data = list(zip(Xtr4, Ytr4))\n",
    "    #clients.update({client_names[3] :data})\n",
    "\n",
    "    return clients\n",
    "clients = create_clients(Xtr1,Ytr1,Xtr2,Ytr2,Xtr3,Ytr3, num_clients=3, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5ea09d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7694\n",
      "8990\n",
      "7315\n",
      "{'client_1': 0, 'client_2': 0, 'client_3': 0}\n"
     ]
    }
   ],
   "source": [
    "from skmultiflow.drift_detection.eddm import EDDM\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "client_index = {}\n",
    "client_window = {}\n",
    "client_window_label = {}\n",
    "client_eddm = {}\n",
    "\n",
    "for (client_name, data) in clients.items():\n",
    "    data, label = zip(*data)\n",
    "    Y = np.asarray(label)\n",
    "    X = np.asarray(data)\n",
    "    client_index.update({client_name:0})\n",
    "    client_window.update({client_name:[]})\n",
    "    client_window_label.update({client_name:[]})\n",
    "    length = len(data)\n",
    "    print(len(data))\n",
    "#client_eddm.update({'client_1':eddm1})\n",
    "#client_eddm.update({'client_2':eddm2})\n",
    "#client_eddm.update({'client_3':eddm3})\n",
    "print(client_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adee5783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_1\n",
      "[0 1]\n",
      "client_2\n",
      "[0 1]\n",
      "client_3\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "for (client_name, data) in clients.items():\n",
    "    print(client_name)\n",
    "    data, label = zip(*data)\n",
    "    Y = np.asarray(label)\n",
    "    X = np.asarray(data)\n",
    "    print(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import math\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from skmultiflow.drift_detection.eddm import EDDM\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "num_clients = 3\n",
    "#adwin = ADWIN(delta =1)\n",
    "window=[]\n",
    "window_label = []\n",
    "window_warning = []\n",
    "window_label_warning = []\n",
    "pos_assigned=0\n",
    "pos_samples = 0\n",
    "neg_samples = 0\n",
    "pos_syn_samples = 0\n",
    "neg_syn_samples = 0\n",
    "generated_samples_per_sample = 0\n",
    "imbalance_ratio = 0 #of window\n",
    "minority_label=1\n",
    "majority_label = 0\n",
    "lambda_initial=0.001\n",
    "###\n",
    "ocis = 0\n",
    "classSize  = {}\n",
    "class_weights_dict = {}\n",
    "labels = []\n",
    "###\n",
    "j =0\n",
    "change=0\n",
    "warning=0\n",
    "\n",
    "\n",
    "bal_acc_global=[] \n",
    "stp_score_global=[]\n",
    "gmean_global = []\n",
    "eddm1 = EDDM()\n",
    "eddm2 = EDDM()\n",
    "eddm3 = EDDM()\n",
    "\n",
    "#one global network\n",
    "global_network = ONN(features_size=170, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "\n",
    "#3 networks for three clients\n",
    "onn_network_1 = ONN(features_size=170, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "onn_network_2 = ONN(features_size=170, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "onn_network_3 = ONN(features_size=170, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "weight = 1\n",
    "client_down =0 \n",
    "for _ in range(length):\n",
    "    if client_down==2:\n",
    "        break\n",
    "    sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer = [],[],[],[]\n",
    "    sum_alpha = []\n",
    "\n",
    "    for (client_name, data) in clients.items():\n",
    "        if client_name not in client_index:\n",
    "            continue\n",
    "        added_points = 0\n",
    "        data, label = zip(*data)\n",
    "        Y = np.asarray(label)\n",
    "        X = np.asarray(data)\n",
    "        \n",
    "        if client_name=='client_1':\n",
    "            eddm = eddm1\n",
    "            onn_network = onn_network_1\n",
    "        elif client_name=='client_2':\n",
    "            eddm = eddm2\n",
    "            onn_network = onn_network_2\n",
    "        else:\n",
    "            eddm = eddm3\n",
    "            onn_network = onn_network_3\n",
    "        i = client_index[client_name]\n",
    "        if i ==0:\n",
    "            print(client_index)\n",
    "        else:\n",
    "            if i%200==0:\n",
    "                galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer = global_network.get_weights('global')\n",
    "                onn_network.set_weights(galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer)\n",
    "        \n",
    "        if np.size(client_window[client_name])!=0:\n",
    "            if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\n",
    "                majority_count = list(client_window_label[client_name]).count(0)\n",
    "                minority_count = list(client_window_label[client_name]).count(1)\n",
    "                if majority_count >  minority_count and minority_count!=0:\n",
    "                    weight = int(majority_count/minority_count)\n",
    "        \n",
    "        if client_index[client_name]<len(Y):\n",
    "            if Y[i]==minority_label:\n",
    "                onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),2)\n",
    "            \n",
    "            else:\n",
    "                onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]))\n",
    "    \n",
    "            if np.size(client_window[client_name])==0:\n",
    "                client_window[client_name] = np.array(X[i])\n",
    "                client_window_label[client_name] = np.array(Y[i])\n",
    "            else:\n",
    "                client_window[client_name]=np.vstack((client_window[client_name],np.array(X[i])))\n",
    "                client_window_label[client_name]= np.vstack((client_window_label[client_name],np.array(Y[i])))\n",
    "            eddm.add_element(Y[i])\n",
    "    \n",
    "\n",
    "            if eddm.detected_change():\n",
    "                print('Change has been detected in data: ' + str(Y[i]) + ' - of index: ' + str(i))\n",
    "                change+=1\n",
    "                client_window[client_name] = []\n",
    "                client_window_label[client_name] = []\n",
    "    \n",
    "            \n",
    "            pos_assigned = onn_network.tp+onn_network.fp-0.2\n",
    "            pos_samples = onn_network.tp+onn_network.fn-0.2\n",
    "            \n",
    "            if np.size(client_window[client_name])!=0:\n",
    "                if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\n",
    "                    if onn_network.eqop_score > 0.005:\n",
    "                        #print(onn_network.stp_score)\n",
    "                        lambda_score = lambda_initial*(1+(onn_network.stp_score/0.2))\n",
    "                        if pos_assigned <= pos_samples:\n",
    "                            X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, 'min_p', p_Group,np_Group)\n",
    "                        else:\n",
    "                            X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, 'maj_np', p_Group,np_Group)\n",
    "                        if X_syn!=-1:\n",
    "                            Y_syn = np.array(Y_syn)\n",
    "                            X_syn = np.array(X_syn)\n",
    "                            X_syn, Y_syn = shuffle(X_syn, Y_syn, random_state=0)\n",
    "                            added_points = len(Y_syn)\n",
    "                            for k in range(len(X_syn)):\n",
    "                                onn_network.partial_fit(np.asarray([X_syn[k, :]]), np.asarray([Y_syn[k]]), 1, test='no')\n",
    "            \n",
    "            client_alpha, client_w_output_layer, client_b_output_layer, client_w_hidden_layer, client_b_hidden_layer = onn_network.get_weights('client')\n",
    "            i=i+1\n",
    "            client_index.update({client_name:i})\n",
    "            scaling_factor = (client_index[client_name]+added_points)/(sum(client_index.values())+added_points)\n",
    "            #scaling_factor = 1/5\n",
    "            scaling_factor2 = 1/3\n",
    "            print(client_index)\n",
    "            p = 0\n",
    "            if p==0:\n",
    "                if sum_alpha==[]:\n",
    "                    sum_alpha = torch.mul(client_alpha, scaling_factor2)\n",
    "                    sum_w_output_layer = client_w_output_layer\n",
    "                    sum_b_output_layer = client_b_output_layer\n",
    "                    sum_w_hidden_layer = client_w_hidden_layer\n",
    "                    sum_b_hidden_layer = client_b_hidden_layer\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                    for j in range(onn_network.max_num_hidden_layers):\n",
    "                        sum_w_output_layer[j] = torch.mul(client_w_output_layer[j], scaling_factor)\n",
    "                        sum_b_output_layer[j] = torch.mul(client_b_output_layer[j], scaling_factor)\n",
    "                        sum_w_hidden_layer[j] = torch.mul(client_w_hidden_layer[j], scaling_factor)\n",
    "                        sum_b_hidden_layer[j] = torch.mul(client_b_hidden_layer[j], scaling_factor)\n",
    "\n",
    "                else:\n",
    "                    sum_alpha = torch.add(sum_alpha, torch.mul(client_alpha, scaling_factor))\n",
    "                    for j in range(onn_network.max_num_hidden_layers):\n",
    "                        sum_w_output_layer[j] = torch.add(sum_w_output_layer[j],torch.mul(client_w_output_layer[j], scaling_factor)) \n",
    "                        sum_b_output_layer[j] = torch.add(sum_b_output_layer[j],torch.mul(client_b_output_layer[j], scaling_factor))\n",
    "                        sum_w_hidden_layer[j] = torch.add(sum_w_hidden_layer[j],torch.mul(client_w_hidden_layer[j], scaling_factor)) \n",
    "                        sum_b_hidden_layer[j] = torch.add(sum_b_hidden_layer[j],torch.mul(client_b_hidden_layer[j], scaling_factor))\n",
    "        if len(Y)==client_index[client_name]:\n",
    "            client_down+=1\n",
    "            del client_index[client_name]\n",
    "    if i%200==0:    \n",
    "        global_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer)\n",
    "\n",
    "        x_test, y_test = shuffle(x_test, y_test, random_state=0)\n",
    "        \n",
    "        for m in range(len(x_test)-1):\n",
    "            prediction_1 = global_network.predict_1(np.asarray([x_test[m, :]]))\n",
    "            global_network.update_eval_metrics(prediction_1,np.asarray([y_test[m]]))\n",
    "            global_network.update_stp_score(prediction_1,np.asarray([x_test[m, :]]))\n",
    "            global_network.update_eqop_score(prediction_1,np.asarray([x_test[m, :]]),np.asarray([y_test[m]]))\n",
    "        \n",
    "        print(\"Balanced accuracy: \" + str(global_network.bal_acc))\n",
    "        print(\"Sensitivity: \" + str(global_network.sen))\n",
    "        print(\"Specificity: \" + str(global_network.spec))\n",
    "        print(\"Stp score: \" + str(global_network.stp_score))\n",
    "        print(\"Eqop score: \" + str(global_network.eqop_score))\n",
    "        \n",
    "        bal_acc_global.append(global_network.bal_acc)\n",
    "        gmean_global.append(math.sqrt(global_network.sen*global_network.spec))\n",
    "        stp_score_global.append(global_network.stp_score)\n",
    "        global_network.reset_eval_metrics()\n",
    "global_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer) \n",
    "for n in range(len(x_test)-1):\n",
    "    prediction_1 = global_network.predict_1(np.asarray([x_test[n, :]]))\n",
    "    global_network.update_eval_metrics(prediction_1,np.asarray([y_test[n]]))\n",
    "    global_network.update_stp_score(prediction_1,np.asarray([x_test[n, :]]))\n",
    "    global_network.update_eqop_score(prediction_1,np.asarray([x_test[n, :]]),np.asarray([y_test[n]]))\n",
    "\n",
    "bal_acc_global.append(global_network.bal_acc)  \n",
    "stp_score_global.append(global_network.stp_score)        #print(\"change\" + str(change))\n",
    "gmean_global.append(math.sqrt(global_network.sen*global_network.spec))\n",
    "#print(\"warning\" + str(warning))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Balanced accuracy: \" + str(onn_network.bal_acc))\n",
    "print(\"Sensitivity: \" + str(onn_network.sen))\n",
    "print(\"Specificity: \" + str(onn_network.spec))\n",
    "print(\"Stp score: \" + str(onn_network.stp_score))\n",
    "print(\"Eqop score: \" + str(onn_network.eqop_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
