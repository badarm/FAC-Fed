{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140e5580",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import collections\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from mab import algs\n",
    "\n",
    "\n",
    "class ONN(nn.Module):\n",
    "    def __init__(self, features_size, max_num_hidden_layers, qtd_neuron_per_hidden_layer, n_classes, batch_size=1,\n",
    "                 b=0.99, n=0.01, s=0.2, use_cuda=False):\n",
    "        super(ONN, self).__init__()\n",
    "\n",
    "        if torch.cuda.is_available() and use_cuda:\n",
    "            print(\"Using CUDA :]\")\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda:0\" if torch.cuda.is_available() and use_cuda else \"cpu\")\n",
    "\n",
    "        self.features_size = features_size\n",
    "        self.max_num_hidden_layers = max_num_hidden_layers\n",
    "        self.qtd_neuron_per_hidden_layer = qtd_neuron_per_hidden_layer\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.b = Parameter(torch.tensor(\n",
    "            b), requires_grad=False).to(self.device)\n",
    "        self.n = Parameter(torch.tensor(\n",
    "            n), requires_grad=False).to(self.device)\n",
    "        self.s = Parameter(torch.tensor(\n",
    "            s), requires_grad=False).to(self.device)\n",
    "\n",
    "        self.hidden_layers = []\n",
    "        self.output_layers = []\n",
    "\n",
    "        self.hidden_layers.append(\n",
    "            nn.Linear(features_size, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        for i in range(max_num_hidden_layers - 1):\n",
    "            self.hidden_layers.append(\n",
    "                nn.Linear(qtd_neuron_per_hidden_layer, qtd_neuron_per_hidden_layer))\n",
    "\n",
    "        for i in range(max_num_hidden_layers):\n",
    "            self.output_layers.append(\n",
    "                nn.Linear(qtd_neuron_per_hidden_layer, n_classes))\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList(self.hidden_layers).to(self.device)\n",
    "        self.output_layers = nn.ModuleList(self.output_layers).to(self.device)\n",
    "\n",
    "        self.alpha = Parameter(torch.Tensor(self.max_num_hidden_layers).fill_(1 / (self.max_num_hidden_layers + 1)),\n",
    "                               requires_grad=False).to(\n",
    "            self.device)\n",
    "\n",
    "        self.loss_array = []\n",
    "        self.tp=0.1\n",
    "        self.tn=0.1\n",
    "        self.fp=0.1\n",
    "        self.fn=0.1\n",
    "        self.bal_acc = 0\n",
    "        self.bal_acc_list = []\n",
    "        self.sen = 0\n",
    "        self.spec = 0\n",
    "        \n",
    "        self.protected_pos = 0.1\n",
    "        self.protected_neg = 0.1\n",
    "        self.non_protected_pos = 0.1\n",
    "        self.non_protected_neg = 0.1\n",
    "        self.stp_score=0\n",
    "        \n",
    "        self.tp_protected = 0.1\n",
    "        self.fn_protected = 0.1\n",
    "        self.tp_non_protected = 0.1\n",
    "        self.fn_non_protected = 0.1\n",
    "        self.eqop_score=0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            self.output_layers[i].weight.grad.data.fill_(0)\n",
    "            self.output_layers[i].bias.grad.data.fill_(0)\n",
    "            self.hidden_layers[i].weight.grad.data.fill_(0)\n",
    "            self.hidden_layers[i].bias.grad.data.fill_(0)\n",
    "\n",
    "    def update_weights(self, X, Y, weight, show_loss, test):\n",
    "        Y = torch.from_numpy(Y).to(self.device)\n",
    "        if test=='yes':\n",
    "            prediction_1 = self.predict_1(X)\n",
    "            self.update_eval_metrics(prediction_1,Y)\n",
    "            self.update_stp_score(prediction_1,X)\n",
    "            self.update_eqop_score(prediction_1,X,Y)\n",
    "        \n",
    "        predictions_per_layer = self.forward(X)\n",
    "        losses_per_layer = []\n",
    "\n",
    "        for out in predictions_per_layer:\n",
    "            criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "            loss = criterion(out.view(self.batch_size, self.n_classes), Y.view(\n",
    "                self.batch_size).long())\n",
    "            losses_per_layer.append(loss*weight)\n",
    "\n",
    "        w = [None] * len(losses_per_layer)\n",
    "        b = [None] * len(losses_per_layer)\n",
    "        \n",
    "        with torch.no_grad():     #Context-manager that disabled gradient calculation\n",
    "            self.weights_output_layers = list()\n",
    "            self.biases_output_layers = list()\n",
    "            for i in range(len(losses_per_layer)):\n",
    "                losses_per_layer[i].backward(retain_graph=True)\n",
    "                self.output_layers[i].weight.data -= self.n * \\\n",
    "                                                   self.alpha[i] * self.output_layers[i].weight.grad.data\n",
    "                self.output_layers[i].bias.data -= self.n * \\\n",
    "                                                 self.alpha[i] * self.output_layers[i].bias.grad.data\n",
    "                self.weights_output_layers.append(self.output_layers[i].weight.data) \n",
    "                self.biases_output_layers.append(self.output_layers[i].bias.data)\n",
    "                \n",
    "                for j in range(i + 1):\n",
    "                    if w[j] is None:\n",
    "                        w[j] = self.alpha[i] * self.hidden_layers[j].weight.grad.data\n",
    "                        b[j] = self.alpha[i] * self.hidden_layers[j].bias.grad.data\n",
    "                    else:\n",
    "                        w[j] += self.alpha[i] * self.hidden_layers[j].weight.grad.data\n",
    "                        b[j] += self.alpha[i] * self.hidden_layers[j].bias.grad.data\n",
    "                self.zero_grad()\n",
    "            self.weights_hidden_layers= list()\n",
    "            self.biases_hidden_layers = list()\n",
    "            \n",
    "            for i in range(len(losses_per_layer)):\n",
    "                self.hidden_layers[i].weight.data -= self.n * w[i]\n",
    "                self.hidden_layers[i].bias.data -= self.n * b[i]\n",
    "                self.weights_hidden_layers.append(self.hidden_layers[i].weight.data)\n",
    "                self.biases_hidden_layers.append(self.hidden_layers[i].bias.data)\n",
    "            for i in range(len(losses_per_layer)):\n",
    "                self.alpha[i] *= torch.pow(self.b, losses_per_layer[i])\n",
    "                self.alpha[i] = torch.max(\n",
    "                  self.alpha[i], self.s / self.max_num_hidden_layers)\n",
    "\n",
    "        z_t = torch.sum(self.alpha)\n",
    "\n",
    "        self.alpha = Parameter(\n",
    "            self.alpha / z_t, requires_grad=False).to(self.device)\n",
    "        if show_loss:\n",
    "            \n",
    "            real_output = torch.sum(torch.mul(\n",
    "                self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, self.batch_size).view(\n",
    "                    self.max_num_hidden_layers, self.batch_size, 1), predictions_per_layer), 0)\n",
    "            criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "            loss = criterion(real_output.view(self.batch_size, self.n_classes), Y.view(self.batch_size).long())\n",
    "            self.loss_array.append(loss)\n",
    "            if (len(self.loss_array) % 1000) == 0:\n",
    "                #print(\"WARNING: Set 'show_loss' to 'False' when not debugging. \"\n",
    "                #      \"It will deteriorate the fitting performance.\")\n",
    "                loss = torch.Tensor(self.loss_array).mean().cpu().numpy()\n",
    "                #print(\"Alpha:\" + str(self.alpha.data.cpu().numpy()))\n",
    "                print(\"Training Loss: \" + str(loss))\n",
    "                print(\"Balanced accuracy: \" + str(self.bal_acc))\n",
    "                print(\"Sensitivity: \" + str(self.sen))\n",
    "                print(\"Specificity: \" + str(self.spec))\n",
    "                print(\"Stp score: \" + str(self.stp_score))\n",
    "                print(\"Eqop score: \" + str(self.eqop_score))\n",
    "                self.loss_array.clear()\n",
    "\n",
    "    def get_weights(self,network_name):\n",
    "        \n",
    "        if network_name == 'global':\n",
    "            weights_output_layers = list()\n",
    "            biases_output_layers = list()\n",
    "            weights_hidden_layers = list()\n",
    "            biases_hidden_layers= list()\n",
    "            for i in range(self.max_num_hidden_layers):\n",
    "                weights_output_layers.append(self.output_layers[i].weight.data) \n",
    "                biases_output_layers.append(self.output_layers[i].bias.data)\n",
    "                weights_hidden_layers.append(self.hidden_layers[i].weight.data)\n",
    "                biases_hidden_layers.append(self.hidden_layers[i].bias.data)\n",
    "               \n",
    "            return self.alpha, weights_output_layers, biases_output_layers, weights_hidden_layers, biases_hidden_layers\n",
    "        else:\n",
    "            return self.alpha, self.weights_output_layers, self.biases_output_layers, self.weights_hidden_layers, self.biases_hidden_layers\n",
    "    \n",
    "    def set_weights(self, alpha, w_output_layer, b_output_layer, w_hidden_layer, b_hidden_layer):\n",
    "        \n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            self.alpha[i] = alpha[i].clone().detach()\n",
    "            self.output_layers[i].weight.data = w_output_layer[i].clone().detach()\n",
    "            self.output_layers[i].bias.data = b_output_layer [i].clone().detach()\n",
    "            self.hidden_layers[i].weight.data =  w_hidden_layer [i].clone().detach()\n",
    "            self.hidden_layers[i].bias.data = b_hidden_layer [i].clone().detach()    \n",
    "    def forward(self, X):\n",
    "        hidden_connections = []\n",
    "\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "\n",
    "        x = F.relu(self.hidden_layers[0](X))\n",
    "        hidden_connections.append(x)\n",
    "\n",
    "        for i in range(1, self.max_num_hidden_layers):\n",
    "            hidden_connections.append(\n",
    "                F.relu(self.hidden_layers[i](hidden_connections[i - 1])))\n",
    "\n",
    "        output_class = []\n",
    "\n",
    "        for i in range(self.max_num_hidden_layers):\n",
    "            output_class.append(self.output_layers[i](hidden_connections[i]))\n",
    "\n",
    "        pred_per_layer = torch.stack(output_class)\n",
    "\n",
    "        return pred_per_layer\n",
    "\n",
    "    def validate_input_X(self, data):\n",
    "        \n",
    "        if len(data.shape) != 2:\n",
    "            raise Exception(\n",
    "                \"Wrong dimension for this X data. It should have only two dimensions.\")\n",
    "\n",
    "    def validate_input_Y(self, data):\n",
    "        if len(data.shape) != 1:\n",
    "            raise Exception(\n",
    "                \"Wrong dimension for this Y data. It should have only one dimensions.\")\n",
    "\n",
    "    def partial_fit_(self, X_data, Y_data, weight, show_loss=True, test='yes'):\n",
    "        \n",
    "        self.validate_input_X(X_data)\n",
    "        self.validate_input_Y(Y_data)\n",
    "        self.update_weights(X_data, Y_data, weight, show_loss, test)\n",
    "\n",
    "    def partial_fit(self, X_data, Y_data, weight, show_loss=True, test='yes'):\n",
    "        \n",
    "        self.partial_fit_(X_data, Y_data, weight, show_loss, test)\n",
    "\n",
    "    def reset_eval_metrics(self):\n",
    "        self.tp=0.1\n",
    "        self.tn=0.1\n",
    "        self.fp=0.1\n",
    "        self.fn=0.1\n",
    "        self.bal_acc = 0\n",
    "        self.bal_acc_list = []\n",
    "        self.sen = 0\n",
    "        self.spec = 0\n",
    "        \n",
    "        self.protected_pos = 0.1\n",
    "        self.protected_neg = 0.1\n",
    "        self.non_protected_pos = 0.1\n",
    "        self.non_protected_neg = 0.1\n",
    "        self.stp_score=0\n",
    "        \n",
    "        self.tp_protected = 0.1\n",
    "        self.fn_protected = 0.1\n",
    "        self.tp_non_protected = 0.1\n",
    "        self.fn_non_protected = 0.1\n",
    "        self.eqop_score=0\n",
    "    def update_eval_metrics(self,prediction_1,Y):\n",
    "        if prediction_1==1 and Y==1:\n",
    "            self.tp+=1\n",
    "        elif prediction_1==1 and Y==0:\n",
    "            self.fp+=1\n",
    "        elif prediction_1==0 and Y==1:\n",
    "            self.fn+=1\n",
    "        else:\n",
    "            self.tn+=1\n",
    "        \n",
    "        self.sen = self.tp/(self.tp + self.fn)\n",
    "        self.spec= self.tn/(self.tn + self.fp)\n",
    "        self.bal_acc = (self.sen + self.spec)/2\n",
    "        self.bal_acc_list.append(self.bal_acc)\n",
    "            \n",
    "    def update_stp_score(self,prediction_1,X):\n",
    "        if X[0][sa_index] == p_Group:\n",
    "            if prediction_1 == 1:\n",
    "                self.protected_pos += 1.\n",
    "            else:\n",
    "                self.protected_neg += 1.\n",
    "        else:\n",
    "            if prediction_1 == 1:\n",
    "                self.non_protected_pos += 1.\n",
    "            else:\n",
    "                self.non_protected_neg += 1.\n",
    "            \n",
    "        C_prot = (self.protected_pos) / (self.protected_pos + self.protected_neg)\n",
    "        C_non_prot = (self.non_protected_pos) / (self.non_protected_pos + self.non_protected_neg)\n",
    "\n",
    "        self.stp_score = C_non_prot - C_prot\n",
    "    \n",
    "    def update_eqop_score(self,prediction_1,X,Y):\n",
    "        if X[0][sa_index] == p_Group:\n",
    "            if prediction_1==1 and Y==1:\n",
    "                self.tp_protected += 1.\n",
    "            elif prediction_1==0 and Y==1:\n",
    "                self.fn_protected += 1.\n",
    "        else:\n",
    "            if prediction_1==1 and Y==1:\n",
    "                self.tp_non_protected += 1.\n",
    "            elif prediction_1==0 and Y==1:\n",
    "                self.fn_non_protected += 1.\n",
    "            \n",
    "        tpr_protected = self.tp_protected / (self.tp_protected + self.fn_protected)\n",
    "        tpr_non_protected = self.tp_non_protected / (self.tp_non_protected + self.fn_non_protected)\n",
    "        self.eqop_score = tpr_non_protected - tpr_protected\n",
    "    \n",
    "    def predict_1(self, X_data):\n",
    "        self.validate_input_X(X_data)\n",
    "        return torch.argmax(torch.sum(torch.mul(\n",
    "            self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, 1).view(\n",
    "                self.max_num_hidden_layers, len(X_data), 1), self.forward(X_data)), 0), dim=1).cpu().numpy()\n",
    "    def predict_(self, X_data):\n",
    "        self.validate_input_X(X_data)\n",
    "        return torch.argmax(torch.sum(torch.mul(\n",
    "            self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, len(X_data)).view(\n",
    "                self.max_num_hidden_layers, len(X_data), 1), self.forward(X_data)), 0), dim=1).cpu().numpy()\n",
    "\n",
    "    def predict(self, X_data):\n",
    "        pred = self.predict_(X_data)\n",
    "        return pred\n",
    "\n",
    "    def export_params_to_json(self):\n",
    "        state_dict = self.state_dict()\n",
    "        params_gp = {}\n",
    "        for key, tensor in state_dict.items():\n",
    "            params_gp[key] = tensor.cpu().numpy().tolist()\n",
    "\n",
    "        return json.dumps(params_gp)\n",
    "\n",
    "    def load_params_from_json(self, json_data):\n",
    "        params = json.loads(json_data)\n",
    "        o_dict = collections.OrderedDict()\n",
    "        for key, tensor in params.items():\n",
    "            o_dict[key] = torch.tensor(tensor).to(self.device)\n",
    "        self.load_state_dict(o_dict)\n",
    "\n",
    "\n",
    "class ONN_THS(ONN):\n",
    "    def __init__(self, features_size, max_num_hidden_layers, qtd_neuron_per_hidden_layer, n_classes, b=0.99, n=0.01,\n",
    "                 s=0.2, e=[0.5, 0.35, 0.2, 0.1, 0.05], use_cuda=False):\n",
    "        super().__init__(features_size, max_num_hidden_layers, qtd_neuron_per_hidden_layer, n_classes, b=b, n=n, s=s,\n",
    "                         use_cuda=use_cuda)\n",
    "        self.e = Parameter(torch.tensor(e), requires_grad=False)\n",
    "        self.arms_values = Parameter(\n",
    "            torch.arange(n_classes), requires_grad=False)\n",
    "        self.explorations_mab = []\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            self.explorations_mab.append(algs.ThompsomSampling(len(e)))\n",
    "\n",
    "    def partial_fit(self, X_data, Y_data, exp_factor, show_loss=True):\n",
    "        self.partial_fit_(X_data, Y_data, show_loss)\n",
    "        self.explorations_mab[Y_data[0]].reward(exp_factor)\n",
    "\n",
    "    def predict(self, X_data):\n",
    "        pred = self.predict_(X_data)[0]\n",
    "        exp_factor = self.explorations_mab[pred].select()[0]\n",
    "        if np.random.uniform() < self.e[exp_factor]:\n",
    "            removed_arms = self.arms_values.clone().numpy().tolist()\n",
    "            removed_arms.remove(pred)\n",
    "            return random.choice(removed_arms), exp_factor\n",
    "\n",
    "        return pred, exp_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30aa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from imblearn.datasets import make_imbalance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e47b4179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib2\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import preprocessing\n",
    "from random import seed, shuffle\n",
    "\n",
    "# import utils as ut\n",
    "\n",
    "SEED = 1234\n",
    "seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "def load_bank():\n",
    "    FEATURES_CLASSIFICATION = [\"age\", \"job\", \"marital\", \"education\", \"default\", \"balance\", \"housing\", \"loan\", \"contact\",\n",
    "                               \"day\", \"month\", \"duration\", \"campaign\", \"pdays\", \"previous\",\n",
    "                               \"poutcome\"]  # features to be used for classification\n",
    "    CONT_VARIABLES = [\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\",\n",
    "                      \"previous\"]  # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"y\"  # the decision variable\n",
    "    SENSITIVE_ATTRS = [\"marital\"]\n",
    "    CAT_VARIABLES = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"previous\", \"poutcome\"]\n",
    "    CAT_VARIABLES_INDICES = [1,2,3,4,6,7,8,10,14,15]\n",
    "    # COMPAS_INPUT_FILE = \"bank-full.csv\"\n",
    "    COMPAS_INPUT_FILE = \"./datasets/bank-full.csv\"\n",
    "\n",
    "    df = pd.read_csv(COMPAS_INPUT_FILE)\n",
    "    \n",
    "    # convert to np array\n",
    "    data = df.to_dict('list')\n",
    "    \n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "\n",
    "    # convert class label 0 to -1\n",
    "    y = data[CLASS_FEATURE]\n",
    "    y[y == \"yes\"] = 1\n",
    "    y[y == 'no'] = -1\n",
    "    y = np.array([int(k) for k in y])\n",
    "\n",
    "    X = np.array([]).reshape(len(y), 0)  # empty array with num rows same as num examples, will hstack the features to it\n",
    "    \n",
    "    x_control = defaultdict(list)\n",
    "    i=0\n",
    "    feature_names = []\n",
    "    \n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        vals = data[attr]\n",
    "        \n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            vals = preprocessing.scale(vals)  # 0 mean and 1 variance\n",
    "            vals = np.reshape(vals, (len(y), -1))  # convert from 1-d arr to a 2-d arr with one col\n",
    "            \n",
    "\n",
    "        else:  # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            lb = preprocessing.LabelBinarizer()\n",
    "            lb.fit(vals)\n",
    "            vals = lb.transform(vals)\n",
    "            #vals = np.reshape(vals, (len(y), -1))\n",
    "           \n",
    "            #if attr == 'job':\n",
    "            #    print(lb.classes_)\n",
    "            #    print(lb.transform(lb.classes_))\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "        # add to sensitive features dict\n",
    "        if attr in SENSITIVE_ATTRS:\n",
    "            x_control[attr] = vals\n",
    "            \n",
    "\n",
    "        # add to learnable features\n",
    "        X = np.hstack((X, vals))\n",
    "        \n",
    "        if attr in CONT_VARIABLES:  # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else:  # categorical features\n",
    "            if vals.shape[1] == 1:  # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in lb.classes_:  # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "    # convert the sensitive feature to 1-d array\n",
    "    \n",
    "    x_control = dict(x_control)\n",
    "    \n",
    "    for k in x_control.keys():\n",
    "        assert (x_control[k].shape[1] == 1)  # make sure that the sensitive feature is binary after one hot encoding\n",
    "        x_control[k] = np.array(x_control[k]).flatten()\n",
    "    \n",
    "    feature_names.append('target')\n",
    "    # '0' is the marital status = 'married'\n",
    "   \n",
    "    \n",
    "    return X, y, feature_names.index(SENSITIVE_ATTRS[0]), 0, x_control\n",
    "\n",
    "X_1,y, sa_index, p_Group, x_control= load_bank()\n",
    "#print(X[0])\n",
    "#print(X[0][1])\n",
    "#print(sa_index)\n",
    "np_Group = 1 #non-protected group's sa_value\n",
    "Y_1 = []\n",
    "for i in y:\n",
    "    if (i == -1):\n",
    "        Y_1.append(0)\n",
    "    else:\n",
    "        Y_1.append(1)\n",
    "Y_1 = np.array(Y_1)\n",
    "from sklearn.utils import shuffle\n",
    "X_1, Y_1 = shuffle(X_1, Y_1)\n",
    "X, x_test, Y, y_test = train_test_split(X_1,Y_1,test_size=0.2, stratify = Y_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be6c708b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8001,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "520200fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(data, predict, k):\n",
    "    #k=8\n",
    "    #if len(data) >= k:\n",
    "    #    warnings.warn('K is set to a value less than total voting groups!')\n",
    "\n",
    "    distances = []\n",
    "    count = 0\n",
    "    for sample in data:\n",
    "        euclidean_distance = np.linalg.norm(np.array(sample)-np.array(predict))\n",
    "        distances.append([euclidean_distance,count])\n",
    "        count+=1\n",
    "    \n",
    "    votes = [i[1] for i in sorted(distances)[:k]] ##votes is returning indexes of k random samples\n",
    "\n",
    "    #vote_result = Counter(votes).most_common(9)[0][0]\n",
    "    return votes\n",
    "def fair_kSMOTE_algo_2(dmajor,dminor,k,r):\n",
    "    S = []\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    #Ns = 12\n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    \n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    i = 0\n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,k) #from minority-p\n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            \n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / (len(N)-1) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return S\n",
    "\n",
    "def fair_kSMOTE(dmajor,dminor_wg,dminor,k,r):\n",
    "    S = []\n",
    "    #Ns =  int(r*(len(dmajor) - len(dminor)))\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    #Ns = 12\n",
    "    \n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    #if r==-1:\n",
    "    #    Nks = 1\n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    \n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    i = 0\n",
    "    \n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,int(k/2)+1) #from minority-p\n",
    "        N2= k_nearest_neighbors(dminor_wg,xb,int(k/2)) #from minority-np\n",
    "    \n",
    "        N3 = np.hstack((N, N2))\n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                \n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "            j = 0\n",
    "            while(j < len(N2)):\n",
    "                #here on random xb\n",
    "                ind = N2[j]\n",
    "                \n",
    "                x_new = x_new + ((dminor_wg[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1    \n",
    "            x_new = x_new / (len(N3)-1) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "   \n",
    "    return S\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def splitYtrain_sa_value(Xtr,Ytr,minority_lable,majority_label,pp_Group=p_Group,npp_Group=np_Group): #splite Ytrain based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x = []\n",
    "    dmaj_np_x = []\n",
    "    dmin_p_x = []\n",
    "    dmin_np_x = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==pp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x.append(Xtr[i])\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==npp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==pp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==npp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x.append(Xtr[i])\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x\n",
    "\n",
    "\n",
    "\n",
    "#create_synth_data(window, window_label, minority_label,majority_label,5,lambda_score, 'min_p')\n",
    "def create_synth_data(x, y, minority_lable,majority_label,k,r,group,pp_group,npp_group):\n",
    "    \n",
    "    \n",
    "    dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x = splitYtrain_sa_value(x,y,minority_lable,majority_label, pp_group,npp_group)\n",
    "    '''\n",
    "    print(\"length of dmin_p_x: \" + str(len(dmin_p_x)))\n",
    "    print(\"length of dmin_np_x: \" + str(len(dmin_np_x)))\n",
    "    print(\"length of dmaj_p_x: \" + str(len(dmaj_p_x)))\n",
    "    print(\"length of dmaj_np_x: \" + str(len(dmaj_np_x)))\n",
    "    '''\n",
    "    if len(dmin_p_x)<4:\n",
    "        return -1, -1\n",
    "    group_names = ['dmin_p_x', 'dmin_np_x', 'dmaj_p_x', 'dmaj_np_x']\n",
    "    \n",
    "    Xtr_new = []\n",
    "    Ytr_new = []  \n",
    "    \n",
    "    \n",
    "    ##Algo 3:\n",
    "    \n",
    "    if group =='min_p':\n",
    "        dmaj_x = dmaj_p_x\n",
    "        dmin_x = dmin_p_x\n",
    "        \n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE(dmaj_x,dmin_np_x,dmin_x,k,r)\n",
    "        #x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = minority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "        \n",
    "    elif group =='maj_np':\n",
    "        dmaj_x = dmin_np_x\n",
    "        dmin_x = dmaj_np_x\n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        \n",
    "        \n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = majority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return Xtr_new,Ytr_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5fe353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, Y = make_classification(n_samples=50000, n_features=8, n_informative=4, n_redundant=0, n_classes=2,\n",
    "#                           n_clusters_per_class=1, class_sep=3)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, shuffle=True)\n",
    "X_train, y_train= X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc23026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_update_classSize(train_data, label,classSize):\n",
    "    theta = 0.9\n",
    "    if (label not in classSize):\n",
    "        up_dict = {train_data[-1]:0.5}\n",
    "        classSize.update(up_dict)\n",
    "    for classValue in classSize:\n",
    "        if classValue == label:\n",
    "            update = theta * classSize.get(classValue) + (1-theta)\n",
    "            classSize[classValue] = update\n",
    "        else:\n",
    "            update = theta * classSize.get(classValue)\n",
    "            classSize[classValue] = update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3b99015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_clients(instances, labels, num_clients, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(instances, labels))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ea09d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10667\n",
      "10667\n",
      "10667\n",
      "{'client_1': 0, 'client_2': 0, 'client_3': 0}\n"
     ]
    }
   ],
   "source": [
    "from skmultiflow.drift_detection.eddm import EDDM\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "clients = create_clients(X_train, y_train, num_clients=3, initial='client')\n",
    "client_index = {}\n",
    "client_window = {}\n",
    "client_window_label = {}\n",
    "client_eddm = {}\n",
    "\n",
    "for (client_name, data) in clients.items():\n",
    "    data, label = zip(*data)\n",
    "    Y = np.asarray(label)\n",
    "    X = np.asarray(data)\n",
    "    client_index.update({client_name:0})\n",
    "    client_window.update({client_name:[]})\n",
    "    client_window_label.update({client_name:[]})\n",
    "    length = len(data)\n",
    "    print(len(data))\n",
    "#client_eddm.update({'client_1':eddm1})\n",
    "#client_eddm.update({'client_2':eddm2})\n",
    "#client_eddm.update({'client_3':eddm3})\n",
    "print(client_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b704b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from skmultiflow.drift_detection.eddm import EDDM\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "num_clients = 3\n",
    "#adwin = ADWIN(delta =1)\n",
    "window=[]\n",
    "window_label = []\n",
    "window_warning = []\n",
    "window_label_warning = []\n",
    "pos_assigned=0\n",
    "pos_samples = 0\n",
    "neg_samples = 0\n",
    "pos_syn_samples = 0\n",
    "neg_syn_samples = 0\n",
    "generated_samples_per_sample = 0\n",
    "imbalance_ratio = 0 #of window\n",
    "minority_label=1\n",
    "majority_label = 0\n",
    "lambda_initial=0.05  ###0.05 original\n",
    "###\n",
    "ocis = 0\n",
    "classSize  = {}\n",
    "class_weights_dict = {}\n",
    "labels = []\n",
    "###\n",
    "j =0\n",
    "change=0\n",
    "warning=0\n",
    "\n",
    "bal_acc_global = []  \n",
    "stp_score_global = []  \n",
    "\n",
    "eddm1 = EDDM()\n",
    "eddm2 = EDDM()\n",
    "eddm3 = EDDM()\n",
    "\n",
    "#one global network\n",
    "global_network = ONN(features_size=46, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "\n",
    "#3 networks for three clients\n",
    "onn_network_1 = ONN(features_size=46, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "onn_network_2 = ONN(features_size=46, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "onn_network_3 = ONN(features_size=46, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=2)\n",
    "\n",
    "weight = 1\n",
    "for _ in range(length):\n",
    "    sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer = [],[],[],[]\n",
    "    sum_alpha = []\n",
    "    \n",
    "    for (client_name, data) in clients.items():\n",
    "        np_Group = 1\n",
    "        p_Group = 0\n",
    "        added_points = 0\n",
    "        data, label = zip(*data)\n",
    "        Y = np.asarray(label)\n",
    "        X = np.asarray(data)\n",
    "        \n",
    "        if client_name=='client_1':\n",
    "            eddm = eddm1\n",
    "            onn_network = onn_network_1\n",
    "        elif client_name=='client_2':\n",
    "            eddm = eddm2\n",
    "            onn_network = onn_network_2\n",
    "        else:\n",
    "            eddm = eddm3\n",
    "            onn_network = onn_network_3\n",
    "            \n",
    "        i = client_index[client_name]\n",
    "        if i ==0:\n",
    "            print(client_index)\n",
    "        else:\n",
    "            if i%200==0:\n",
    "                galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer = global_network.get_weights('global')\n",
    "                onn_network.set_weights(galpha, gw_output_layer, gb_output_layer, gw_hidden_layer, gb_hidden_layer)\n",
    "        \n",
    "        if np.size(client_window[client_name])!=0:\n",
    "            if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\n",
    "                majority_count = list(client_window_label[client_name]).count(0)\n",
    "                minority_count = list(client_window_label[client_name]).count(1)\n",
    "                if majority_count >  minority_count and minority_count!=0:\n",
    "                    weight = int(majority_count/minority_count)\n",
    "        if Y[i]==minority_label:\n",
    "            onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),weight)\n",
    "            \n",
    "        else:\n",
    "            onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]),1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #onn_network.partial_fit(np.asarray([X[i, :]]), np.asarray([Y[i]]))\n",
    "    \n",
    "        if np.size(client_window[client_name])==0:\n",
    "            client_window[client_name] = np.array(X[i])\n",
    "            client_window_label[client_name] = np.array(Y[i])\n",
    "        else:\n",
    "            client_window[client_name]=np.vstack((client_window[client_name],np.array(X[i])))\n",
    "            client_window_label[client_name]= np.vstack((client_window_label[client_name],np.array(Y[i])))\n",
    "        eddm.add_element(Y[i])\n",
    "    \n",
    "\n",
    "        if eddm.detected_change():\n",
    "            print('Change has been detected in data: ' + str(Y[i]) + ' - of index: ' + str(i))\n",
    "            change+=1\n",
    "            client_window[client_name] = []\n",
    "            client_window_label[client_name] = []\n",
    "            onn_network.reset_eval_metrics()\n",
    "            \n",
    "        pos_assigned = onn_network.tp+onn_network.fp-0.2\n",
    "        pos_samples = onn_network.tp+onn_network.fn-0.2\n",
    "        \n",
    "        if np.size(client_window[client_name])!=0:\n",
    "            if client_window[client_name].ndim==2 and len(client_window[client_name])>30:\n",
    "                if onn_network.eqop_score < 0:\n",
    "                    pp_Group = np_Group\n",
    "                    npp_Group = p_Group\n",
    "                else:\n",
    "                    pp_Group = p_Group\n",
    "                    npp_Group = np_Group\n",
    "                disc_score = abs(onn_network.eqop_score)    \n",
    "                if disc_score > 0.01:\n",
    "                    #print(onn_network.stp_score)\n",
    "                    lambda_score = lambda_initial*(1+(onn_network.eqop_score/0.1))\n",
    "                    if pos_assigned <= pos_samples:\n",
    "                        X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, 'min_p', pp_Group,npp_Group)\n",
    "                    else:\n",
    "                        X_syn,Y_syn = create_synth_data(client_window[client_name], client_window_label[client_name], minority_label,majority_label,4,lambda_score, 'maj_np', pp_Group,npp_Group)\n",
    "                    if X_syn!=-1:\n",
    "                        Y_syn = np.array(Y_syn)\n",
    "                        X_syn = np.array(X_syn)\n",
    "                        X_syn, Y_syn = shuffle(X_syn, Y_syn, random_state=0)\n",
    "                        added_points = len(Y_syn)\n",
    "                        for k in range(len(X_syn)):\n",
    "                            onn_network.partial_fit(np.asarray([X_syn[k, :]]), np.asarray([Y_syn[k]]), 1, test='no')\n",
    "        client_alpha, client_w_output_layer, client_b_output_layer, client_w_hidden_layer, client_b_hidden_layer = onn_network.get_weights('client')\n",
    "        i=i+1\n",
    "        client_index.update({client_name:i})\n",
    "        #scaling_factor = (client_index[client_name]+added_points)/(sum(client_index.values())+added_points)\n",
    "        scaling_factor = 1/5\n",
    "        print(client_index)\n",
    "        p = 0\n",
    "        if p==0:\n",
    "            if sum_alpha==[]:\n",
    "                sum_alpha = torch.mul(client_alpha, scaling_factor)\n",
    "                sum_w_output_layer = client_w_output_layer\n",
    "                sum_b_output_layer = client_b_output_layer\n",
    "                sum_w_hidden_layer = client_w_hidden_layer\n",
    "                sum_b_hidden_layer = client_b_hidden_layer\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                for j in range(onn_network.max_num_hidden_layers):\n",
    "                    sum_w_output_layer[j] = torch.mul(client_w_output_layer[j], scaling_factor)\n",
    "                    sum_b_output_layer[j] = torch.mul(client_b_output_layer[j], scaling_factor)\n",
    "                    sum_w_hidden_layer[j] = torch.mul(client_w_hidden_layer[j], scaling_factor)\n",
    "                    sum_b_hidden_layer[j] = torch.mul(client_b_hidden_layer[j], scaling_factor)\n",
    "\n",
    "            else:\n",
    "                sum_alpha = torch.add(sum_alpha, torch.mul(client_alpha, scaling_factor))\n",
    "                for j in range(onn_network.max_num_hidden_layers):\n",
    "                    sum_w_output_layer[j] = torch.add(sum_w_output_layer[j],torch.mul(client_w_output_layer[j], scaling_factor)) \n",
    "                    sum_b_output_layer[j] = torch.add(sum_b_output_layer[j],torch.mul(client_b_output_layer[j], scaling_factor))\n",
    "                    sum_w_hidden_layer[j] = torch.add(sum_w_hidden_layer[j],torch.mul(client_w_hidden_layer[j], scaling_factor)) \n",
    "                    sum_b_hidden_layer[j] = torch.add(sum_b_hidden_layer[j],torch.mul(client_b_hidden_layer[j], scaling_factor))\n",
    "        \n",
    "    if i%200==0:    \n",
    "        global_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer)\n",
    "\n",
    "        x_test, y_test = shuffle(x_test, y_test, random_state=0)\n",
    "        \n",
    "        for m in range(len(x_test)-1):\n",
    "            prediction_1 = global_network.predict_1(np.asarray([x_test[m, :]]))\n",
    "            global_network.update_eval_metrics(prediction_1,np.asarray([y_test[m]]))\n",
    "            global_network.update_stp_score(prediction_1,np.asarray([x_test[m, :]]))\n",
    "            global_network.update_eqop_score(prediction_1,np.asarray([x_test[m, :]]),np.asarray([y_test[m]]))\n",
    "        \n",
    "        bal_acc_global.append(global_network.bal_acc)\n",
    "        stp_score_global.append(global_network.eqop_score)\n",
    "        global_network.reset_eval_metrics()\n",
    "global_network.set_weights(sum_alpha, sum_w_output_layer, sum_b_output_layer, sum_w_hidden_layer, sum_b_hidden_layer) \n",
    "for n in range(len(x_test)-1):\n",
    "    prediction_1 = global_network.predict_1(np.asarray([x_test[n, :]]))\n",
    "    global_network.update_eval_metrics(prediction_1,np.asarray([y_test[n]]))\n",
    "    global_network.update_stp_score(prediction_1,np.asarray([x_test[n, :]]))\n",
    "    global_network.update_eqop_score(prediction_1,np.asarray([x_test[n, :]]),np.asarray([y_test[n]]))\n",
    "\n",
    "bal_acc_global.append(global_network.bal_acc)  \n",
    "stp_score_global.append(global_network.eqop_score)  \n",
    "        \n",
    "        #print(\"change\" + str(change))\n",
    "#print(\"warning\" + str(warning))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Balanced accuracy: \" + str(onn_network.bal_acc))\n",
    "print(\"Sensitivity: \" + str(onn_network.sen))\n",
    "print(\"Specificity: \" + str(onn_network.spec))\n",
    "print(\"Stp score: \" + str(onn_network.stp_score))\n",
    "print(\"Eqop score: \" + str(onn_network.eqop_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22b5cda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy: 0.8419621632134513\n",
      "Sensitivity: 0.8926196095258528\n",
      "Specificity: 0.7913047169010498\n",
      "Gmean: 0.8404368551034519\n",
      "Stp score: 0.14633253862764245\n",
      "Eqop score: 0.028198868260186583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'BalanceACC')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "onn_network = global_network\n",
    "print(\"Balanced accuracy: \" + str(onn_network.bal_acc))\n",
    "print(\"Sensitivity: \" + str(onn_network.sen))\n",
    "print(\"Specificity: \" + str(onn_network.spec))\n",
    "print(\"Gmean: \" + str(math.sqrt(onn_network.sen*onn_network.spec)))\n",
    "print(\"Stp score: \" + str(onn_network.stp_score))\n",
    "print(\"Eqop score: \" + str(onn_network.eqop_score))\n",
    "plt.plot(bal_acc_global)\n",
    "plt.title('Global Server')\n",
    "plt.xlabel('Instances')\n",
    "plt.ylabel('BalanceACC')\n",
    "plt.plot(stp_score_global)\n",
    "plt.title('Global Server')\n",
    "plt.xlabel('Instances')\n",
    "plt.ylabel('BalanceACC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbd622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
